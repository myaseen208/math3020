[
  {
    "objectID": "slides/lesson01.html",
    "href": "slides/lesson01.html",
    "title": "The Role of Statistics in Engineering",
    "section": "",
    "text": "Statistics: The Science of Uncertainty\n\nEngineering Method: Systematic problem-solving approach\nData-Driven Decisions: Transform uncertainty into actionable insights\nVariability Management: Understanding and controlling variation\nProcess Monitoring: Continuous improvement through data\n\n\nWhy Statistics Matters in Engineering:\n\nQuality Control: Ensure products meet specifications\nProcess Optimization: Improve efficiency and performance\nRisk Assessment: Quantify and manage uncertainty\nDesign Validation: Verify engineering solutions work\n\n\n\n\n\n\n\n\n\nTipModern Engineering Reality\n\n\n\nToday’s engineers must navigate uncertainty, manage variability, and make data-driven decisions in increasingly complex systems.\n\n\n\n\n\n\n\nStatistical Foundations:\n\nRole of Statistics in engineering problem-solving\nVariability Impact on data and decision-making\nPopulation vs Sample concepts and applications\nRandom Variables and sources of variability\n\n\nPractical Applications:\n\nStatistical Reasoning and the data-to-wisdom pipeline\nParameters vs Statistics - inference fundamentals\nDescriptive vs Inferential statistics\nEngineering Examples with real-world applications\n\n\n\nGoal: Develop statistical reasoning skills essential for modern engineering practice.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#learning-objectives",
    "href": "slides/lesson01.html#learning-objectives",
    "title": "The Role of Statistics in Engineering",
    "section": "",
    "text": "Statistical Foundations:\n\nRole of Statistics in engineering problem-solving\nVariability Impact on data and decision-making\nPopulation vs Sample concepts and applications\nRandom Variables and sources of variability\n\n\nPractical Applications:\n\nStatistical Reasoning and the data-to-wisdom pipeline\nParameters vs Statistics - inference fundamentals\nDescriptive vs Inferential statistics\nEngineering Examples with real-world applications\n\n\n\nGoal: Develop statistical reasoning skills essential for modern engineering practice.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#the-engineering-problem-solving-method",
    "href": "slides/lesson01.html#the-engineering-problem-solving-method",
    "title": "The Role of Statistics in Engineering",
    "section": "2.1 The Engineering Problem-Solving Method",
    "text": "2.1 The Engineering Problem-Solving Method\n\n\nTraditional 8-Step Process:\n\nProblem Description - Clear, concise definition\nFactor Identification - What affects the problem?\nModel Proposal - Scientific/engineering knowledge\nExperimentation - Test and validate hypotheses\nModel Refinement - Based on observed data\nSolution Development - Manipulate model for solutions\nSolution Validation - Confirm effectiveness\nConclusions - Recommendations and decisions\n\n\nStatistical Integration:\n\nStep 1-2: Pattern recognition in data\nStep 3: Statistical models and relationships\nStep 4: Designed experiments and sampling\nStep 5: Data analysis and model fitting\nStep 6: Optimization and prediction\nStep 7: Validation and testing\nStep 8: Uncertainty quantification\n\n\n\nKey Insight: Statistics isn’t separate from engineering - it’s integral to every step!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#engineering-foundation",
    "href": "slides/lesson01.html#engineering-foundation",
    "title": "The Role of Statistics in Engineering",
    "section": "2.2 Engineering Foundation",
    "text": "2.2 Engineering Foundation\n\n\n\n\n\n\nNoteThe Engineering Problem-Solving Method\n\n\n\nEngineering Foundation: Engineers solve problems of interest to society by the efficient application of scientific principles. The engineering or scientific method is the approach to formulating and solving these problems.\nStatistical Integration: In today’s data-driven engineering environment, statistical methods are essential tools that complement traditional engineering analysis, helping engineers make sense of uncertain, variable data and draw reliable conclusions.\n\n\n\n\n\n\n\n\n\nThe engineering problem-solving method",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#statistics-in-engineering-problem-solving",
    "href": "slides/lesson01.html#statistics-in-engineering-problem-solving",
    "title": "The Role of Statistics in Engineering",
    "section": "2.3 Statistics in Engineering Problem-Solving",
    "text": "2.3 Statistics in Engineering Problem-Solving\n\n\n1. Problem Recognition and Definition\n\nHelps identify patterns in data that indicate problems\nQuantifies the magnitude and frequency of issues\nProvides tools for problem prioritization\n\n2. Hypothesis Formation\n\nUses data analysis to suggest potential causes\nApplies statistical models to test theories\nEmploys correlation analysis to identify relationships\n\n\n3. Data Collection and Analysis\n\nDesigns efficient experiments and sampling plans\nProvides methods for data quality assessment\nOffers tools for exploratory data analysis\n\n4. Conclusion and Decision Making\n\nQuantifies uncertainty in results\nProvides confidence intervals and hypothesis tests\nEnables risk-based decision making",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#engineering-example-statistical-problem-solving",
    "href": "slides/lesson01.html#engineering-example-statistical-problem-solving",
    "title": "The Role of Statistics in Engineering",
    "section": "3.1 Engineering Example: Statistical Problem-Solving",
    "text": "3.1 Engineering Example: Statistical Problem-Solving\n\n\n\n\n\n\nNoteExample: Manufacturing Process Investigation\n\n\n\nEngineering Context: A manufacturing engineer notices increased variability in product dimensions, which could lead to quality issues and customer complaints. This demonstrates systematic application of statistical methods to engineering problem-solving.\n\n\n\n\n[1] \"Manufacturing Problem Example - Step by Step Analysis\"\n\n\n\nVariability Comparison Between Time Periods\n\n\nperiod\ncount\nmean_dimension\nstd_dev\nmin_value\nmax_value\n\n\n\n\nFirst_Half\n15\n50.322\n0.148\n50.172\n50.587\n\n\nSecond_Half\n15\n49.731\n0.138\n49.528\n49.976\n\n\n\n\n\n\nAnalysis by Operator\n\n\noperator\ncount\nmean_dimension\nstd_dev\n\n\n\n\nA\n10\n49.940\n0.290\n\n\nB\n10\n50.206\n0.327\n\n\nC\n10\n49.934\n0.332\n\n\n\n\n\n[1] \"Temperature-Dimension Correlation: 0.249\"\n\n\nStatistical Analysis Steps:\n\nProblem Recognition: Control charts show process instability\nHypothesis Formation: Possible causes include temperature, operator, or material variation\nData Collection: Design experiment to test factors systematically\nAnalysis and Conclusion: Statistical tests identify significant factors",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#probability-quantifying-uncertainty",
    "href": "slides/lesson01.html#probability-quantifying-uncertainty",
    "title": "The Role of Statistics in Engineering",
    "section": "4.1 Probability: Quantifying Uncertainty",
    "text": "4.1 Probability: Quantifying Uncertainty\n\n\nProbability Concepts:\n\nUsed to quantify likelihood or chance\nUsed to represent risk or uncertainty in engineering applications\nCan be interpreted as our degree of belief or relative frequency\n\nKey Probability Concepts:\n\nSample Space (S): Set of all possible outcomes\nEvent (A): Subset of the sample space\nProbability P(A): Measure of likelihood, where 0 ≤ P(A) ≤ 1\n\n\nEngineering Applications:\n\nReliability analysis: P(component failure)\nQuality control: P(defective product)\nRisk assessment: P(system malfunction)\nDesign optimization: P(meeting specifications)\nSafety analysis: P(hazardous events)",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#statistics-the-science-of-learning-from-data",
    "href": "slides/lesson01.html#statistics-the-science-of-learning-from-data",
    "title": "The Role of Statistics in Engineering",
    "section": "4.2 Statistics: The Science of Learning from Data",
    "text": "4.2 Statistics: The Science of Learning from Data\n\n\nStatistics Definition:\nDeals with the collection, presentation, analysis, and use of data to:\n\nMake decisions\nSolve problems\nDesign products and processes\n\nStatistical techniques are useful for describing and understanding variability.\n\nVariability in Engineering:\nBy variability, we mean successive observations of a system or phenomenon do not produce exactly the same result.\nStatistics gives us a framework for:\n\nDescribing this variability\nLearning about potential sources of variability\nMaking decisions despite uncertainty",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#the-art-and-science-of-learning-from-data",
    "href": "slides/lesson01.html#the-art-and-science-of-learning-from-data",
    "title": "The Role of Statistics in Engineering",
    "section": "5.1 The Art and Science of Learning from Data",
    "text": "5.1 The Art and Science of Learning from Data\n\n\nStatistics is the science of uncertainty & variability\nTurning Data into Information:\nData → Information → Knowledge → Wisdom\nThe Statistical Thinking Process:\n\nRecognize the need for data-based decisions\nUnderstand the importance of data quality\nAppreciate the role of variability\nUse appropriate statistical methods\nCommunicate results effectively\n\n\n\n\n\n\n\n\n\nBig Data in Engineering\n\n\nStatistics is the Art and Science of learning from Data.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#population-vs.-sample",
    "href": "slides/lesson01.html#population-vs.-sample",
    "title": "The Role of Statistics in Engineering",
    "section": "6.1 Population vs. Sample",
    "text": "6.1 Population vs. Sample\n\n\nPopulation\n\nSet of measurements of interest\nCharacteristics of the population (parameters) are typically of interest\nUsually unknown and must be estimated\n\nSample\n\nSubset of measurements of interest\nA characteristic of the sample (statistic) is used to infer population characteristics (parameters)\nObservable and computable\n\n\nParameter\n\nA characteristic of the population (usually unknown and estimated from sample data)\nExamples: μ, σ, π\n\nStatistic\n\nA characteristic of the sample (computed from observed data)\nExamples: x̄ ,s ,p",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#types-of-statistical-analysis",
    "href": "slides/lesson01.html#types-of-statistical-analysis",
    "title": "The Role of Statistics in Engineering",
    "section": "6.2 Types of Statistical Analysis",
    "text": "6.2 Types of Statistical Analysis\n\n\nDescriptive Statistics\n\nDescribing the important characteristics of a set of data\nMeasures of central tendency (mean, median, mode)\nMeasures of variability (standard deviation, range)\nGraphical displays (histograms, box plots)\nSummary tables and charts\n\n\nInferential Statistics\n\nUsing sample data to make inferences (or generalizations) about a population\nHypothesis testing\nConfidence intervals\nRegression analysis\nAnalysis of variance (ANOVA)\n\nStatistical Inference: Making a statement about the population (parameter) based on the sample (statistic)",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#population-vs.-sample-in-practice",
    "href": "slides/lesson01.html#population-vs.-sample-in-practice",
    "title": "The Role of Statistics in Engineering",
    "section": "7.1 Population vs. Sample in Practice",
    "text": "7.1 Population vs. Sample in Practice\n\n\n\n\n\n\nNoteExample: O-Ring Development for Semiconductor Equipment\n\n\n\nEngineering Context: An engineer is developing a rubber compound for use in O-rings. The O-rings are to be employed as seals in plasma etching tools used in the semiconductor industry, so their resistance to acids and other corrosive substances is an important characteristic.\nThe engineer uses the standard rubber compound to produce eight O-rings in a development laboratory and measures the tensile strength of each specimen after immersion in a nitric acid solution at 30°C for 25 minutes.\n\n\nData: The tensile strengths (in psi) of the eight O-rings are: 1030, 1035, 1020, 1049, 1028, 1026, 1019, and 1010\n\n\n\nO-Ring Sample Statistics & Confidence Interval\n\n\nsample_size\nsample_mean\nsample_median\nsample_std_dev\nsample_min\nsample_max\nstd_error\nt_value\nmargin_error\nci_lower\nci_upper\n\n\n\n\n8\n1027.1\n1027\n11.7\n1010\n1049\n4.14\n2.36\n9.8\n1017.3\n1036.9",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#analysis-of-o-ring-data",
    "href": "slides/lesson01.html#analysis-of-o-ring-data",
    "title": "The Role of Statistics in Engineering",
    "section": "7.2 Analysis of O-Ring Data",
    "text": "7.2 Analysis of O-Ring Data\n\n\nStatistical Components:\n\nPopulation: All possible O-rings made with this rubber compound\nSample: The eight O-rings tested (n = 8)\nParameter: True mean tensile strength (μ) of all O-rings\nStatistic: Sample mean tensile strength (x̄ = 1027.1 psi)\n\n\nKey Observation:\nAs we should have anticipated, not all the O-ring specimens exhibit the same measurement of tensile strength.\nThis demonstrates the fundamental concept of variability in engineering measurements.\n\n\n\n\n\n\n\n\nImportantEngineering Insight\n\n\n\nThe variation in tensile strength is not a measurement error - it’s real variability that must be understood and managed in the engineering design process.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#understanding-random-variables",
    "href": "slides/lesson01.html#understanding-random-variables",
    "title": "The Role of Statistics in Engineering",
    "section": "8.1 Understanding Random Variables",
    "text": "8.1 Understanding Random Variables\n\n\nRandom Variable Concept:\nSince tensile strength varies or exhibits variability, it is a random variable.\nA random variable X can be modeled by:\nX = \\mu + \\epsilon\nwhere μ is a constant and ε is a random disturbance, or “noise” term.\n\nSources of Variability:\n\nCommon Causes: Natural variation inherent in the process\nSpecial Causes: Unusual events that create additional variation\nMeasurement Error: Variation due to measurement system\nEnvironmental Factors: Temperature, humidity, vibration effects\n\n\n\n\n\n\n\n\n\nTipEngineering Application\n\n\n\nUnderstanding sources of variability helps engineers design more robust processes and make better decisions under uncertainty.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/lesson01.html#the-statistical-foundation-of-modern-engineering",
    "href": "slides/lesson01.html#the-statistical-foundation-of-modern-engineering",
    "title": "The Role of Statistics in Engineering",
    "section": "9.1 The Statistical Foundation of Modern Engineering",
    "text": "9.1 The Statistical Foundation of Modern Engineering\n\n\nCore Concepts Mastered:\n\nEngineering Method: Statistics integrated in every step\nVariability: The reality of engineering measurements\nPopulation vs Sample: Foundation of statistical inference\nRandom Variables: Mathematical framework for uncertainty\n\n\nPractical Skills Developed:\n\nStatistical Thinking: Data-driven problem solving\nParameter Estimation: Sample statistics to infer population parameters\nVariability Analysis: Understanding sources and impacts\nDecision Making: Under uncertainty with confidence",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Lecture Slides",
    "section": "",
    "text": "Welcome to the lecture slides collection for Math-3020. Each lecture is designed as an interactive presentation covering key statistical concepts with real-world applications in science and engineering.\n📊 View All Slides 📚 Return to Textbook",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "slides/index.html#course-lecture-slides",
    "href": "slides/index.html#course-lecture-slides",
    "title": "Lecture Slides",
    "section": "",
    "text": "Welcome to the lecture slides collection for Math-3020. Each lecture is designed as an interactive presentation covering key statistical concepts with real-world applications in science and engineering.\n📊 View All Slides 📚 Return to Textbook",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "slides/index.html#how-to-use-the-slides",
    "href": "slides/index.html#how-to-use-the-slides",
    "title": "Lecture Slides",
    "section": "2 How to Use the Slides",
    "text": "2 How to Use the Slides\n\n\n2.1 🎯 Navigation Tips\n\nUse arrow keys or spacebar to navigate\nPress F for fullscreen mode\nPress S for speaker notes\nPress O for slide overview\nPress Q to activate laser pointer\n\n\n\n2.2 ✨ Interactive Features\n\nLive code examples with R\nInteractive plots and visualizations\nClickable equations for detailed explanations\nEmbedded videos and animations",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "slides/index.html#lecture-schedule",
    "href": "slides/index.html#lecture-schedule",
    "title": "Lecture Slides",
    "section": "3 Lecture Schedule",
    "text": "3 Lecture Schedule\n\n\n3.1 Lecture 1: Introduction to R\n📊 View Slides\n\n\n3.2 Lesson 01: The Engineering Method and Statistical Thinking\n📊 View Slides",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "slides/index.html#quick-access-tools",
    "href": "slides/index.html#quick-access-tools",
    "title": "Lecture Slides",
    "section": "4 Quick Access Tools",
    "text": "4 Quick Access Tools\n\n\n\n\n\n\nNote🔧 Presentation Tools\n\n\n\n\nPresenter Mode: Press S during any presentation to access speaker notes\nLaser Pointer: Press Q to activate the orange laser pointer\nDrawing Board: Use the chalkboard feature to annotate slides\nZoom: Press Alt + Click to zoom into specific areas\n\n\n\n\n\n\n\n\n\nTip📱 Mobile Viewing\n\n\n\nAll slides are optimized for mobile devices. For the best experience on phones and tablets: - Use landscape orientation - Enable fullscreen mode - Use touch gestures for navigation\n\n\n\n\n\n\n\n\nImportant💡 Before Each Class\n\n\n\n\nReview the previous lecture slides\nPreview today’s slides for key concepts\nPrepare questions about difficult topics\nBring calculator and notes for examples",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "slides/index.html#keyboard-shortcuts-reference",
    "href": "slides/index.html#keyboard-shortcuts-reference",
    "title": "Lecture Slides",
    "section": "5 Keyboard Shortcuts Reference",
    "text": "5 Keyboard Shortcuts Reference\n\n\n\nKey\nAction\n\n\n\n\n→ / Space\nNext slide\n\n\n←\nPrevious slide\n\n\nF\nToggle fullscreen\n\n\nS\nSpeaker notes\n\n\nO\nSlide overview\n\n\nQ\nLaser pointer\n\n\nB\nBlackout screen\n\n\nEsc\nExit fullscreen\n\n\n\n\n\nNeed help with slides? Contact through email or visit office hours.\nSlides are updated regularly. Check back for the latest versions.",
    "crumbs": [
      "Home",
      "Lectures",
      "Lecture Slides"
    ]
  },
  {
    "objectID": "introR/index.html",
    "href": "introR/index.html",
    "title": "Introduction to R Programming",
    "section": "",
    "text": "A comprehensive guide to R programming for statistical computing in Math-3020. Master essential R skills through one complete document available in your preferred format.\n📊 View HTML Version 📚 Download Resources",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#introduction-to-r-programming",
    "href": "introR/index.html#introduction-to-r-programming",
    "title": "Introduction to R Programming",
    "section": "",
    "text": "A comprehensive guide to R programming for statistical computing in Math-3020. Master essential R skills through one complete document available in your preferred format.\n📊 View HTML Version 📚 Download Resources",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#download-formats",
    "href": "introR/index.html#download-formats",
    "title": "Introduction to R Programming",
    "section": "2 Choose Your Format",
    "text": "2 Choose Your Format\n\n\n🌐 ### Interactive HTML Access the complete R programming guide online with interactive code examples that you can run and modify directly in your browser.\nFeatures: - Executable R code blocks - Interactive plots and visualizations\n- Complete searchable content - Mobile-friendly responsive design - Always up-to-date content\nView HTML Version →\n\n\n📑 ### PDF Document Download the complete R programming guide as a professional PDF document, perfect for offline reading, printing, and annotation.\nFeatures: - Print-friendly layout - Professional typography - Complete code listings - Offline accessibility - Perfect for studying\nDownload PDF →\n\n\n📝 ### Word Document Get the complete guide as an editable Word document for note-taking, customization, and creating your personalized R reference.\nFeatures: - Fully editable content - Add your own notes - Customizable formatting - Easy sharing and collaboration - Personal reference creation\nDownload DOCX →",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#getting-started",
    "href": "introR/index.html#getting-started",
    "title": "Introduction to R Programming",
    "section": "3 What’s Included in introR",
    "text": "3 What’s Included in introR\nThis comprehensive guide covers everything you need to know about R programming for statistical analysis in Math-3020:\n\n\n3.1 🎯 R Fundamentals\nMaster the basics of R programming and statistical computing.\n\nInstalling R and RStudio\nBasic syntax and data types\nVariables, vectors, and functions\nR workspace and help system\nCommon errors and debugging\n\n\n\n3.2 📊 Data Management\nLearn to import, clean, and manipulate data effectively.\n\nReading data from various sources\nData frames and tibbles\nFiltering, sorting, and subsetting\nCreating new variables\nData cleaning techniques\n\n\n\n3.3 📈 Data Visualization\nCreate compelling visualizations for data exploration and presentation.\n\nBase R plotting system\nggplot2 fundamentals\nStatistical plots and charts\nCustomizing graphics\nExporting high-quality figures\n\n\n\n3.4 🔢 Statistical Analysis\nApply R to statistical methods covered in Math-3020.\n\nDescriptive statistics\nProbability distributions\nHypothesis testing\nConfidence intervals\nRegression analysis and ANOVA\n\n\n\n3.5 ⚡ Advanced Programming\nDevelop advanced R programming skills for complex analyses.\n\nControl structures and loops\nWriting custom functions\nError handling and debugging\nPackage management\nReproducible research workflows\n\n\n\n3.6 💪 Hands-On Practice\nReinforce your learning with practical exercises and real datasets.\n\nStep-by-step tutorials\nReal-world data problems\nWorked solutions\nChallenge exercises\nCourse-specific examples",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#document-overview",
    "href": "introR/index.html#document-overview",
    "title": "Introduction to R Programming",
    "section": "4 Document Overview",
    "text": "4 Document Overview\nComplete Content: All R programming topics needed for Math-3020 in one comprehensive document Estimated Reading Time: 4-6 hours for complete coverage\nPractice Time: Additional 10-15 hours for exercises and examples Skill Level: Beginner to intermediate R programming",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#why-learn-r-for-statistics",
    "href": "introR/index.html#why-learn-r-for-statistics",
    "title": "Introduction to R Programming",
    "section": "5 Why Learn R for Statistics?",
    "text": "5 Why Learn R for Statistics?\n\n\n\n\n\n\nNote🚀 Industry Standard\n\n\n\nR is the premier language for statistical analysis, used by statisticians, data scientists, and researchers worldwide. Learning R gives you access to cutting-edge statistical methods and a vast ecosystem of packages.\n\n\n\n\n\n\n\n\nTip📈 Career Ready\n\n\n\nR skills are highly valued in: - Data analysis and statistics roles - Research and academic positions\n- Bioinformatics and scientific computing - Business analytics and consulting - Government and policy analysis\n\n\n\n\n\n\n\n\nImportant🎓 Course Integration\n\n\n\nThis R guide is specifically designed for Math-3020 students: - All statistical concepts include R implementations - Examples use datasets relevant to science and engineering - Code examples align with course topics and homework - Practice problems mirror exam-style questions",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#getting-started-1",
    "href": "introR/index.html#getting-started-1",
    "title": "Introduction to R Programming",
    "section": "6 Getting Started",
    "text": "6 Getting Started\n\n6.1 Software Requirements\n\nR (Required): Download the latest version from r-project.org\nRStudio (Recommended): Download the free version from rstudio.com\nRequired R Packages: We’ll install these as needed throughout the course\nInternet Connection: For downloading packages and accessing help resources\n\n\n\n\n\n\n\n\nWarningInstallation Help\n\n\n\nNew to R installation? Don’t worry! Our R Basics chapter includes detailed, step-by-step installation instructions for Windows, Mac, and Linux systems.",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#learning-path-recommendations",
    "href": "introR/index.html#learning-path-recommendations",
    "title": "Introduction to R Programming",
    "section": "7 Learning Path Recommendations",
    "text": "7 Learning Path Recommendations\n\n\n7.1 🥇 For Beginners\nRecommended Approach: 1. Download your preferred format (HTML for interactive learning) 2. Start from the beginning and work through systematically 3. Practice all code examples as you read 4. Complete exercises before moving to next sections 5. Use PDF version for reference and offline study\nEstimated Time: 15-20 hours total\n\n\n7.2 🚀 For Experienced Programmers\nFast Track Approach: 1. Use HTML version for quick navigation 2. Skim basics and focus on R-specific syntax 3. Concentrate on statistical analysis sections 4. Download Word version to add your own notes 5. Challenge yourself with advanced programming examples\nEstimated Time: 8-12 hours total",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "introR/index.html#additional-resources",
    "href": "introR/index.html#additional-resources",
    "title": "Introduction to R Programming",
    "section": "8 Additional Resources",
    "text": "8 Additional Resources\n\nR Documentation: Built-in help system (?function_name)\nRStudio Cheatsheets: Quick reference guides for common tasks\nStack Overflow: Community Q&A for programming questions\nCourse Discussion Forum: Ask questions specific to Math-3020\nOffice Hours: Get personalized help from the instructor\n\n\n\nReady to start learning R? Choose your preferred format above and begin your R programming journey.\nComplete R programming guide specifically designed for Math-3020 success.\n\nQuestions about R or need technical support? Contact myaseen208@gmail.com or visit office hours.",
    "crumbs": [
      "Home",
      "R Programming Guide",
      "Introduction to R Programming"
    ]
  },
  {
    "objectID": "book/Ch05.html",
    "href": "book/Ch05.html",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "TipMajor Themes of Chapter 5\n\n\n\n\nTwo-Sample Comparisons: Comparing parameters between two populations or treatments\nPaired vs. Independent Samples: Understanding when to use different comparison methods\nVariance Comparisons: Testing equality of variances using F-tests\nProportion Comparisons: Comparing success rates between two groups\nIntroduction to ANOVA: Extending two-sample methods to multiple samples\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nTest hypotheses and construct confidence intervals for the difference between two means when variances are known and unknown.\nUnderstand and apply the pooled variance procedure for two-sample t-tests.\nConduct and interpret paired t-tests for dependent samples.\nTest hypotheses and construct confidence intervals for the ratio of two variances using F-tests.\nCompare two population proportions using appropriate statistical methods.\nCalculate sample sizes and power for two-sample tests.\nUnderstand the basic concepts of Analysis of Variance (ANOVA).\nDistinguish between completely randomized and randomized complete block experimental designs.\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo-Sample Problems\n\n\n\nMany engineering problems involve comparing two populations, processes, or treatments. Common scenarios include:\n\nComparing the mean strength of materials from two suppliers\nTesting whether a new manufacturing process reduces defect rates\nEvaluating if a design modification improves performance\nComparing measurements before and after a process change\n\nThe choice of statistical method depends on:\n\nWhether samples are independent or paired\nWhether population variances are known or unknown\nWhether variances can be assumed equal or unequal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo-Sample Z-Test (σ₁, σ₂ known)\n\n\n\nWhen both population variances are known, the test statistic for testing H₀: μ₁ = μ₂ is:\nZ_0 = \\frac{(\\overline{X_1} - \\overline{X_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\nUnder H₀: μ₁ - μ₂ = 0, Z₀ ~ N(0,1)\nCommon Hypothesis Tests:\n\nTwo-sided: H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂\nOne-sided: H₀: μ₁ ≤ μ₂ vs H₁: μ₁ &gt; μ₂\nOne-sided: H₀: μ₁ ≥ μ₂ vs H₁: μ₁ &lt; μ₂\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA company wants to compare the tensile strength of cables produced by two different machines. Machine 1 produces cables with σ₁ = 12 psi, and Machine 2 with σ₂ = 10 psi. Random samples yield:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Cable Strength Data Summary by Machine:\"\n\n\n     machine     n      Min       Q1   Median     Mean       Q3      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Machine_1    20 221.4006 239.0774 246.4398 246.6995 251.5847 266.4430\n2: Machine_2    25 221.1331 231.0529 235.9208 238.0735 246.2158 259.6896\n          SD       Var\n       &lt;num&gt;     &lt;num&gt;\n1: 11.671984 136.23521\n2:  9.449202  89.28741\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\n\n# Cable strength comparison (two-sample Z-test, sigma known)\nset.seed(123)\ncable_data &lt;- data.table(\n  machine = rep(c(\"Machine_1\", \"Machine_2\"), c(20, 25)),\n  strength = c(rnorm(20, mean = 245, sd = 12), rnorm(25, mean = 238, sd = 10))\n)\n\n# Summary statistics using collapse functions\ncable_summary &lt;- cable_data %&gt;% \n  fgroup_by(machine) %&gt;% \n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength)\n  )\n\nprint(\"Cable Strength Data Summary by Machine:\")\nprint(cable_summary)\n\n# Write data to CSV for download\nfwrite(cable_data, \"data/cable_strength.csv\")\n\n\n\n\n\n\n\nTest H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂ at α = 0.05.\nManual Calculation:\nGiven: n₁ = 20, x̄₁ = 245, σ₁ = 12, n₂ = 25, x̄₂ = 238, σ₂ = 10\n\nTest Statistic: Z_0 = \\frac{(245 - 238) - 0}{\\sqrt{\\frac{12^2}{20} + \\frac{10^2}{25}}} = \\frac{7}{\\sqrt{7.2 + 4}} = \\frac{7}{3.35} = 2.09\nCritical Value: z₀.₀₂₅ = 1.96\nDecision: |Z₀| = 2.09 &gt; 1.96, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Two-Sample Z-Test Results:\"\n\n\n                     Test_Type    n1    n2 xbar1  xbar2 sigma1 sigma2 Pooled_SE\n                        &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;     &lt;num&gt;\n1: Two-sample Z-test (σ known)    20    25 246.7 238.07     12     10     3.347\n   Z_Statistic P_Value Alpha Z_Critical  Decision\n         &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:      2.5775    0.01  0.05       1.96 Reject H0\n                          Conclusion\n                              &lt;char&gt;\n1: Means are significantly different\n\n\n\n\n\n# Two-sample Z-test (manual and R calculations)\n# Known population standard deviations\nsigma1 &lt;- 12\nsigma2 &lt;- 10\nalpha &lt;- 0.05\n\n# Extract sample statistics\nmachine1_data &lt;- cable_data[machine == \"Machine_1\"]\nmachine2_data &lt;- cable_data[machine == \"Machine_2\"]\n\nn1 &lt;- fnobs(machine1_data$strength)\nn2 &lt;- fnobs(machine2_data$strength)\nxbar1 &lt;- fmean(machine1_data$strength)\nxbar2 &lt;- fmean(machine2_data$strength)\n\n# Manual calculation\npooled_se &lt;- sqrt((sigma1^2)/n1 + (sigma2^2)/n2)\nz_stat &lt;- (xbar1 - xbar2) / pooled_se\np_value_z &lt;- 2 * (1 - pnorm(abs(z_stat)))  # Two-sided test\nz_critical &lt;- qnorm(1 - alpha/2)\n\n# Test results\nz_test_results &lt;- data.table(\n  Test_Type = \"Two-sample Z-test (σ known)\",\n  n1 = n1,\n  n2 = n2,\n  xbar1 = round(xbar1, 2),\n  xbar2 = round(xbar2, 2),\n  sigma1 = sigma1,\n  sigma2 = sigma2,\n  Pooled_SE = round(pooled_se, 3),\n  Z_Statistic = round(z_stat, 4),\n  P_Value = round(p_value_z, 4),\n  Alpha = alpha,\n  Z_Critical = round(z_critical, 3),\n  Decision = ifelse(abs(z_stat) &gt; z_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_z &lt; alpha, \n                     \"Means are significantly different\", \n                     \"No significant difference in means\")\n)\n\nprint(\"Two-Sample Z-Test Results:\")\nprint(z_test_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePower and Sample Size for Two-Sample Z-Test\n\n\n\nType II Error: For a specific difference δ = |μ₁ - μ₂|:\n\\beta = P\\left(-z_{\\alpha/2} - \\frac{\\delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\leq Z \\leq z_{\\alpha/2} - \\frac{\\delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right)\nSample Size: For equal sample sizes (n₁ = n₂ = n):\nn = \\frac{(z_{\\alpha/2} + z_\\beta)^2(\\sigma_1^2 + \\sigma_2^2)}{\\delta^2}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Power curves for two-sample Z-test\n\n\n\n\n\n\n\n\n# Power curves for two-sample Z-test\ndelta_values &lt;- seq(0, 15, by = 0.5)\nsample_sizes &lt;- c(10, 20, 30, 50)\nalpha_power &lt;- 0.05\nsigma1_power &lt;- 12\nsigma2_power &lt;- 10\n\n# Function to calculate power for two-sample Z-test\ncalculate_power_two_sample &lt;- function(delta, n1, n2, sigma1, sigma2, alpha) {\n  se &lt;- sqrt(sigma1^2/n1 + sigma2^2/n2)\n  z_alpha2 &lt;- qnorm(1 - alpha/2)\n  \n  # Power for two-sided test\n  power &lt;- 1 - pnorm(z_alpha2 - delta/se) + pnorm(-z_alpha2 - delta/se)\n  return(power)\n}\n\n# Generate power curve data\npower_curve_data &lt;- data.table()\nfor(n_val in sample_sizes) {\n  for(delta in delta_values) {\n    power_val &lt;- calculate_power_two_sample(delta, n_val, n_val, sigma1_power, sigma2_power, alpha_power)\n    power_curve_data &lt;- rbind(power_curve_data, \n                             data.table(delta = delta, n = factor(n_val), power = power_val))\n  }\n}\n\n# Plot power curves\nggplot(data = power_curve_data, mapping = aes(x = delta, y = power, color = n)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = 12, y = 0.82, label = \"Power = 0.8\", hjust = 0) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(breaks = pretty_breaks(n = 6), limits = c(0, 1)) +\n  labs(\n    x = \"True Difference in Means (δ = |μ₁ - μ₂|)\",\n    y = \"Power (1 - β)\",\n    color = \"Sample Size\\n(each group)\",\n    title = \"Power Curves for Two-Sample Z-Test\",\n    subtitle = \"H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂, α = 0.05, σ₁ = 12, σ₂ = 10\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ₁ - μ₂ (σ₁, σ₂ known)\n\n\n\nA 100(1-α)% confidence interval for μ₁ - μ₂:\n(\\overline{x_1} - \\overline{x_2}) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Means (σ known):\"\n\n\n   Parameter Confidence_Level Difference Standard_Error Z_Critical Margin_Error\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;          &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   μ₁ - μ₂              95%       8.63          3.347       1.96        6.559\n   Lower_Bound Upper_Bound Width\n         &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:        2.07       15.19 13.12\n                                         Interpretation\n                                                 &lt;char&gt;\n1: 95% confident that μ₁ - μ₂ is between 2.07 and 15.19\n\n\n\n\n\n# Confidence interval for difference in means (sigma known)\nmargin_error_z &lt;- z_critical * pooled_se\nci_lower_z &lt;- (xbar1 - xbar2) - margin_error_z\nci_upper_z &lt;- (xbar1 - xbar2) + margin_error_z\n\nz_ci_results &lt;- data.table(\n  Parameter = \"μ₁ - μ₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(xbar1 - xbar2, 2),\n  Standard_Error = round(pooled_se, 3),\n  Z_Critical = round(z_critical, 3),\n  Margin_Error = round(margin_error_z, 3),\n  Lower_Bound = round(ci_lower_z, 2),\n  Upper_Bound = round(ci_upper_z, 2),\n  Width = round(ci_upper_z - ci_lower_z, 2),\n  Interpretation = paste(\"95% confident that μ₁ - μ₂ is between\", \n                        round(ci_lower_z, 2), \"and\", round(ci_upper_z, 2))\n)\n\nprint(\"95% Confidence Interval for Difference in Means (σ known):\")\nprint(z_ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo-Sample t-Test (σ₁, σ₂ unknown)\n\n\n\nWhen population variances are unknown, we must consider two cases:\nCase 1: Equal Variances (σ₁² = σ₂²)\n\nUse pooled variance:\n\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}\n\nTest statistic:\n\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\nDegrees of freedom: ν = n₁ + n₂ - 2\n\nCase 2: Unequal Variances (σ₁² ≠ σ₂²)\n\nWelch’s t-test\nTest statistic:\n\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\nDegrees of freedom:\n\n\\nu = \\frac{(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2})^2}{\\frac{S_1^4}{n_1^2(n_1-1)} + \\frac{S_2^4}{n_2^2(n_2-1)}}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the breaking strength of two types of plastic. Random samples give:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Plastic Strength Data Summary by Type:\"\n\n\n     type     n      Min       Q1   Median     Mean       Q3      Max        SD\n   &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;\n1: Type_A    15 150.6854 155.8163 167.2005 163.4680 169.8374 176.0622  8.599716\n2: Type_B    18 139.6099 152.5444 156.1428 158.8415 170.5125 178.5483 11.920081\n         Var  SE_Mean\n       &lt;num&gt;    &lt;num&gt;\n1:  73.95511 2.220437\n2: 142.08832 2.809590\n\n\n\n\n\n# Plastic strength comparison (two-sample t-test, sigma unknown)\nset.seed(456)\nplastic_data &lt;- data.table(\n  type = rep(c(\"Type_A\", \"Type_B\"), c(15, 18)),\n  strength = c(rnorm(15, mean = 162.5, sd = 8.2), rnorm(18, mean = 157.8, sd = 9.1))\n)\n\n# Summary statistics\nplastic_summary &lt;- plastic_data %&gt;% \n  fgroup_by(type) %&gt;% \n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength),\n    SE_Mean = fsd(strength) / sqrt(fnobs(strength))\n  )\n\nprint(\"Plastic Strength Data Summary by Type:\")\nprint(plastic_summary)\n\n# Write data to CSV for download\nfwrite(plastic_data, \"data/plastic_strength.csv\")\n\n\n\n\n\n\n\nFirst, test for equal variances, then perform appropriate t-test.\nManual Calculation (assuming equal variances):\nGiven: n₁ = 15, x̄₁ = 162.5, s₁ = 8.2, n₂ = 18, x̄₂ = 157.8, s₂ = 9.1\n\nPooled Variance: S_p^2 = \\frac{(15-1)(8.2)^2 + (18-1)(9.1)^2}{15 + 18 - 2} = \\frac{940.36 + 1408.57}{31} = 75.77\nTest Statistic: T_0 = \\frac{162.5 - 157.8}{8.71\\sqrt{\\frac{1}{15} + \\frac{1}{18}}} = \\frac{4.7}{3.01} = 1.56\nCritical Value: t₀.₀₂₅,₃₁ = 2.040\nDecision: |T₀| = 1.56 &lt; 2.040, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"F-test for equal variances: F = 0.52 , p-value = 0.2229\"\n\n\n[1] \"Two-Sample t-Test Results (Equal Variances):\"\n\n\n                             Test_Type    n1    n2  xbar1  xbar2    s1    s2\n                                &lt;char&gt; &lt;int&gt; &lt;int&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1: Two-sample t-test (equal variances)    15    18 163.47 158.84   8.6 11.92\n   Pooled_SD SE_Difference t_Statistic    df P_Value Alpha t_Critical\n       &lt;num&gt;         &lt;num&gt;       &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     10.55         3.689      1.2543    31  0.2191  0.05       2.04\n            Decision                         Conclusion\n              &lt;char&gt;                             &lt;char&gt;\n1: Fail to reject H0 No significant difference in means\n\n\n[1] \"R's t.test verification:\"\n\n\n\n    Two Sample t-test\n\ndata:  typeA_data$strength and typeB_data$strength\nt = 1.2543, df = 31, p-value = 0.2191\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.896371 12.149429\nsample estimates:\nmean of x mean of y \n 163.4680  158.8415 \n\n\n\n\n\n# Two-sample t-test with equal variances (manual and R calculations)\ntypeA_data &lt;- plastic_data[type == \"Type_A\"]\ntypeB_data &lt;- plastic_data[type == \"Type_B\"]\n\nn1_t &lt;- fnobs(typeA_data$strength)\nn2_t &lt;- fnobs(typeB_data$strength)\nxbar1_t &lt;- fmean(typeA_data$strength)\nxbar2_t &lt;- fmean(typeB_data$strength)\ns1_t &lt;- fsd(typeA_data$strength)\ns2_t &lt;- fsd(typeB_data$strength)\n\n# Test for equal variances first (F-test)\nf_stat_equal &lt;- s1_t^2 / s2_t^2\ndf1_f &lt;- n1_t - 1\ndf2_f &lt;- n2_t - 1\np_value_f &lt;- 2 * min(pf(f_stat_equal, df1_f, df2_f), 1 - pf(f_stat_equal, df1_f, df2_f))\n\nprint(paste(\"F-test for equal variances: F =\", round(f_stat_equal, 3), \", p-value =\", round(p_value_f, 4)))\n\n# Assuming equal variances (pooled variance)\nsp_squared &lt;- ((n1_t - 1) * s1_t^2 + (n2_t - 1) * s2_t^2) / (n1_t + n2_t - 2)\nsp &lt;- sqrt(sp_squared)\nse_pooled &lt;- sp * sqrt(1/n1_t + 1/n2_t)\n\n# Manual t-test calculation\nt_stat &lt;- (xbar1_t - xbar2_t) / se_pooled\ndf_t &lt;- n1_t + n2_t - 2\np_value_t &lt;- 2 * (1 - pt(abs(t_stat), df = df_t))\nt_critical &lt;- qt(1 - alpha/2, df = df_t)\n\n# Test results\nt_test_results &lt;- data.table(\n  Test_Type = \"Two-sample t-test (equal variances)\",\n  n1 = n1_t,\n  n2 = n2_t,\n  xbar1 = round(xbar1_t, 2),\n  xbar2 = round(xbar2_t, 2),\n  s1 = round(s1_t, 2),\n  s2 = round(s2_t, 2),\n  Pooled_SD = round(sp, 2),\n  SE_Difference = round(se_pooled, 3),\n  t_Statistic = round(t_stat, 4),\n  df = df_t,\n  P_Value = round(p_value_t, 4),\n  Alpha = alpha,\n  t_Critical = round(t_critical, 3),\n  Decision = ifelse(abs(t_stat) &gt; t_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_t &lt; alpha, \n                     \"Means are significantly different\", \n                     \"No significant difference in means\")\n)\n\nprint(\"Two-Sample t-Test Results (Equal Variances):\")\nprint(t_test_results)\n\n# Using R's built-in t.test for verification\nt_test_r &lt;- t.test(typeA_data$strength, typeB_data$strength, var.equal = TRUE)\nprint(\"R's t.test verification:\")\nprint(t_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Two-Sample t-Test\n\n\n\nFor equal sample sizes and equal variances:\nn \\approx \\frac{2(t_{\\alpha/2,\\nu} + t_{\\beta,\\nu})^2 s_p^2}{\\delta^2}\nwhere ν = 2n - 2 and s_p² is estimated from pilot data.\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ₁ - μ₂ (σ₁, σ₂ unknown)\n\n\n\nEqual Variances:\n(\\overline{x_1} - \\overline{x_2}) \\pm t_{\\alpha/2,n_1+n_2-2} \\cdot S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\nUnequal Variances:\n(\\overline{x_1} - \\overline{x_2}) \\pm t_{\\alpha/2,\\nu} \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Means (σ unknown):\"\n\n\n   Parameter Confidence_Level Difference Pooled_SD Standard_Error t_Critical\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;     &lt;num&gt;          &lt;num&gt;      &lt;num&gt;\n1:   μ₁ - μ₂              95%       4.63     10.55          3.689       2.04\n   Margin_Error Lower_Bound Upper_Bound Width\n          &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:        7.523        -2.9       12.15 15.05\n                                         Interpretation\n                                                 &lt;char&gt;\n1: 95% confident that μ₁ - μ₂ is between -2.9 and 12.15\n\n\n[1] \"Welch's t-test (unequal variances):\"\n\n\n\n    Welch Two Sample t-test\n\ndata:  typeA_data$strength and typeB_data$strength\nt = 1.2919, df = 30.446, p-value = 0.2061\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.682527 11.935585\nsample estimates:\nmean of x mean of y \n 163.4680  158.8415 \n\n\n\n\n\n# Confidence interval for difference in means (sigma unknown)\nmargin_error_t &lt;- t_critical * se_pooled\nci_lower_t &lt;- (xbar1_t - xbar2_t) - margin_error_t\nci_upper_t &lt;- (xbar1_t - xbar2_t) + margin_error_t\n\nt_ci_results &lt;- data.table(\n  Parameter = \"μ₁ - μ₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(xbar1_t - xbar2_t, 2),\n  Pooled_SD = round(sp, 2),\n  Standard_Error = round(se_pooled, 3),\n  t_Critical = round(t_critical, 3),\n  Margin_Error = round(margin_error_t, 3),\n  Lower_Bound = round(ci_lower_t, 2),\n  Upper_Bound = round(ci_upper_t, 2),\n  Width = round(ci_upper_t - ci_lower_t, 2),\n  Interpretation = paste(\"95% confident that μ₁ - μ₂ is between\", \n                        round(ci_lower_t, 2), \"and\", round(ci_upper_t, 2))\n)\n\nprint(\"95% Confidence Interval for Difference in Means (σ unknown):\")\nprint(t_ci_results)\n\n# Welch's t-test (unequal variances) for comparison\nwelch_test &lt;- t.test(typeA_data$strength, typeB_data$strength, var.equal = FALSE)\nprint(\"Welch's t-test (unequal variances):\")\nprint(welch_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePaired t-Test\n\n\n\nWhen observations come in natural pairs (before/after, matched subjects, etc.), use the paired t-test:\n\nCalculate differences: d_i = x₁ᵢ - x₂ᵢ\nTest statistic:\n\nT_0 = \\frac{\\overline{d} - \\mu_d}{S_d/\\sqrt{n}}\n\nDegrees of freedom: ν = n - 1\n\nWhere:\n\n\\overline{d} = \\frac{1}{n}\\sum_{i=1}^n d_i\nS_d^2 = \\frac{1}{n-1}\\sum_{i=1}^n (d_i - \\overline{d})^2\n\nAssumptions:\n\nDifferences are normally distributed\nPairs are independent\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA manufacturing process improvement is tested on 12 production lines. Measure output before and after the improvement:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Process Improvement Paired Data Summary:\"\n\n\n       n Mean_Before Mean_After Mean_Difference SD_Before SD_After\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;           &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:    12    81.70922   89.14206        7.432843   6.36605 4.794191\n   SD_Difference SE_Difference Min_Difference Max_Difference\n           &lt;num&gt;         &lt;num&gt;          &lt;num&gt;          &lt;num&gt;\n1:      6.906354      1.993693      -2.023308        19.6769\n\n\n[1] \"Individual Differences:\"\n\n\n    line_id   before    after difference\n      &lt;int&gt;    &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n 1:       1 89.19277 88.91711 -0.2756677\n 2:       2 66.91386 86.59075 19.6768963\n 3:       3 84.84256 97.20931 12.3667432\n 4:       4 86.46512 84.44181 -2.0233083\n 5:       5 82.10919 93.42153 11.3123453\n 6:       6 81.12413 85.69777  4.5736465\n 7:       7 79.66950 91.82031 12.1508159\n 8:       8 83.60425 84.41997  0.8157253\n 9:       9 76.91232 84.98462  8.0723014\n10:      10 90.91757 95.37602  4.4584503\n11:      11 81.78163 83.81734  2.0357137\n12:      12 76.97776 93.00821 16.0304489\n\n\n\n\n\n# Process improvement paired t-test example\nset.seed(789)\nprocess_data &lt;- data.table(\n  line_id = 1:12,\n  before = rnorm(12, mean = 85, sd = 8),\n  after = rnorm(12, mean = 90.25, sd = 7.5)\n)\n\n# Calculate differences\nprocess_data[, difference := after - before]\n\n# Summary statistics for paired data\npaired_summary &lt;- process_data %&gt;% \n  fsummarise(\n    n = fnobs(difference),\n    Mean_Before = fmean(before),\n    Mean_After = fmean(after),\n    Mean_Difference = fmean(difference),\n    SD_Before = fsd(before),\n    SD_After = fsd(after),\n    SD_Difference = fsd(difference),\n    SE_Difference = fsd(difference) / sqrt(fnobs(difference)),\n    Min_Difference = fmin(difference),\n    Max_Difference = fmax(difference)\n  )\n\nprint(\"Process Improvement Paired Data Summary:\")\nprint(paired_summary)\n\n# Individual differences\ndifferences_table &lt;- process_data[, .(line_id, before, after, difference)]\nprint(\"Individual Differences:\")\nprint(differences_table)\n\n# Write data to CSV for download\nfwrite(process_data, \"data/process_improvement.csv\")\n\n\n\n\n\n\n\nTest H₀: μ_d = 0 vs H₁: μ_d &gt; 0 at α = 0.05.\nManual Calculation:\nCalculate differences: d_i = After_i - Before_i\nIf d̄ = 5.25 and s_d = 3.2:\n\nTest Statistic: T_0 = \\frac{5.25 - 0}{3.2/\\sqrt{12}} = \\frac{5.25}{0.924} = 5.68\nCritical Value: t₀.₀₅,₁₁ = 1.796\nDecision: T₀ = 5.68 &gt; 1.796, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Paired t-Test Results:\"\n\n\n       Test_Type n_pairs Mean_Difference SD_Difference SE_Difference\n          &lt;char&gt;   &lt;int&gt;           &lt;num&gt;         &lt;num&gt;         &lt;num&gt;\n1: Paired t-test      12           7.433         6.906         1.994\n   Null_Difference t_Statistic    df P_Value Alpha t_Critical  Decision\n             &lt;num&gt;       &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:               0      3.7282    11  0.0017  0.05      1.796 Reject H0\n                         Conclusion\n                             &lt;char&gt;\n1: Significant improvement detected\n\n\n[1] \"R's paired t.test verification:\"\n\n\n\n    Paired t-test\n\ndata:  process_data$after and process_data$before\nt = 3.7282, df = 11, p-value = 0.001667\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 3.8524    Inf\nsample estimates:\nmean difference \n       7.432843 \n\n\n[1] \"95% CI for mean difference: [ 3.045 ,  11.821 ]\"\n\n\n\n\n\n# Paired t-test (manual and R calculations)\nn_paired &lt;- fnobs(process_data$difference)\nd_bar &lt;- fmean(process_data$difference)\ns_d &lt;- fsd(process_data$difference)\nmu_d0 &lt;- 0  # Null hypothesis: no difference\n\n# Manual calculation\nt_stat_paired &lt;- (d_bar - mu_d0) / (s_d / sqrt(n_paired))\ndf_paired &lt;- n_paired - 1\np_value_paired &lt;- 1 - pt(t_stat_paired, df = df_paired)  # One-sided upper test\nt_critical_paired &lt;- qt(1 - alpha, df = df_paired)\n\n# Test results\npaired_test_results &lt;- data.table(\n  Test_Type = \"Paired t-test\",\n  n_pairs = n_paired,\n  Mean_Difference = round(d_bar, 3),\n  SD_Difference = round(s_d, 3),\n  SE_Difference = round(s_d / sqrt(n_paired), 3),\n  Null_Difference = mu_d0,\n  t_Statistic = round(t_stat_paired, 4),\n  df = df_paired,\n  P_Value = round(p_value_paired, 4),\n  Alpha = alpha,\n  t_Critical = round(t_critical_paired, 3),\n  Decision = ifelse(t_stat_paired &gt; t_critical_paired, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_paired &lt; alpha, \n                     \"Significant improvement detected\", \n                     \"No significant improvement\")\n)\n\nprint(\"Paired t-Test Results:\")\nprint(paired_test_results)\n\n# Using R's paired t.test for verification\npaired_test_r &lt;- t.test(process_data$after, process_data$before, paired = TRUE, alternative = \"greater\")\nprint(\"R's paired t.test verification:\")\nprint(paired_test_r)\n\n# Confidence interval for mean difference\nt_critical_ci_paired &lt;- qt(1 - alpha/2, df = df_paired)\nmargin_error_paired &lt;- t_critical_ci_paired * (s_d / sqrt(n_paired))\nci_lower_paired &lt;- d_bar - margin_error_paired\nci_upper_paired &lt;- d_bar + margin_error_paired\n\nprint(paste(\"95% CI for mean difference: [\", round(ci_lower_paired, 3), \", \", round(ci_upper_paired, 3), \"]\"))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Paired t-test visualization\n\n\n\n\n\n\n\n\n# Paired t-test visualization\n\n# Create long format for plotting\nprocess_long &lt;- process_data %&gt;%\n  pivot_longer(cols = c(before, after), names_to = \"time\", values_to = \"output\") %&gt;%\n  data.table()\n\n# Before/After comparison plot\np1 &lt;- ggplot(data = process_long, mapping = aes(x = time, y = output, group = line_id)) +\n  geom_line(color = \"gray\", alpha = 0.7) +\n  geom_point(mapping = aes(color = time), size = 3) +\n  scale_color_manual(values = c(\"before\" = \"red\", \"after\" = \"blue\")) +\n  labs(\n    x = \"Time Period\",\n    y = \"Output\",\n    title = \"Before/After Process Improvement\",\n    color = \"Period\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Differences histogram\np2 &lt;- ggplot(data = process_data, mapping = aes(x = difference)) +\n  geom_histogram(bins = 8, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = d_bar, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"solid\", linewidth = 1) +\n  labs(\n    x = \"Difference (After - Before)\",\n    y = \"Frequency\",\n    title = \"Distribution of Differences\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteF-Test for Equality of Variances\n\n\n\nTo test H₀: σ₁² = σ₂² vs H₁: σ₁² ≠ σ₂², use:\nF_0 = \\frac{S_1^2}{S_2^2}\nUnder H₀, F₀ ~ F_{n₁-1, n₂-1}\nConvention: Put the larger sample variance in the numerator so F₀ ≥ 1.\nProperties of F-distribution:\n\nAlways positive\nRight-skewed\nApproaches 1 as both df → ∞\nF₁₋α,ν₁,ν₂ = 1/F_α,ν₂,ν₁\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the variability in measurements from two instruments:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Instrument Comparison Data Summary:\"\n\n\n     instrument     n      Min       Q1   Median     Mean       Q3      Max\n         &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Instrument_1    16 23.17625 24.45219 25.33319 25.39453 25.69963 28.86313\n2: Instrument_2    21 22.07273 24.18302 25.35367 25.00446 25.81188 26.85136\n         SD      Var\n      &lt;num&gt;    &lt;num&gt;\n1: 1.522413 2.317741\n2: 1.359890 1.849302\n\n\n\n\n\n# Instrument comparison for F-test (variance comparison)\nset.seed(321)\ninstrument_data &lt;- data.table(\n  instrument = rep(c(\"Instrument_1\", \"Instrument_2\"), c(16, 21)),\n  measurement = c(rnorm(16, mean = 25, sd = sqrt(2.5)), rnorm(21, mean = 25.2, sd = sqrt(1.8)))\n)\n\n# Summary statistics\ninstrument_summary &lt;- instrument_data %&gt;% \n  fgroup_by(instrument) %&gt;% \n  fsummarise(\n    n = fnobs(measurement),\n    Min = fmin(measurement),\n    Q1 = fquantile(measurement, 0.25),\n    Median = fmedian(measurement),\n    Mean = fmean(measurement),\n    Q3 = fquantile(measurement, 0.75),\n    Max = fmax(measurement),\n    SD = fsd(measurement),\n    Var = fvar(measurement)\n  )\n\nprint(\"Instrument Comparison Data Summary:\")\nprint(instrument_summary)\n\n# Write data to CSV for download\nfwrite(instrument_data, \"data/instrument_comparison.csv\")\n\n\n\n\n\n\n\nTest H₀: σ₁² = σ₂² vs H₁: σ₁² ≠ σ₂² at α = 0.05.\nManual Calculation:\nGiven: n₁ = 16, s₁² = 2.5, n₂ = 21, s₂² = 1.8\n\nTest Statistic: F_0 = \\frac{2.5}{1.8} = 1.39\nCritical Value: F₀.₀₂₅,₁₅,₂₀ = 2.57\nDecision: F₀ = 1.39 &lt; 2.57, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"F-Test Results for Equality of Variances:\"\n\n\n                          Test_Type    n1    n2 s1_squared s2_squared\n                             &lt;char&gt; &lt;int&gt; &lt;int&gt;      &lt;num&gt;      &lt;num&gt;\n1: F-test for equality of variances    16    21     2.3177     1.8493\n   Larger_Variance F_Statistic   df1   df2 P_Value Alpha F_Critical\n            &lt;char&gt;       &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:    Instrument_1      1.2533    15    20   0.627  0.05      2.573\n            Decision                             Conclusion\n              &lt;char&gt;                                 &lt;char&gt;\n1: Fail to reject H0 No significant difference in variances\n\n\n[1] \"R's var.test verification:\"\n\n\n\n    F test to compare two variances\n\ndata:  inst1_data$measurement and inst2_data$measurement\nF = 1.2533, num df = 15, denom df = 20, p-value = 0.627\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4870809 3.4539881\nsample estimates:\nratio of variances \n          1.253306 \n\n\n\n\n\n# F-test for equality of variances\ninst1_data &lt;- instrument_data[instrument == \"Instrument_1\"]\ninst2_data &lt;- instrument_data[instrument == \"Instrument_2\"]\n\nn1_f &lt;- fnobs(inst1_data$measurement)\nn2_f &lt;- fnobs(inst2_data$measurement)\ns1_f_squared &lt;- fvar(inst1_data$measurement)\ns2_f_squared &lt;- fvar(inst2_data$measurement)\n\n# F-test (put larger variance in numerator)\nif(s1_f_squared &gt;= s2_f_squared) {\n  f_stat &lt;- s1_f_squared / s2_f_squared\n  df1_f_test &lt;- n1_f - 1\n  df2_f_test &lt;- n2_f - 1\n  larger_var &lt;- \"Instrument_1\"\n} else {\n  f_stat &lt;- s2_f_squared / s1_f_squared\n  df1_f_test &lt;- n2_f - 1\n  df2_f_test &lt;- n1_f - 1\n  larger_var &lt;- \"Instrument_2\"\n}\n\n# Calculate p-value for two-sided test\np_value_f_test &lt;- 2 * (1 - pf(f_stat, df1_f_test, df2_f_test))\nf_critical &lt;- qf(1 - alpha/2, df1_f_test, df2_f_test)\n\n# Test results\nf_test_results &lt;- data.table(\n  Test_Type = \"F-test for equality of variances\",\n  n1 = n1_f,\n  n2 = n2_f,\n  s1_squared = round(s1_f_squared, 4),\n  s2_squared = round(s2_f_squared, 4),\n  Larger_Variance = larger_var,\n  F_Statistic = round(f_stat, 4),\n  df1 = df1_f_test,\n  df2 = df2_f_test,\n  P_Value = round(p_value_f_test, 4),\n  Alpha = alpha,\n  F_Critical = round(f_critical, 3),\n  Decision = ifelse(f_stat &gt; f_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_f_test &lt; alpha, \n                     \"Variances are significantly different\", \n                     \"No significant difference in variances\")\n)\n\nprint(\"F-Test Results for Equality of Variances:\")\nprint(f_test_results)\n\n# Using R's var.test for verification\nvar_test_r &lt;- var.test(inst1_data$measurement, inst2_data$measurement)\nprint(\"R's var.test verification:\")\nprint(var_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for σ₁²/σ₂²\n\n\n\nA 100(1-α)% confidence interval for σ₁²/σ₂²:\n\\frac{s_1^2/s_2^2}{F_{\\alpha/2,n_1-1,n_2-1}} \\leq \\frac{\\sigma_1^2}{\\sigma_2^2} \\leq \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2,n_1-1,n_2-1}}\nNote: F₁₋α/₂,ν₁,ν₂ = 1/F_α/₂,ν₂,ν₁\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Ratio of Variances:\"\n\n\n   Parameter Confidence_Level Sample_Ratio Lower_Bound Upper_Bound  Width\n      &lt;char&gt;           &lt;char&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:   σ₁²/σ₂²              95%       1.2533      0.4871       3.454 2.9669\n                                           Interpretation\n                                                   &lt;char&gt;\n1: 95% confident that σ₁²/σ₂² is between 0.4871 and 3.454\n\n\n\n\n\n# Confidence interval for ratio of variances\nf_alpha2_lower &lt;- qf(alpha/2, df1_f_test, df2_f_test)\nf_alpha2_upper &lt;- qf(1 - alpha/2, df1_f_test, df2_f_test)\n\n# Note: For CI, use original ratio without putting larger in numerator\nratio_original &lt;- s1_f_squared / s2_f_squared\nci_lower_f &lt;- ratio_original / f_alpha2_upper\nci_upper_f &lt;- ratio_original / f_alpha2_lower\n\nvariance_ratio_ci &lt;- data.table(\n  Parameter = \"σ₁²/σ₂²\",\n  Confidence_Level = \"95%\",\n  Sample_Ratio = round(ratio_original, 4),\n  Lower_Bound = round(ci_lower_f, 4),\n  Upper_Bound = round(ci_upper_f, 4),\n  Width = round(ci_upper_f - ci_lower_f, 4),\n  Interpretation = paste(\"95% confident that σ₁²/σ₂² is between\", \n                        round(ci_lower_f, 4), \"and\", round(ci_upper_f, 4))\n)\n\nprint(\"95% Confidence Interval for Ratio of Variances:\")\nprint(variance_ratio_ci)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTwo-Sample Z-Test for Proportions\n\n\n\nTo test H₀: p₁ = p₂, use the pooled estimate:\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\nTest Statistic:\nZ_0 = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\nConditions: n₁p̂ ≥ 5, n₁(1-p̂) ≥ 5, n₂p̂ ≥ 5, n₂(1-p̂) ≥ 5\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare defect rates between two production lines:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Defect Rate Comparison Summary:\"\n\n\n     line sample_size defects  prop\n   &lt;char&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;\n1: Line_1         200      16 0.080\n2: Line_2         250      14 0.056\n\n\n[1] \"Overall Summary:\"\n\n\n   Total_Items Total_Defects Line1_Rate Line2_Rate Overall_Rate\n         &lt;num&gt;         &lt;num&gt;      &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:         450            30       0.08      0.056   0.06666667\n\n\n\n\n\n# Defect rate comparison (two-sample proportion test)\ndefect_comparison_data &lt;- data.table(\n  line = c(\"Line_1\", \"Line_2\"),\n  sample_size = c(200, 250),\n  defects = c(16, 14),\n  prop = c(16/200, 14/250)\n)\n\n# Summary\ndefect_summary &lt;- defect_comparison_data %&gt;% \n  fsummarise(\n    Total_Items = fsum(sample_size),\n    Total_Defects = fsum(defects),\n    Line1_Rate = defects[1] / sample_size[1],\n    Line2_Rate = defects[2] / sample_size[2],\n    Overall_Rate = fsum(defects) / fsum(sample_size)\n  )\n\nprint(\"Defect Rate Comparison Summary:\")\nprint(defect_comparison_data)\nprint(\"Overall Summary:\")\nprint(defect_summary)\n\n# Write data to CSV for download\nfwrite(defect_comparison_data, \"data/defect_comparison.csv\")\n\n\n\n\n\n\n\nTest H₀: p₁ = p₂ vs H₁: p₁ ≠ p₂ at α = 0.05.\nManual Calculation:\nGiven: n₁ = 200, x₁ = 16, n₂ = 250, x₂ = 14\n\nSample Proportions:\n\np̂₁ = 16/200 = 0.08\np̂₂ = 14/250 = 0.056\n\nPooled Proportion: \\hat{p} = \\frac{16 + 14}{200 + 250} = \\frac{30}{450} = 0.067\nTest Statistic: Z_0 = \\frac{0.08 - 0.056}{\\sqrt{0.067(0.933)(\\frac{1}{200} + \\frac{1}{250})}} = \\frac{0.024}{0.024} = 1.00\nDecision: |Z₀| = 1.00 &lt; 1.96, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Large Sample Conditions:\"\n\n\n     Condition     Value    Met\n        &lt;char&gt;     &lt;num&gt; &lt;lgcl&gt;\n1:     n₁p̂ ≥ 5  13.33333   TRUE\n2: n₁(1-p̂) ≥ 5 186.66667   TRUE\n3:     n₂p̂ ≥ 5  16.66667   TRUE\n4: n₂(1-p̂) ≥ 5 233.33333   TRUE\n\n\n[1] \"Two-Sample Proportion Test Results:\"\n\n\n                    Test_Type    n1    n2    x1    x2 p1_hat p2_hat p_pooled\n                       &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n1: Two-sample proportion test   200   250    16    14   0.08  0.056   0.0667\n   SE_Difference Z_Statistic P_Value Alpha Z_Critical          Decision\n           &lt;num&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;            &lt;char&gt;\n1:        0.0237      1.0142  0.3105  0.05       1.96 Fail to reject H0\n                                 Conclusion\n                                     &lt;char&gt;\n1: No significant difference in proportions\n\n\n[1] \"R's prop.test verification:\"\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x1_prop, x2_prop) out of c(n1_prop, n2_prop)\nX-squared = 0.67902, df = 1, p-value = 0.4099\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.02768004  0.07568004\nsample estimates:\nprop 1 prop 2 \n 0.080  0.056 \n\n\n\n\n\n# Two-sample proportion test\nn1_prop &lt;- defect_comparison_data[line == \"Line_1\", sample_size]\nn2_prop &lt;- defect_comparison_data[line == \"Line_2\", sample_size]\nx1_prop &lt;- defect_comparison_data[line == \"Line_1\", defects]\nx2_prop &lt;- defect_comparison_data[line == \"Line_2\", defects]\np1_hat &lt;- x1_prop / n1_prop\np2_hat &lt;- x2_prop / n2_prop\n\n# Pooled proportion\np_pooled &lt;- (x1_prop + x2_prop) / (n1_prop + n2_prop)\n\n# Check conditions\nconditions &lt;- data.table(\n  Condition = c(\"n₁p̂ ≥ 5\", \"n₁(1-p̂) ≥ 5\", \"n₂p̂ ≥ 5\", \"n₂(1-p̂) ≥ 5\"),\n  Value = c(n1_prop * p_pooled, n1_prop * (1 - p_pooled), \n           n2_prop * p_pooled, n2_prop * (1 - p_pooled)),\n  Met = c(n1_prop * p_pooled &gt;= 5, n1_prop * (1 - p_pooled) &gt;= 5,\n         n2_prop * p_pooled &gt;= 5, n2_prop * (1 - p_pooled) &gt;= 5)\n)\n\nprint(\"Large Sample Conditions:\")\nprint(conditions)\n\n# Manual calculation\nse_prop &lt;- sqrt(p_pooled * (1 - p_pooled) * (1/n1_prop + 1/n2_prop))\nz_stat_prop &lt;- (p1_hat - p2_hat) / se_prop\np_value_prop &lt;- 2 * (1 - pnorm(abs(z_stat_prop)))  # Two-sided test\nz_critical_prop &lt;- qnorm(1 - alpha/2)\n\n# Test results\nprop_test_results &lt;- data.table(\n  Test_Type = \"Two-sample proportion test\",\n  n1 = n1_prop,\n  n2 = n2_prop,\n  x1 = x1_prop,\n  x2 = x2_prop,\n  p1_hat = round(p1_hat, 4),\n  p2_hat = round(p2_hat, 4),\n  p_pooled = round(p_pooled, 4),\n  SE_Difference = round(se_prop, 4),\n  Z_Statistic = round(z_stat_prop, 4),\n  P_Value = round(p_value_prop, 4),\n  Alpha = alpha,\n  Z_Critical = round(z_critical_prop, 3),\n  Decision = ifelse(abs(z_stat_prop) &gt; z_critical_prop, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_prop &lt; alpha, \n                     \"Proportions are significantly different\", \n                     \"No significant difference in proportions\")\n)\n\nprint(\"Two-Sample Proportion Test Results:\")\nprint(prop_test_results)\n\n# Using R's prop.test for verification\nprop_test_r &lt;- prop.test(x = c(x1_prop, x2_prop), n = c(n1_prop, n2_prop))\nprint(\"R's prop.test verification:\")\nprint(prop_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Two-Proportion Test\n\n\n\nFor equal sample sizes (n₁ = n₂ = n) and difference δ = |p₁ - p₂|:\nn = \\frac{[z_{\\alpha/2}\\sqrt{2\\bar{p}(1-\\bar{p})} + z_\\beta\\sqrt{p_1(1-p_1) + p_2(1-p_2)}]^2}{\\delta^2}\nwhere \\bar{p} = \\frac{p_1 + p_2}{2}\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for p₁ - p₂\n\n\n\nA 100(1-α)% confidence interval for p₁ - p₂:\n(\\hat{p_1} - \\hat{p_2}) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p_1}(1-\\hat{p_1})}{n_1} + \\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}}\nNote: Use individual sample proportions, not the pooled estimate.\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Proportions:\"\n\n\n   Parameter Confidence_Level Difference Standard_Error Z_Critical Margin_Error\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;          &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   p₁ - p₂              95%      0.024         0.0241       1.96       0.0472\n   Lower_Bound Upper_Bound  Width\n         &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     -0.0232      0.0712 0.0944\n                                             Interpretation\n                                                     &lt;char&gt;\n1: 95% confident that p₁ - p₂ is between -0.0232 and 0.0712\n\n\n\n\n\n# Confidence interval for difference in proportions\nse_diff_ci &lt;- sqrt(p1_hat * (1 - p1_hat) / n1_prop + p2_hat * (1 - p2_hat) / n2_prop)\nmargin_error_prop &lt;- z_critical_prop * se_diff_ci\nci_lower_prop &lt;- (p1_hat - p2_hat) - margin_error_prop\nci_upper_prop &lt;- (p1_hat - p2_hat) + margin_error_prop\n\nprop_ci_results &lt;- data.table(\n  Parameter = \"p₁ - p₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(p1_hat - p2_hat, 4),\n  Standard_Error = round(se_diff_ci, 4),\n  Z_Critical = round(z_critical_prop, 3),\n  Margin_Error = round(margin_error_prop, 4),\n  Lower_Bound = round(ci_lower_prop, 4),\n  Upper_Bound = round(ci_upper_prop, 4),\n  Width = round(ci_upper_prop - ci_lower_prop, 4),\n  Interpretation = paste(\"95% confident that p₁ - p₂ is between\", \n                        round(ci_lower_prop, 4), \"and\", round(ci_upper_prop, 4))\n)\n\nprint(\"95% Confidence Interval for Difference in Proportions:\")\nprint(prop_ci_results)\n\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 1: Summary of two-sample inference procedures\n\n\n\n\nSummary of Two-Sample Inference Procedures\n\n\nParameter\nTest_Statistic\nDistribution\nConfidence_Interval\nAssumptions\n\n\n\n\nμ₁ - μ₂ (σ known)\nZ = (x̄₁-x̄₂)/√(σ₁²/n₁+σ₂²/n₂)\nN(0,1)\n(x̄₁-x̄₂) ± z_{α/2}√(σ₁²/n₁+σ₂²/n₂)\nNormal populations, σ known\n\n\nμ₁ - μ₂ (σ unknown, equal)\nt = (x̄₁-x̄₂)/(sp√(1/n₁+1/n₂))\nt(n₁+n₂-2)\n(x̄₁-x̄₂) ± t_{α/2}sp√(1/n₁+1/n₂)\nNormal populations, σ₁²=σ₂²\n\n\nμ₁ - μ₂ (σ unknown, unequal)\nt = (x̄₁-x̄₂)/√(s₁²/n₁+s₂²/n₂)\nt(ν)\n(x̄₁-x̄₂) ± t_{α/2}√(s₁²/n₁+s₂²/n₂)\nNormal populations, σ₁²≠σ₂²\n\n\nμ_d (paired)\nt = d̄/(sd/√n)\nt(n-1)\nd̄ ± t_{α/2}sd/√n\nNormal differences\n\n\nσ₁²/σ₂²\nF = s₁²/s₂²\nF(n₁-1,n₂-1)\n(s₁²/s₂²)/F_{α/2} to (s₁²/s₂²)/F_{1-α/2}\nNormal populations\n\n\np₁ - p₂\nZ = (p̂₁-p̂₂)/√(p̂(1-p̂)(1/n₁+1/n₂))\nN(0,1)\n(p̂₁-p̂₂) ± z_{α/2}√(p̂₁(1-p̂₁)/n₁+p̂₂(1-p̂₂)/n₂)\nLarge samples, np≥5\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Summary table of two-sample inference procedures\ninference_procedures &lt;- data.table(\n  Parameter = c(\"μ₁ - μ₂ (σ known)\", \"μ₁ - μ₂ (σ unknown, equal)\", \n               \"μ₁ - μ₂ (σ unknown, unequal)\", \"μ_d (paired)\",\n               \"σ₁²/σ₂²\", \"p₁ - p₂\"),\n  Test_Statistic = c(\"Z = (x̄₁-x̄₂)/√(σ₁²/n₁+σ₂²/n₂)\", \n                    \"t = (x̄₁-x̄₂)/(sp√(1/n₁+1/n₂))\",\n                    \"t = (x̄₁-x̄₂)/√(s₁²/n₁+s₂²/n₂)\",\n                    \"t = d̄/(sd/√n)\",\n                    \"F = s₁²/s₂²\",\n                    \"Z = (p̂₁-p̂₂)/√(p̂(1-p̂)(1/n₁+1/n₂))\"),\n  Distribution = c(\"N(0,1)\", \"t(n₁+n₂-2)\", \"t(ν)\", \"t(n-1)\", \"F(n₁-1,n₂-1)\", \"N(0,1)\"),\n  Confidence_Interval = c(\"(x̄₁-x̄₂) ± z_{α/2}√(σ₁²/n₁+σ₂²/n₂)\",\n                         \"(x̄₁-x̄₂) ± t_{α/2}sp√(1/n₁+1/n₂)\",\n                         \"(x̄₁-x̄₂) ± t_{α/2}√(s₁²/n₁+s₂²/n₂)\",\n                         \"d̄ ± t_{α/2}sd/√n\",\n                         \"(s₁²/s₂²)/F_{α/2} to (s₁²/s₂²)/F_{1-α/2}\",\n                         \"(p̂₁-p̂₂) ± z_{α/2}√(p̂₁(1-p̂₁)/n₁+p̂₂(1-p̂₂)/n₂)\"),\n  Assumptions = c(\"Normal populations, σ known\", \"Normal populations, σ₁²=σ₂²\",\n                 \"Normal populations, σ₁²≠σ₂²\", \"Normal differences\",\n                 \"Normal populations\", \"Large samples, np≥5\")\n)\n\nkbl(inference_procedures, \n    caption = \"Summary of Two-Sample Inference Procedures\") %&gt;%\n  kable_styling() %&gt;%\n  column_spec(2, width = \"3.5cm\") %&gt;%\n  column_spec(3, width = \"2cm\") %&gt;%\n  column_spec(4, width = \"3.5cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIntroduction to ANOVA\n\n\n\nWhen comparing more than two groups, multiple t-tests lead to inflated Type I error rates. Analysis of Variance (ANOVA) provides a single test for:\nH₀: μ₁ = μ₂ = … = μₐ\nH₁: At least one mean differs\nOne-Way ANOVA Model:\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\nwhere:\n\ny_{ij} = j-th observation in i-th treatment\nμ = overall mean\nτᵢ = effect of i-th treatment\nεᵢⱼ \\sim N(0, σ²)\n\nF-Statistic:\nF_0 = \\frac{MS_{Treatments}}{MS_{Error}} = \\frac{SS_{Treatments}/(a-1)}{SS_{Error}/(N-a)}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the yield from four different chemical processes:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Process Yield Data Summary by Process:\"\n\n\n     process     n      Min       Q1   Median     Mean       Q3      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Process_A     6 71.95873 73.73665 75.00213 75.90529 77.54712 81.75850\n2: Process_B     6 74.64733 76.58680 79.26790 78.91072 81.56075 82.26307\n3: Process_C     6 68.06958 68.56533 70.23732 70.18130 71.91936 72.05552\n4: Process_D     6 76.94869 79.26128 81.26787 80.64549 82.49767 82.88851\n         SD       Var\n      &lt;num&gt;     &lt;num&gt;\n1: 3.561280 12.682715\n2: 3.180471 10.115399\n3: 1.927890  3.716761\n4: 2.362796  5.582806\n\n\n[1] \"Overall Summary:\"\n\n\n   Total_n Overall_Mean Overall_SD Overall_Var Between_Process_Range\n     &lt;int&gt;        &lt;num&gt;      &lt;num&gt;       &lt;num&gt;                 &lt;num&gt;\n1:      24      76.4107   4.845237    23.47632               10.4642\n\n\n\n\n\n# One-way ANOVA example (comparing multiple processes)\nset.seed(654)\nprocess_yield_data &lt;- data.table(\n  process = rep(c(\"Process_A\", \"Process_B\", \"Process_C\", \"Process_D\"), each = 6),\n  yield = c(rnorm(6, mean = 75, sd = 4),   # Process A\n           rnorm(6, mean = 78, sd = 4),   # Process B  \n           rnorm(6, mean = 72, sd = 4),   # Process C\n           rnorm(6, mean = 80, sd = 4))   # Process D\n)\n\n# Summary statistics by process\nyield_summary &lt;- process_yield_data %&gt;% \n  fgroup_by(process) %&gt;% \n  fsummarise(\n    n = fnobs(yield),\n    Min = fmin(yield),\n    Q1 = fquantile(yield, 0.25),\n    Median = fmedian(yield),\n    Mean = fmean(yield),\n    Q3 = fquantile(yield, 0.75),\n    Max = fmax(yield),\n    SD = fsd(yield),\n    Var = fvar(yield)\n  )\n\nprint(\"Process Yield Data Summary by Process:\")\nprint(yield_summary)\n\n# Overall summary\noverall_summary &lt;- process_yield_data %&gt;% \n  fsummarise(\n    Total_n = fnobs(yield),\n    Overall_Mean = fmean(yield),\n    Overall_SD = fsd(yield),\n    Overall_Var = fvar(yield),\n    Between_Process_Range = fmax(yield_summary$Mean) - fmin(yield_summary$Mean)\n  )\n\nprint(\"Overall Summary:\")\nprint(overall_summary)\n\n# Write data to CSV for download\nfwrite(process_yield_data, \"data/process_yield.csv\")\n\n\n\n\n\n\n\nANOVA Table:\n\n\n\nSource\nSS\ndf\nMS\nF\nP-value\n\n\n\n\nTotal\nSSₜ\nN-1\n\n\n\n\n\nTreatments\nSSₜᵣ\na-1\nMSₜᵣ\nF₀\nP\n\n\nError\nSSₑ\nN-a\nMSₑ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"One-Way ANOVA Table:\"\n\n\n       Source     SS    df     MS F_Statistic P_Value\n       &lt;char&gt;  &lt;num&gt; &lt;num&gt;  &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Treatments 379.47     3 126.49      15.763       0\n2:      Error 160.49    20   8.02          NA      NA\n3:      Total 539.96    23     NA          NA      NA\n\n\n[1] \"ANOVA Test Results:\"\n\n\n   F_Statistic F_Critical P_Value Alpha  Decision\n         &lt;num&gt;      &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;char&gt;\n1:      15.763      3.098       0  0.05 Reject H0\n                          Conclusion\n                              &lt;char&gt;\n1: At least one process mean differs\n\n\n[1] \"R's ANOVA verification:\"\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nprocess      3  379.5  126.49   15.76 1.7e-05 ***\nResiduals   20  160.5    8.02                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# One-way ANOVA calculations (manual and R)\n# ANOVA calculations\na &lt;- length(unique(process_yield_data$process))  # number of treatments\nN &lt;- fnobs(process_yield_data$yield)  # total observations\nn_per_group &lt;- N / a  # observations per group (balanced design)\n\n# Calculate sum of squares\ngrand_mean &lt;- fmean(process_yield_data$yield)\n\n# Treatment sum of squares (SST)\ngroup_means &lt;- yield_summary$Mean\nSST &lt;- n_per_group * sum((group_means - grand_mean)^2)\n\n# Error sum of squares (SSE) \nSSE &lt;- sum(yield_summary$Var * (yield_summary$n - 1))\n\n# Total sum of squares (SS_Total)\nSS_Total &lt;- sum((process_yield_data$yield - grand_mean)^2)\n\n# Degrees of freedom\ndf_treatments &lt;- a - 1\ndf_error &lt;- N - a\ndf_total &lt;- N - 1\n\n# Mean squares\nMS_treatments &lt;- SST / df_treatments\nMS_error &lt;- SSE / df_error\n\n# F-statistic\nf_stat_anova &lt;- MS_treatments / MS_error\np_value_anova &lt;- 1 - pf(f_stat_anova, df_treatments, df_error)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Treatments\", \"Error\", \"Total\"),\n  SS = c(round(SST, 2), round(SSE, 2), round(SS_Total, 2)),\n  df = c(df_treatments, df_error, df_total),\n  MS = c(round(MS_treatments, 2), round(MS_error, 2), NA),\n  F_Statistic = c(round(f_stat_anova, 4), NA, NA),\n  P_Value = c(round(p_value_anova, 4), NA, NA)\n)\n\nprint(\"One-Way ANOVA Table:\")\nprint(anova_table)\n\n# ANOVA conclusion\nalpha_anova &lt;- 0.05\nf_critical_anova &lt;- qf(1 - alpha_anova, df_treatments, df_error)\nanova_conclusion &lt;- data.table(\n  F_Statistic = round(f_stat_anova, 4),\n  F_Critical = round(f_critical_anova, 3),\n  P_Value = round(p_value_anova, 4),\n  Alpha = alpha_anova,\n  Decision = ifelse(f_stat_anova &gt; f_critical_anova, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_anova &lt; alpha_anova,\n                     \"At least one process mean differs\",\n                     \"No significant difference between process means\")\n)\n\nprint(\"ANOVA Test Results:\")\nprint(anova_conclusion)\n\n# Using R's aov function for verification\nanova_r &lt;- aov(yield ~ process, data = process_yield_data)\nanova_summary_r &lt;- summary(anova_r)\nprint(\"R's ANOVA verification:\")\nprint(anova_summary_r)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: One-way ANOVA visualization\n\n\n\n\n\n\n\n\n# One-way ANOVA visualization\n# Box plot by process\np1 &lt;- ggplot(data = process_yield_data, mapping = aes(x = process, y = yield, fill = process)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"red\") +\n  geom_hline(yintercept = grand_mean, linetype = \"dashed\", color = \"blue\") +\n  labs(\n    x = \"Process\",\n    y = \"Yield\",\n    title = \"Yield by Process\",\n    subtitle = \"Red diamonds = group means, blue line = grand mean\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Residuals vs fitted plot\nfitted_values &lt;- rep(group_means, each = 6)\nresiduals &lt;- process_yield_data$yield - fitted_values\nresidual_data &lt;- data.table(fitted = fitted_values, residuals = residuals, process = process_yield_data$process)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(mapping = aes(color = process), size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\",\n    color = \"Process\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandomized Complete Block Design\n\n\n\nWhen experimental units are heterogeneous, blocking reduces error variance:\nModel:\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\nwhere:\n\nτᵢ = treatment effect\nβⱼ = block effect\nOne observation per treatment-block combination\n\nAdvantages:\n\nControls for known sources of variation\nIncreases precision of treatment comparisons\nReduces experimental error\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nTest four coating methods on different types of metal substrates (blocks):\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Coating Experiment Data Summary by Coating:\"\n\n\n     coating     n     Mean       SD      Min      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Coating_A     5 85.34018 5.821115 78.63352 93.27538\n2: Coating_B     5 85.93684 6.051341 79.31872 91.13382\n3: Coating_C     5 81.46412 7.616026 72.13510 91.27308\n4: Coating_D     5 90.18421 4.988870 82.43243 95.37133\n\n\n[1] \"Summary by Substrate (Block):\"\n\n\n   substrate     n     Mean       SD      Min      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:  Aluminum     4 78.79708 4.624031 72.13510 82.43243\n2:    Copper     4 91.60979 2.779976 88.66094 95.37133\n3:    Nickel     4 80.94072 6.287063 75.63415 90.04449\n4:     Steel     4 86.92283 2.550402 84.62341 89.42896\n5:  Titanium     4 90.38625 4.640854 83.65485 93.64384\n\n\n\n\n\n# Randomized complete block design example (coating experiment)\nset.seed(987)\ncoating_data &lt;- data.table(\n  coating = rep(c(\"Coating_A\", \"Coating_B\", \"Coating_C\", \"Coating_D\"), 5),\n  substrate = rep(c(\"Steel\", \"Aluminum\", \"Copper\", \"Titanium\", \"Nickel\"), each = 4),\n  adhesion = c(\n    # Steel substrate\n    rnorm(4, mean = c(85, 88, 82, 90), sd = 3),\n    # Aluminum substrate  \n    rnorm(4, mean = c(78, 82, 75, 85), sd = 3),\n    # Copper substrate\n    rnorm(4, mean = c(92, 95, 89, 98), sd = 3),\n    # Titanium substrate\n    rnorm(4, mean = c(88, 91, 85, 94), sd = 3),\n    # Nickel substrate\n    rnorm(4, mean = c(80, 83, 77, 87), sd = 3)\n  )\n)\n\n# Summary by coating\ncoating_summary &lt;- coating_data %&gt;% \n  fgroup_by(coating) %&gt;% \n  fsummarise(\n    n = fnobs(adhesion),\n    Mean = fmean(adhesion),\n    SD = fsd(adhesion),\n    Min = fmin(adhesion),\n    Max = fmax(adhesion)\n  )\n\n# Summary by substrate (block)\nsubstrate_summary &lt;- coating_data %&gt;% \n  fgroup_by(substrate) %&gt;% \n  fsummarise(\n    n = fnobs(adhesion),\n    Mean = fmean(adhesion),\n    SD = fsd(adhesion),\n    Min = fmin(adhesion),\n    Max = fmax(adhesion)\n  )\n\nprint(\"Coating Experiment Data Summary by Coating:\")\nprint(coating_summary)\nprint(\"Summary by Substrate (Block):\")\nprint(substrate_summary)\n\n# Write data to CSV for download\nfwrite(coating_data, \"data/coating_experiment.csv\")\n\n\n\n\n\n\n\nANOVA Table for RCBD:\n\n\n\nSource\nSS\ndf\nMS\nF\nP-value\n\n\n\n\nTotal\nSSₜ\nab-1\n\n\n\n\n\nTreatments\nSSₜᵣ\na-1\nMSₜᵣ\nMSₜᵣ/MSₑ\nP₁\n\n\nBlocks\nSSᵦₗ\nb-1\nMSᵦₗ\nMSᵦₗ/MSₑ\nP₂\n\n\nError\nSSₑ\n(a-1)(b-1)\nMSₑ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"RCBD ANOVA Table:\"\n\n\n       Source     SS    df     MS F_Statistic P_Value\n       &lt;char&gt;  &lt;num&gt; &lt;num&gt;  &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1:   Coatings 191.16     3  63.72      7.7335  0.0039\n2: Substrates 514.71     4 128.68     15.6170  0.0001\n3:      Error  98.88    12   8.24          NA      NA\n4:      Total 804.75    19     NA          NA      NA\n\n\n[1] \"RCBD Test Results:\"\n\n\n             Effect F_Statistic P_Value  Decision\n             &lt;char&gt;       &lt;num&gt;   &lt;num&gt;    &lt;char&gt;\n1:   Coating Effect      7.7335  0.0039 Reject H0\n2: Substrate Effect     15.6170  0.0001 Reject H0\n                             Conclusion\n                                 &lt;char&gt;\n1:   Coating means differ significantly\n2: Substrate means differ significantly\n\n\n[1] \"R's RCBD ANOVA verification:\"\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncoating      3  191.2   63.72   7.733 0.003871 ** \nsubstrate    4  514.7  128.68  15.617 0.000106 ***\nResiduals   12   98.9    8.24                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# RCBD ANOVA calculations\na_rcbd &lt;- length(unique(coating_data$coating))  # treatments\nb_rcbd &lt;- length(unique(coating_data$substrate))  # blocks  \nN_rcbd &lt;- fnobs(coating_data$adhesion)\n\n# Calculate sum of squares for RCBD\ngrand_mean_rcbd &lt;- fmean(coating_data$adhesion)\n\n# Treatment sum of squares\ntreatment_means &lt;- coating_summary$Mean\nSS_treatments_rcbd &lt;- b_rcbd * sum((treatment_means - grand_mean_rcbd)^2)\n\n# Block sum of squares  \nblock_means &lt;- substrate_summary$Mean\nSS_blocks_rcbd &lt;- a_rcbd * sum((block_means - grand_mean_rcbd)^2)\n\n# Total sum of squares\nSS_total_rcbd &lt;- sum((coating_data$adhesion - grand_mean_rcbd)^2)\n\n# Error sum of squares (by subtraction)\nSS_error_rcbd &lt;- SS_total_rcbd - SS_treatments_rcbd - SS_blocks_rcbd\n\n# Degrees of freedom\ndf_treatments_rcbd &lt;- a_rcbd - 1\ndf_blocks_rcbd &lt;- b_rcbd - 1  \ndf_error_rcbd &lt;- (a_rcbd - 1) * (b_rcbd - 1)\ndf_total_rcbd &lt;- N_rcbd - 1\n\n# Mean squares\nMS_treatments_rcbd &lt;- SS_treatments_rcbd / df_treatments_rcbd\nMS_blocks_rcbd &lt;- SS_blocks_rcbd / df_blocks_rcbd\nMS_error_rcbd &lt;- SS_error_rcbd / df_error_rcbd\n\n# F-statistics\nf_treatments_rcbd &lt;- MS_treatments_rcbd / MS_error_rcbd\nf_blocks_rcbd &lt;- MS_blocks_rcbd / MS_error_rcbd\n\n# P-values\np_treatments_rcbd &lt;- 1 - pf(f_treatments_rcbd, df_treatments_rcbd, df_error_rcbd)\np_blocks_rcbd &lt;- 1 - pf(f_blocks_rcbd, df_blocks_rcbd, df_error_rcbd)\n\n# RCBD ANOVA table\nrcbd_anova_table &lt;- data.table(\n  Source = c(\"Coatings\", \"Substrates\", \"Error\", \"Total\"),\n  SS = c(round(SS_treatments_rcbd, 2), round(SS_blocks_rcbd, 2), \n         round(SS_error_rcbd, 2), round(SS_total_rcbd, 2)),\n  df = c(df_treatments_rcbd, df_blocks_rcbd, df_error_rcbd, df_total_rcbd),\n  MS = c(round(MS_treatments_rcbd, 2), round(MS_blocks_rcbd, 2), \n         round(MS_error_rcbd, 2), NA),\n  F_Statistic = c(round(f_treatments_rcbd, 4), round(f_blocks_rcbd, 4), NA, NA),\n  P_Value = c(round(p_treatments_rcbd, 4), round(p_blocks_rcbd, 4), NA, NA)\n)\n\nprint(\"RCBD ANOVA Table:\")\nprint(rcbd_anova_table)\n\n# RCBD conclusions\nrcbd_conclusions &lt;- data.table(\n  Effect = c(\"Coating Effect\", \"Substrate Effect\"),\n  F_Statistic = c(round(f_treatments_rcbd, 4), round(f_blocks_rcbd, 4)),\n  P_Value = c(round(p_treatments_rcbd, 4), round(p_blocks_rcbd, 4)),\n  Decision = c(\n    ifelse(p_treatments_rcbd &lt; alpha_anova, \"Reject H0\", \"Fail to reject H0\"),\n    ifelse(p_blocks_rcbd &lt; alpha_anova, \"Reject H0\", \"Fail to reject H0\")\n  ),\n  Conclusion = c(\n    ifelse(p_treatments_rcbd &lt; alpha_anova, \"Coating means differ significantly\", \"No significant coating effect\"),\n    ifelse(p_blocks_rcbd &lt; alpha_anova, \"Substrate means differ significantly\", \"No significant substrate effect\")\n  )\n)\n\nprint(\"RCBD Test Results:\")\nprint(rcbd_conclusions)\n\n# Using R's aov for verification\nrcbd_r &lt;- aov(adhesion ~ coating + substrate, data = coating_data)\nrcbd_summary_r &lt;- summary(rcbd_r)\nprint(\"R's RCBD ANOVA verification:\")\nprint(rcbd_summary_r)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 4: Randomized complete block design visualization\n\n\n\n\n\n\n\n\n# RCBD visualization\n# Interaction plot\ninteraction_plot_data &lt;- coating_data %&gt;%\n  fgroup_by(coating, substrate) %&gt;%\n  fsummarise(mean_adhesion = fmean(adhesion))\n\np1_rcbd &lt;- ggplot(data = interaction_plot_data, mapping = aes(x = coating, y = mean_adhesion, \n                                                             color = substrate, group = substrate)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  labs(\n    x = \"Coating Type\",\n    y = \"Mean Adhesion\",\n    color = \"Substrate\",\n    title = \"Interaction Plot: Coating × Substrate\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Box plot by coating with substrate grouping\np2_rcbd &lt;- ggplot(data = coating_data, mapping = aes(x = coating, y = adhesion, fill = substrate)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    x = \"Coating Type\",\n    y = \"Adhesion\",\n    fill = \"Substrate\",\n    title = \"Adhesion by Coating and Substrate\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Combine plots\ngrid.arrange(p1_rcbd, p2_rcbd, nrow = 2, ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteComprehensive Example\n\n\n\nA manufacturing company conducts a comprehensive quality study comparing two suppliers, testing before/after process improvements, and evaluating multiple quality metrics:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Comprehensive Quality Study Summary by Supplier:\"\n\n\n     supplier     n Mean_Before Mean_After Mean_Improvement SD_Before SD_After\n       &lt;char&gt; &lt;int&gt;       &lt;num&gt;      &lt;num&gt;            &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: Supplier_A    30    83.49488   92.93163         9.436750  8.242329 5.746158\n2: Supplier_B    30    82.31349   87.60750         5.294008 11.587749 8.813704\n   SD_Improvement Defect_Rate_Before Defect_Rate_After Var_Before Var_After\n            &lt;num&gt;              &lt;num&gt;             &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n1:       12.23185         0.16666667        0.00000000   67.93598  33.01834\n2:       15.98305         0.06666667        0.03333333  134.27592  77.68137\n\n\n[1] \"Overall Quality Summary:\"\n\n\n   Total_n Overall_Mean_Before Overall_Mean_After Overall_Improvement\n     &lt;int&gt;               &lt;num&gt;              &lt;num&gt;               &lt;num&gt;\n1:      60            82.90418           90.26956            7.365379\n   Overall_Defect_Before Overall_Defect_After\n                   &lt;num&gt;                &lt;num&gt;\n1:             0.1166667           0.01666667\n\n\n\n\n\n# Comprehensive quality improvement study\nset.seed(111)\ncomprehensive_quality_data &lt;- data.table(\n  part_id = 1:60,\n  supplier = rep(c(\"Supplier_A\", \"Supplier_B\"), each = 30),\n  before_improvement = c(rnorm(30, mean = 85, sd = 8), rnorm(30, mean = 82, sd = 9)),\n  after_improvement = c(rnorm(30, mean = 92, sd = 7), rnorm(30, mean = 88, sd = 8)),\n  defective_before = sample(c(0, 1), 60, replace = TRUE, prob = c(0.88, 0.12)),\n  defective_after = sample(c(0, 1), 60, replace = TRUE, prob = c(0.94, 0.06))\n)\n\n# Calculate improvements\ncomprehensive_quality_data[, improvement := after_improvement - before_improvement]\n\n# Comprehensive summary\nquality_comprehensive_summary &lt;- comprehensive_quality_data %&gt;% \n  fgroup_by(supplier) %&gt;% \n  fsummarise(\n    n = fnobs(part_id),\n    # Before/After means\n    Mean_Before = fmean(before_improvement),\n    Mean_After = fmean(after_improvement),\n    Mean_Improvement = fmean(improvement),\n    # Standard deviations\n    SD_Before = fsd(before_improvement),\n    SD_After = fsd(after_improvement),\n    SD_Improvement = fsd(improvement),\n    # Defect rates\n    Defect_Rate_Before = fmean(defective_before),\n    Defect_Rate_After = fmean(defective_after),\n    # Variance comparison\n    Var_Before = fvar(before_improvement),\n    Var_After = fvar(after_improvement)\n  )\n\nprint(\"Comprehensive Quality Study Summary by Supplier:\")\nprint(quality_comprehensive_summary)\n\n# Overall summary\noverall_quality_summary &lt;- comprehensive_quality_data %&gt;% \n  fsummarise(\n    Total_n = fnobs(part_id),\n    Overall_Mean_Before = fmean(before_improvement),\n    Overall_Mean_After = fmean(after_improvement),\n    Overall_Improvement = fmean(improvement),\n    Overall_Defect_Before = fmean(defective_before),\n    Overall_Defect_After = fmean(defective_after)\n  )\n\nprint(\"Overall Quality Summary:\")\nprint(overall_quality_summary)\n\n# Write data to CSV for download\nfwrite(comprehensive_quality_data, \"data/comprehensive_quality.csv\")\n\n\n\n\n\n\n\nComplete Analysis:\n\nCompare supplier means (independent samples)\nTest variance equality between suppliers\nBefore/after comparison (paired test)\nCompare defect proportions\nMulti-factor ANOVA\n\n\nR OutputR Code\n\n\n\n\nError in select(., part_id, supplier, before_improvement, after_improvement): could not find function \"select\"\n\n\nError: object 'before_after_long' not found\n\n\nError in select(., supplier, before_improvement, after_improvement): could not find function \"select\"\n\n\nError: object 'variance_data' not found\n\n\nError in select(., supplier, Rate_Before, Rate_After): could not find function \"select\"\n\n\nError: object 'defect_summary_comp' not found\n\n\nError: object 'p2_comp' not found\n\n\n\n\n\n# Comprehensive quality analysis dashboard\n\n# 1. Supplier comparison (before improvement)\np1_comp &lt;- ggplot(data = comprehensive_quality_data, \n                  mapping = aes(x = supplier, y = before_improvement, fill = supplier)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.4) +\n  labs(x = \"Supplier\", y = \"Quality Score\", title = \"Quality Before Improvement\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")\n\n# 2. Before/After comparison (paired)\nbefore_after_long &lt;- comprehensive_quality_data %&gt;%\n  select(part_id, supplier, before_improvement, after_improvement) %&gt;%\n  pivot_longer(cols = c(before_improvement, after_improvement), \n               names_to = \"period\", values_to = \"quality\") %&gt;%\n  data.table()\n\np2_comp &lt;- ggplot(data = before_after_long, \n                  mapping = aes(x = period, y = quality, group = part_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(mapping = aes(color = period), alpha = 0.6) +\n  facet_wrap(~supplier) +\n  labs(x = \"Period\", y = \"Quality Score\", title = \"Before/After Comparison by Supplier\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Variance comparison\nvariance_data &lt;- comprehensive_quality_data %&gt;%\n  select(supplier, before_improvement, after_improvement) %&gt;%\n  pivot_longer(cols = c(before_improvement, after_improvement),\n               names_to = \"period\", values_to = \"quality\") %&gt;%\n  data.table()\n\np3_comp &lt;- ggplot(data = variance_data, mapping = aes(x = quality, fill = period)) +\n  geom_density(alpha = 0.6) +\n  facet_wrap(~supplier) +\n  labs(x = \"Quality Score\", y = \"Density\", title = \"Distribution Comparison\", fill = \"Period\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Defect rate comparison\ndefect_summary_comp &lt;- comprehensive_quality_data %&gt;%\n  fgroup_by(supplier) %&gt;%\n  fsummarise(\n    Before = fsum(defective_before),\n    After = fsum(defective_after),\n    n = fnobs(part_id)\n  ) %&gt;%\n  mutate(\n    Rate_Before = Before / n,\n    Rate_After = After / n\n  ) %&gt;%\n  select(supplier, Rate_Before, Rate_After) %&gt;%\n  pivot_longer(cols = c(Rate_Before, Rate_After), names_to = \"period\", values_to = \"rate\") %&gt;%\n  data.table()\n\np4_comp &lt;- ggplot(data = defect_summary_comp, \n                  mapping = aes(x = supplier, y = rate, fill = period)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  labs(x = \"Supplier\", y = \"Defect Rate\", title = \"Defect Rate Comparison\", fill = \"Period\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine all plots\ngrid.arrange(p1_comp, p2_comp, p3_comp, p4_comp, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEssential Formulas Summary\n\n\n\nTwo-Sample Z-Test (σ known):\nZ_0 = \\frac{(\\overline{X_1} - \\overline{X_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\nTwo-Sample t-Test (σ unknown, equal variances):\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\nPaired t-Test:\nT_0 = \\frac{\\overline{d} - \\mu_d}{S_d/\\sqrt{n}}\nF-Test for Variances:\nF_0 = \\frac{S_1^2}{S_2^2}\nTwo-Proportion Z-Test:\nZ_0 = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\nOne-Way ANOVA:\nF_0 = \\frac{MS_{Treatments}}{MS_{Error}}\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 3: Decision framework for choosing appropriate test\n\n\n\n\nDecision Framework for Choosing Appropriate Two-Sample Test\n\n\nSituation\nTest_Method\nKey_Assumptions\nWhen_to_Use\n\n\n\n\nTwo independent groups, σ known\nTwo-sample Z-test\nNormal populations, σ₁, σ₂ known\nPopulation SDs known from historical data\n\n\nTwo independent groups, σ unknown, equal variances\nTwo-sample t-test (pooled)\nNormal populations, σ₁² = σ₂²\nMost common two-sample situation\n\n\nTwo independent groups, σ unknown, unequal variances\nWelch's t-test\nNormal populations\nWhen variances clearly unequal\n\n\nPaired/matched observations\nPaired t-test\nNormal differences\nBefore/after, matched pairs\n\n\nCompare two variances\nF-test\nNormal populations\nCheck equal variance assumption\n\n\nCompare two proportions\nTwo-proportion Z-test\nLarge samples\nCompare success rates\n\n\nCompare more than two groups\nOne-way ANOVA\nNormal populations, equal variances\nMultiple groups to compare\n\n\nCompare groups with blocking\nRCBD ANOVA\nNormal populations, additive model\nControl for known nuisance factors\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4\n\n\n# Decision framework for choosing appropriate test\ndecision_framework &lt;- data.table(\n  Situation = c(\"Two independent groups, σ known\", \"Two independent groups, σ unknown, equal variances\",\n               \"Two independent groups, σ unknown, unequal variances\", \"Paired/matched observations\",\n               \"Compare two variances\", \"Compare two proportions\",\n               \"Compare more than two groups\", \"Compare groups with blocking\"),\n  Test_Method = c(\"Two-sample Z-test\", \"Two-sample t-test (pooled)\", \n                 \"Welch's t-test\", \"Paired t-test\",\n                 \"F-test\", \"Two-proportion Z-test\",\n                 \"One-way ANOVA\", \"RCBD ANOVA\"),\n  Key_Assumptions = c(\"Normal populations, σ₁, σ₂ known\", \"Normal populations, σ₁² = σ₂²\",\n                     \"Normal populations\", \"Normal differences\",\n                     \"Normal populations\", \"Large samples\",\n                     \"Normal populations, equal variances\", \"Normal populations, additive model\"),\n  When_to_Use = c(\"Population SDs known from historical data\", \"Most common two-sample situation\",\n                 \"When variances clearly unequal\", \"Before/after, matched pairs\",\n                 \"Check equal variance assumption\", \"Compare success rates\",\n                 \"Multiple groups to compare\", \"Control for known nuisance factors\")\n)\n\nkbl(decision_framework, \n    caption = \"Decision Framework for Choosing Appropriate Two-Sample Test\") %&gt;%\n  kable_styling() %&gt;%\n  column_spec(1, width = \"3cm\") %&gt;%\n  column_spec(2, width = \"2.5cm\") %&gt;%\n  column_spec(3, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"3cm\")\n\n\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Sample Size Requirements for Two-Sample Tests:\"\n\n\n             Test_Type Alpha Power      Effect_Size Required_n_per_group\n                &lt;char&gt; &lt;num&gt; &lt;num&gt;           &lt;char&gt;                &lt;num&gt;\n1:   Two-sample t-test  0.05   0.8        δ=5, σ=10                   63\n2:   Two-sample t-test  0.05   0.8        δ=3, σ=10                  175\n3:   Two-sample t-test  0.05   0.8        δ=1, σ=10                 1570\n4: Two-proportion test  0.05   0.8   p₁=0.2, p₂=0.3                  294\n5: Two-proportion test  0.05   0.8  p₁=0.1, p₂=0.15                  686\n6: Two-proportion test  0.05   0.8 p₁=0.05, p₂=0.10                  435\n\n\n\n\n\n# Sample size determination for various two-sample tests\n\n# Function for two-sample t-test sample size\nsample_size_two_sample_t &lt;- function(alpha, beta, delta, sigma_pooled) {\n  t_alpha2 &lt;- qt(1 - alpha/2, df = Inf)  # Use normal approximation for large samples\n  t_beta &lt;- qt(1 - beta, df = Inf)\n  n &lt;- 2 * ((t_alpha2 + t_beta) * sigma_pooled / delta)^2\n  return(ceiling(n))\n}\n\n# Function for two-proportion test sample size\nsample_size_two_prop &lt;- function(alpha, beta, p1, p2) {\n  z_alpha2 &lt;- qnorm(1 - alpha/2)\n  z_beta &lt;- qnorm(1 - beta)\n  p_bar &lt;- (p1 + p2) / 2\n  delta &lt;- abs(p1 - p2)\n  \n  n &lt;- (z_alpha2 * sqrt(2 * p_bar * (1 - p_bar)) + z_beta * sqrt(p1 * (1 - p1) + p2 * (1 - p2)))^2 / delta^2\n  return(ceiling(n))\n}\n\n# Sample size examples\nsample_size_examples &lt;- data.table(\n  Test_Type = c(\"Two-sample t-test\", \"Two-sample t-test\", \"Two-sample t-test\",\n               \"Two-proportion test\", \"Two-proportion test\", \"Two-proportion test\"),\n  Alpha = rep(0.05, 6),\n  Power = rep(0.80, 6),\n  Effect_Size = c(\"δ=5, σ=10\", \"δ=3, σ=10\", \"δ=1, σ=10\",\n                 \"p₁=0.2, p₂=0.3\", \"p₁=0.1, p₂=0.15\", \"p₁=0.05, p₂=0.10\"),\n  Required_n_per_group = c(\n    sample_size_two_sample_t(0.05, 0.20, 5, 10),\n    sample_size_two_sample_t(0.05, 0.20, 3, 10),\n    sample_size_two_sample_t(0.05, 0.20, 1, 10),\n    sample_size_two_prop(0.05, 0.20, 0.2, 0.3),\n    sample_size_two_prop(0.05, 0.20, 0.1, 0.15),\n    sample_size_two_prop(0.05, 0.20, 0.05, 0.10)\n  )\n)\n\nprint(\"Sample Size Requirements for Two-Sample Tests:\")\nprint(sample_size_examples)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\nError in parse(text = input): &lt;text&gt;:17:3: unexpected symbol\n16:   ),\n17: F value\n      ^\n\n\n\n\n\n# Calculate effect sizes for all examples in the chapter\n\neffect_size_summary &lt;- data.table(\n  Example = c(\"Cable Strength (Z-test)\", \"Plastic Strength (t-test)\", \n             \"Process Improvement (Paired)\", \"Defect Rate Comparison\", \n             \"Instrument Variance\", \"Process Yield (ANOVA)\"),\n  Effect_Type = c(\"Cohen's d\", \"Cohen's d\", \"Cohen's d\", \n                 \"Difference in proportions\", \"Variance ratio\", \"Eta-squared\"),\n  Calculated_Effect = c(\n    abs(xbar1 - xbar2) / sqrt((sigma1^2 + sigma2^2)/2),  # Cable strength\n    abs(xbar1_t - xbar2_t) / sp,                         # Plastic strength\n    abs(d_bar) / s_d,                                    # Process improvement\n    abs(p1_hat - p2_hat),                                # Defect rates\n    max(s1_f_squared, s2_f_squared) / min(s1_f_squared, s2_f_squared), # Variance ratio\n    SST / SS_Total                                       # ANOVA eta-squared\n  ),\nF value`[1]) &lt; 0.001\n  )\n)\n\nprint(\"Validation of Manual vs R Calculations:\")\nprint(validation_checks)\n\n# Check if all validations passed\nall_passed &lt;- all(validation_checks$Match)\nprint(paste(\"All validation checks passed:\", all_passed))\n\nif (!all_passed) {\n  print(\"WARNING: Some manual calculations do not match R functions\")\n  print(\"Please review the calculations above\")\n} else {\n  print(\"SUCCESS: All manual calculations match R function results\")\n}\n\nprint(paste0(\"\\n\", strrep(\"=\", 80)))\nprint(\"CHAPTER 5 COMPLETE - ALL EXAMPLES VERIFIED\")\nprint(strrep(\"=\", 80))\n\n\n\n\n\n\n\nThis chapter covered the fundamental concepts of two-sample comparisons in engineering statistics:\n\nTwo-sample Z-tests when population variances are known\nTwo-sample t-tests for unknown variances (equal and unequal)\nPaired t-tests for dependent observations\nF-tests for comparing variances\nTwo-proportion tests for comparing success rates\nANOVA for comparing multiple groups\nExperimental design concepts (CRD and RCBD)\n\nThe key is choosing the appropriate method based on:\n\nData structure (independent vs. paired)\nKnowledge of population parameters\nAssumptions about variance equality\nType of data (continuous vs. categorical)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#introduction",
    "href": "book/Ch05.html#introduction",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteTwo-Sample Problems\n\n\n\nMany engineering problems involve comparing two populations, processes, or treatments. Common scenarios include:\n\nComparing the mean strength of materials from two suppliers\nTesting whether a new manufacturing process reduces defect rates\nEvaluating if a design modification improves performance\nComparing measurements before and after a process change\n\nThe choice of statistical method depends on:\n\nWhether samples are independent or paired\nWhether population variances are known or unknown\nWhether variances can be assumed equal or unequal",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#inference-on-the-means-of-two-populations-variances-known",
    "href": "book/Ch05.html#inference-on-the-means-of-two-populations-variances-known",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteTwo-Sample Z-Test (σ₁, σ₂ known)\n\n\n\nWhen both population variances are known, the test statistic for testing H₀: μ₁ = μ₂ is:\nZ_0 = \\frac{(\\overline{X_1} - \\overline{X_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\nUnder H₀: μ₁ - μ₂ = 0, Z₀ ~ N(0,1)\nCommon Hypothesis Tests:\n\nTwo-sided: H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂\nOne-sided: H₀: μ₁ ≤ μ₂ vs H₁: μ₁ &gt; μ₂\nOne-sided: H₀: μ₁ ≥ μ₂ vs H₁: μ₁ &lt; μ₂\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA company wants to compare the tensile strength of cables produced by two different machines. Machine 1 produces cables with σ₁ = 12 psi, and Machine 2 with σ₂ = 10 psi. Random samples yield:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Cable Strength Data Summary by Machine:\"\n\n\n     machine     n      Min       Q1   Median     Mean       Q3      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Machine_1    20 221.4006 239.0774 246.4398 246.6995 251.5847 266.4430\n2: Machine_2    25 221.1331 231.0529 235.9208 238.0735 246.2158 259.6896\n          SD       Var\n       &lt;num&gt;     &lt;num&gt;\n1: 11.671984 136.23521\n2:  9.449202  89.28741\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\n\n# Cable strength comparison (two-sample Z-test, sigma known)\nset.seed(123)\ncable_data &lt;- data.table(\n  machine = rep(c(\"Machine_1\", \"Machine_2\"), c(20, 25)),\n  strength = c(rnorm(20, mean = 245, sd = 12), rnorm(25, mean = 238, sd = 10))\n)\n\n# Summary statistics using collapse functions\ncable_summary &lt;- cable_data %&gt;% \n  fgroup_by(machine) %&gt;% \n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength)\n  )\n\nprint(\"Cable Strength Data Summary by Machine:\")\nprint(cable_summary)\n\n# Write data to CSV for download\nfwrite(cable_data, \"data/cable_strength.csv\")\n\n\n\n\n\n\n\nTest H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂ at α = 0.05.\nManual Calculation:\nGiven: n₁ = 20, x̄₁ = 245, σ₁ = 12, n₂ = 25, x̄₂ = 238, σ₂ = 10\n\nTest Statistic: Z_0 = \\frac{(245 - 238) - 0}{\\sqrt{\\frac{12^2}{20} + \\frac{10^2}{25}}} = \\frac{7}{\\sqrt{7.2 + 4}} = \\frac{7}{3.35} = 2.09\nCritical Value: z₀.₀₂₅ = 1.96\nDecision: |Z₀| = 2.09 &gt; 1.96, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Two-Sample Z-Test Results:\"\n\n\n                     Test_Type    n1    n2 xbar1  xbar2 sigma1 sigma2 Pooled_SE\n                        &lt;char&gt; &lt;int&gt; &lt;int&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;     &lt;num&gt;\n1: Two-sample Z-test (σ known)    20    25 246.7 238.07     12     10     3.347\n   Z_Statistic P_Value Alpha Z_Critical  Decision\n         &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:      2.5775    0.01  0.05       1.96 Reject H0\n                          Conclusion\n                              &lt;char&gt;\n1: Means are significantly different\n\n\n\n\n\n# Two-sample Z-test (manual and R calculations)\n# Known population standard deviations\nsigma1 &lt;- 12\nsigma2 &lt;- 10\nalpha &lt;- 0.05\n\n# Extract sample statistics\nmachine1_data &lt;- cable_data[machine == \"Machine_1\"]\nmachine2_data &lt;- cable_data[machine == \"Machine_2\"]\n\nn1 &lt;- fnobs(machine1_data$strength)\nn2 &lt;- fnobs(machine2_data$strength)\nxbar1 &lt;- fmean(machine1_data$strength)\nxbar2 &lt;- fmean(machine2_data$strength)\n\n# Manual calculation\npooled_se &lt;- sqrt((sigma1^2)/n1 + (sigma2^2)/n2)\nz_stat &lt;- (xbar1 - xbar2) / pooled_se\np_value_z &lt;- 2 * (1 - pnorm(abs(z_stat)))  # Two-sided test\nz_critical &lt;- qnorm(1 - alpha/2)\n\n# Test results\nz_test_results &lt;- data.table(\n  Test_Type = \"Two-sample Z-test (σ known)\",\n  n1 = n1,\n  n2 = n2,\n  xbar1 = round(xbar1, 2),\n  xbar2 = round(xbar2, 2),\n  sigma1 = sigma1,\n  sigma2 = sigma2,\n  Pooled_SE = round(pooled_se, 3),\n  Z_Statistic = round(z_stat, 4),\n  P_Value = round(p_value_z, 4),\n  Alpha = alpha,\n  Z_Critical = round(z_critical, 3),\n  Decision = ifelse(abs(z_stat) &gt; z_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_z &lt; alpha, \n                     \"Means are significantly different\", \n                     \"No significant difference in means\")\n)\n\nprint(\"Two-Sample Z-Test Results:\")\nprint(z_test_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePower and Sample Size for Two-Sample Z-Test\n\n\n\nType II Error: For a specific difference δ = |μ₁ - μ₂|:\n\\beta = P\\left(-z_{\\alpha/2} - \\frac{\\delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\leq Z \\leq z_{\\alpha/2} - \\frac{\\delta}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\right)\nSample Size: For equal sample sizes (n₁ = n₂ = n):\nn = \\frac{(z_{\\alpha/2} + z_\\beta)^2(\\sigma_1^2 + \\sigma_2^2)}{\\delta^2}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Power curves for two-sample Z-test\n\n\n\n\n\n\n\n\n# Power curves for two-sample Z-test\ndelta_values &lt;- seq(0, 15, by = 0.5)\nsample_sizes &lt;- c(10, 20, 30, 50)\nalpha_power &lt;- 0.05\nsigma1_power &lt;- 12\nsigma2_power &lt;- 10\n\n# Function to calculate power for two-sample Z-test\ncalculate_power_two_sample &lt;- function(delta, n1, n2, sigma1, sigma2, alpha) {\n  se &lt;- sqrt(sigma1^2/n1 + sigma2^2/n2)\n  z_alpha2 &lt;- qnorm(1 - alpha/2)\n  \n  # Power for two-sided test\n  power &lt;- 1 - pnorm(z_alpha2 - delta/se) + pnorm(-z_alpha2 - delta/se)\n  return(power)\n}\n\n# Generate power curve data\npower_curve_data &lt;- data.table()\nfor(n_val in sample_sizes) {\n  for(delta in delta_values) {\n    power_val &lt;- calculate_power_two_sample(delta, n_val, n_val, sigma1_power, sigma2_power, alpha_power)\n    power_curve_data &lt;- rbind(power_curve_data, \n                             data.table(delta = delta, n = factor(n_val), power = power_val))\n  }\n}\n\n# Plot power curves\nggplot(data = power_curve_data, mapping = aes(x = delta, y = power, color = n)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.8, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = 12, y = 0.82, label = \"Power = 0.8\", hjust = 0) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(breaks = pretty_breaks(n = 6), limits = c(0, 1)) +\n  labs(\n    x = \"True Difference in Means (δ = |μ₁ - μ₂|)\",\n    y = \"Power (1 - β)\",\n    color = \"Sample Size\\n(each group)\",\n    title = \"Power Curves for Two-Sample Z-Test\",\n    subtitle = \"H₀: μ₁ = μ₂ vs H₁: μ₁ ≠ μ₂, α = 0.05, σ₁ = 12, σ₂ = 10\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ₁ - μ₂ (σ₁, σ₂ known)\n\n\n\nA 100(1-α)% confidence interval for μ₁ - μ₂:\n(\\overline{x_1} - \\overline{x_2}) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Means (σ known):\"\n\n\n   Parameter Confidence_Level Difference Standard_Error Z_Critical Margin_Error\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;          &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   μ₁ - μ₂              95%       8.63          3.347       1.96        6.559\n   Lower_Bound Upper_Bound Width\n         &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:        2.07       15.19 13.12\n                                         Interpretation\n                                                 &lt;char&gt;\n1: 95% confident that μ₁ - μ₂ is between 2.07 and 15.19\n\n\n\n\n\n# Confidence interval for difference in means (sigma known)\nmargin_error_z &lt;- z_critical * pooled_se\nci_lower_z &lt;- (xbar1 - xbar2) - margin_error_z\nci_upper_z &lt;- (xbar1 - xbar2) + margin_error_z\n\nz_ci_results &lt;- data.table(\n  Parameter = \"μ₁ - μ₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(xbar1 - xbar2, 2),\n  Standard_Error = round(pooled_se, 3),\n  Z_Critical = round(z_critical, 3),\n  Margin_Error = round(margin_error_z, 3),\n  Lower_Bound = round(ci_lower_z, 2),\n  Upper_Bound = round(ci_upper_z, 2),\n  Width = round(ci_upper_z - ci_lower_z, 2),\n  Interpretation = paste(\"95% confident that μ₁ - μ₂ is between\", \n                        round(ci_lower_z, 2), \"and\", round(ci_upper_z, 2))\n)\n\nprint(\"95% Confidence Interval for Difference in Means (σ known):\")\nprint(z_ci_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#inference-on-the-means-of-two-populations-variances-unknown",
    "href": "book/Ch05.html#inference-on-the-means-of-two-populations-variances-unknown",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteTwo-Sample t-Test (σ₁, σ₂ unknown)\n\n\n\nWhen population variances are unknown, we must consider two cases:\nCase 1: Equal Variances (σ₁² = σ₂²)\n\nUse pooled variance:\n\nS_p^2 = \\frac{(n_1-1)S_1^2 + (n_2-1)S_2^2}{n_1 + n_2 - 2}\n\nTest statistic:\n\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\nDegrees of freedom: ν = n₁ + n₂ - 2\n\nCase 2: Unequal Variances (σ₁² ≠ σ₂²)\n\nWelch’s t-test\nTest statistic:\n\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\n\nDegrees of freedom:\n\n\\nu = \\frac{(\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2})^2}{\\frac{S_1^4}{n_1^2(n_1-1)} + \\frac{S_2^4}{n_2^2(n_2-1)}}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the breaking strength of two types of plastic. Random samples give:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Plastic Strength Data Summary by Type:\"\n\n\n     type     n      Min       Q1   Median     Mean       Q3      Max        SD\n   &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;     &lt;num&gt;\n1: Type_A    15 150.6854 155.8163 167.2005 163.4680 169.8374 176.0622  8.599716\n2: Type_B    18 139.6099 152.5444 156.1428 158.8415 170.5125 178.5483 11.920081\n         Var  SE_Mean\n       &lt;num&gt;    &lt;num&gt;\n1:  73.95511 2.220437\n2: 142.08832 2.809590\n\n\n\n\n\n# Plastic strength comparison (two-sample t-test, sigma unknown)\nset.seed(456)\nplastic_data &lt;- data.table(\n  type = rep(c(\"Type_A\", \"Type_B\"), c(15, 18)),\n  strength = c(rnorm(15, mean = 162.5, sd = 8.2), rnorm(18, mean = 157.8, sd = 9.1))\n)\n\n# Summary statistics\nplastic_summary &lt;- plastic_data %&gt;% \n  fgroup_by(type) %&gt;% \n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength),\n    SE_Mean = fsd(strength) / sqrt(fnobs(strength))\n  )\n\nprint(\"Plastic Strength Data Summary by Type:\")\nprint(plastic_summary)\n\n# Write data to CSV for download\nfwrite(plastic_data, \"data/plastic_strength.csv\")\n\n\n\n\n\n\n\nFirst, test for equal variances, then perform appropriate t-test.\nManual Calculation (assuming equal variances):\nGiven: n₁ = 15, x̄₁ = 162.5, s₁ = 8.2, n₂ = 18, x̄₂ = 157.8, s₂ = 9.1\n\nPooled Variance: S_p^2 = \\frac{(15-1)(8.2)^2 + (18-1)(9.1)^2}{15 + 18 - 2} = \\frac{940.36 + 1408.57}{31} = 75.77\nTest Statistic: T_0 = \\frac{162.5 - 157.8}{8.71\\sqrt{\\frac{1}{15} + \\frac{1}{18}}} = \\frac{4.7}{3.01} = 1.56\nCritical Value: t₀.₀₂₅,₃₁ = 2.040\nDecision: |T₀| = 1.56 &lt; 2.040, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"F-test for equal variances: F = 0.52 , p-value = 0.2229\"\n\n\n[1] \"Two-Sample t-Test Results (Equal Variances):\"\n\n\n                             Test_Type    n1    n2  xbar1  xbar2    s1    s2\n                                &lt;char&gt; &lt;int&gt; &lt;int&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n1: Two-sample t-test (equal variances)    15    18 163.47 158.84   8.6 11.92\n   Pooled_SD SE_Difference t_Statistic    df P_Value Alpha t_Critical\n       &lt;num&gt;         &lt;num&gt;       &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:     10.55         3.689      1.2543    31  0.2191  0.05       2.04\n            Decision                         Conclusion\n              &lt;char&gt;                             &lt;char&gt;\n1: Fail to reject H0 No significant difference in means\n\n\n[1] \"R's t.test verification:\"\n\n\n\n    Two Sample t-test\n\ndata:  typeA_data$strength and typeB_data$strength\nt = 1.2543, df = 31, p-value = 0.2191\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.896371 12.149429\nsample estimates:\nmean of x mean of y \n 163.4680  158.8415 \n\n\n\n\n\n# Two-sample t-test with equal variances (manual and R calculations)\ntypeA_data &lt;- plastic_data[type == \"Type_A\"]\ntypeB_data &lt;- plastic_data[type == \"Type_B\"]\n\nn1_t &lt;- fnobs(typeA_data$strength)\nn2_t &lt;- fnobs(typeB_data$strength)\nxbar1_t &lt;- fmean(typeA_data$strength)\nxbar2_t &lt;- fmean(typeB_data$strength)\ns1_t &lt;- fsd(typeA_data$strength)\ns2_t &lt;- fsd(typeB_data$strength)\n\n# Test for equal variances first (F-test)\nf_stat_equal &lt;- s1_t^2 / s2_t^2\ndf1_f &lt;- n1_t - 1\ndf2_f &lt;- n2_t - 1\np_value_f &lt;- 2 * min(pf(f_stat_equal, df1_f, df2_f), 1 - pf(f_stat_equal, df1_f, df2_f))\n\nprint(paste(\"F-test for equal variances: F =\", round(f_stat_equal, 3), \", p-value =\", round(p_value_f, 4)))\n\n# Assuming equal variances (pooled variance)\nsp_squared &lt;- ((n1_t - 1) * s1_t^2 + (n2_t - 1) * s2_t^2) / (n1_t + n2_t - 2)\nsp &lt;- sqrt(sp_squared)\nse_pooled &lt;- sp * sqrt(1/n1_t + 1/n2_t)\n\n# Manual t-test calculation\nt_stat &lt;- (xbar1_t - xbar2_t) / se_pooled\ndf_t &lt;- n1_t + n2_t - 2\np_value_t &lt;- 2 * (1 - pt(abs(t_stat), df = df_t))\nt_critical &lt;- qt(1 - alpha/2, df = df_t)\n\n# Test results\nt_test_results &lt;- data.table(\n  Test_Type = \"Two-sample t-test (equal variances)\",\n  n1 = n1_t,\n  n2 = n2_t,\n  xbar1 = round(xbar1_t, 2),\n  xbar2 = round(xbar2_t, 2),\n  s1 = round(s1_t, 2),\n  s2 = round(s2_t, 2),\n  Pooled_SD = round(sp, 2),\n  SE_Difference = round(se_pooled, 3),\n  t_Statistic = round(t_stat, 4),\n  df = df_t,\n  P_Value = round(p_value_t, 4),\n  Alpha = alpha,\n  t_Critical = round(t_critical, 3),\n  Decision = ifelse(abs(t_stat) &gt; t_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_t &lt; alpha, \n                     \"Means are significantly different\", \n                     \"No significant difference in means\")\n)\n\nprint(\"Two-Sample t-Test Results (Equal Variances):\")\nprint(t_test_results)\n\n# Using R's built-in t.test for verification\nt_test_r &lt;- t.test(typeA_data$strength, typeB_data$strength, var.equal = TRUE)\nprint(\"R's t.test verification:\")\nprint(t_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Two-Sample t-Test\n\n\n\nFor equal sample sizes and equal variances:\nn \\approx \\frac{2(t_{\\alpha/2,\\nu} + t_{\\beta,\\nu})^2 s_p^2}{\\delta^2}\nwhere ν = 2n - 2 and s_p² is estimated from pilot data.\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ₁ - μ₂ (σ₁, σ₂ unknown)\n\n\n\nEqual Variances:\n(\\overline{x_1} - \\overline{x_2}) \\pm t_{\\alpha/2,n_1+n_2-2} \\cdot S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\nUnequal Variances:\n(\\overline{x_1} - \\overline{x_2}) \\pm t_{\\alpha/2,\\nu} \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Means (σ unknown):\"\n\n\n   Parameter Confidence_Level Difference Pooled_SD Standard_Error t_Critical\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;     &lt;num&gt;          &lt;num&gt;      &lt;num&gt;\n1:   μ₁ - μ₂              95%       4.63     10.55          3.689       2.04\n   Margin_Error Lower_Bound Upper_Bound Width\n          &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:        7.523        -2.9       12.15 15.05\n                                         Interpretation\n                                                 &lt;char&gt;\n1: 95% confident that μ₁ - μ₂ is between -2.9 and 12.15\n\n\n[1] \"Welch's t-test (unequal variances):\"\n\n\n\n    Welch Two Sample t-test\n\ndata:  typeA_data$strength and typeB_data$strength\nt = 1.2919, df = 30.446, p-value = 0.2061\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -2.682527 11.935585\nsample estimates:\nmean of x mean of y \n 163.4680  158.8415 \n\n\n\n\n\n# Confidence interval for difference in means (sigma unknown)\nmargin_error_t &lt;- t_critical * se_pooled\nci_lower_t &lt;- (xbar1_t - xbar2_t) - margin_error_t\nci_upper_t &lt;- (xbar1_t - xbar2_t) + margin_error_t\n\nt_ci_results &lt;- data.table(\n  Parameter = \"μ₁ - μ₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(xbar1_t - xbar2_t, 2),\n  Pooled_SD = round(sp, 2),\n  Standard_Error = round(se_pooled, 3),\n  t_Critical = round(t_critical, 3),\n  Margin_Error = round(margin_error_t, 3),\n  Lower_Bound = round(ci_lower_t, 2),\n  Upper_Bound = round(ci_upper_t, 2),\n  Width = round(ci_upper_t - ci_lower_t, 2),\n  Interpretation = paste(\"95% confident that μ₁ - μ₂ is between\", \n                        round(ci_lower_t, 2), \"and\", round(ci_upper_t, 2))\n)\n\nprint(\"95% Confidence Interval for Difference in Means (σ unknown):\")\nprint(t_ci_results)\n\n# Welch's t-test (unequal variances) for comparison\nwelch_test &lt;- t.test(typeA_data$strength, typeB_data$strength, var.equal = FALSE)\nprint(\"Welch's t-test (unequal variances):\")\nprint(welch_test)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#the-paired-t-test",
    "href": "book/Ch05.html#the-paired-t-test",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NotePaired t-Test\n\n\n\nWhen observations come in natural pairs (before/after, matched subjects, etc.), use the paired t-test:\n\nCalculate differences: d_i = x₁ᵢ - x₂ᵢ\nTest statistic:\n\nT_0 = \\frac{\\overline{d} - \\mu_d}{S_d/\\sqrt{n}}\n\nDegrees of freedom: ν = n - 1\n\nWhere:\n\n\\overline{d} = \\frac{1}{n}\\sum_{i=1}^n d_i\nS_d^2 = \\frac{1}{n-1}\\sum_{i=1}^n (d_i - \\overline{d})^2\n\nAssumptions:\n\nDifferences are normally distributed\nPairs are independent\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA manufacturing process improvement is tested on 12 production lines. Measure output before and after the improvement:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Process Improvement Paired Data Summary:\"\n\n\n       n Mean_Before Mean_After Mean_Difference SD_Before SD_After\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;           &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:    12    81.70922   89.14206        7.432843   6.36605 4.794191\n   SD_Difference SE_Difference Min_Difference Max_Difference\n           &lt;num&gt;         &lt;num&gt;          &lt;num&gt;          &lt;num&gt;\n1:      6.906354      1.993693      -2.023308        19.6769\n\n\n[1] \"Individual Differences:\"\n\n\n    line_id   before    after difference\n      &lt;int&gt;    &lt;num&gt;    &lt;num&gt;      &lt;num&gt;\n 1:       1 89.19277 88.91711 -0.2756677\n 2:       2 66.91386 86.59075 19.6768963\n 3:       3 84.84256 97.20931 12.3667432\n 4:       4 86.46512 84.44181 -2.0233083\n 5:       5 82.10919 93.42153 11.3123453\n 6:       6 81.12413 85.69777  4.5736465\n 7:       7 79.66950 91.82031 12.1508159\n 8:       8 83.60425 84.41997  0.8157253\n 9:       9 76.91232 84.98462  8.0723014\n10:      10 90.91757 95.37602  4.4584503\n11:      11 81.78163 83.81734  2.0357137\n12:      12 76.97776 93.00821 16.0304489\n\n\n\n\n\n# Process improvement paired t-test example\nset.seed(789)\nprocess_data &lt;- data.table(\n  line_id = 1:12,\n  before = rnorm(12, mean = 85, sd = 8),\n  after = rnorm(12, mean = 90.25, sd = 7.5)\n)\n\n# Calculate differences\nprocess_data[, difference := after - before]\n\n# Summary statistics for paired data\npaired_summary &lt;- process_data %&gt;% \n  fsummarise(\n    n = fnobs(difference),\n    Mean_Before = fmean(before),\n    Mean_After = fmean(after),\n    Mean_Difference = fmean(difference),\n    SD_Before = fsd(before),\n    SD_After = fsd(after),\n    SD_Difference = fsd(difference),\n    SE_Difference = fsd(difference) / sqrt(fnobs(difference)),\n    Min_Difference = fmin(difference),\n    Max_Difference = fmax(difference)\n  )\n\nprint(\"Process Improvement Paired Data Summary:\")\nprint(paired_summary)\n\n# Individual differences\ndifferences_table &lt;- process_data[, .(line_id, before, after, difference)]\nprint(\"Individual Differences:\")\nprint(differences_table)\n\n# Write data to CSV for download\nfwrite(process_data, \"data/process_improvement.csv\")\n\n\n\n\n\n\n\nTest H₀: μ_d = 0 vs H₁: μ_d &gt; 0 at α = 0.05.\nManual Calculation:\nCalculate differences: d_i = After_i - Before_i\nIf d̄ = 5.25 and s_d = 3.2:\n\nTest Statistic: T_0 = \\frac{5.25 - 0}{3.2/\\sqrt{12}} = \\frac{5.25}{0.924} = 5.68\nCritical Value: t₀.₀₅,₁₁ = 1.796\nDecision: T₀ = 5.68 &gt; 1.796, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Paired t-Test Results:\"\n\n\n       Test_Type n_pairs Mean_Difference SD_Difference SE_Difference\n          &lt;char&gt;   &lt;int&gt;           &lt;num&gt;         &lt;num&gt;         &lt;num&gt;\n1: Paired t-test      12           7.433         6.906         1.994\n   Null_Difference t_Statistic    df P_Value Alpha t_Critical  Decision\n             &lt;num&gt;       &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:               0      3.7282    11  0.0017  0.05      1.796 Reject H0\n                         Conclusion\n                             &lt;char&gt;\n1: Significant improvement detected\n\n\n[1] \"R's paired t.test verification:\"\n\n\n\n    Paired t-test\n\ndata:  process_data$after and process_data$before\nt = 3.7282, df = 11, p-value = 0.001667\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 3.8524    Inf\nsample estimates:\nmean difference \n       7.432843 \n\n\n[1] \"95% CI for mean difference: [ 3.045 ,  11.821 ]\"\n\n\n\n\n\n# Paired t-test (manual and R calculations)\nn_paired &lt;- fnobs(process_data$difference)\nd_bar &lt;- fmean(process_data$difference)\ns_d &lt;- fsd(process_data$difference)\nmu_d0 &lt;- 0  # Null hypothesis: no difference\n\n# Manual calculation\nt_stat_paired &lt;- (d_bar - mu_d0) / (s_d / sqrt(n_paired))\ndf_paired &lt;- n_paired - 1\np_value_paired &lt;- 1 - pt(t_stat_paired, df = df_paired)  # One-sided upper test\nt_critical_paired &lt;- qt(1 - alpha, df = df_paired)\n\n# Test results\npaired_test_results &lt;- data.table(\n  Test_Type = \"Paired t-test\",\n  n_pairs = n_paired,\n  Mean_Difference = round(d_bar, 3),\n  SD_Difference = round(s_d, 3),\n  SE_Difference = round(s_d / sqrt(n_paired), 3),\n  Null_Difference = mu_d0,\n  t_Statistic = round(t_stat_paired, 4),\n  df = df_paired,\n  P_Value = round(p_value_paired, 4),\n  Alpha = alpha,\n  t_Critical = round(t_critical_paired, 3),\n  Decision = ifelse(t_stat_paired &gt; t_critical_paired, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_paired &lt; alpha, \n                     \"Significant improvement detected\", \n                     \"No significant improvement\")\n)\n\nprint(\"Paired t-Test Results:\")\nprint(paired_test_results)\n\n# Using R's paired t.test for verification\npaired_test_r &lt;- t.test(process_data$after, process_data$before, paired = TRUE, alternative = \"greater\")\nprint(\"R's paired t.test verification:\")\nprint(paired_test_r)\n\n# Confidence interval for mean difference\nt_critical_ci_paired &lt;- qt(1 - alpha/2, df = df_paired)\nmargin_error_paired &lt;- t_critical_ci_paired * (s_d / sqrt(n_paired))\nci_lower_paired &lt;- d_bar - margin_error_paired\nci_upper_paired &lt;- d_bar + margin_error_paired\n\nprint(paste(\"95% CI for mean difference: [\", round(ci_lower_paired, 3), \", \", round(ci_upper_paired, 3), \"]\"))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Paired t-test visualization\n\n\n\n\n\n\n\n\n# Paired t-test visualization\n\n# Create long format for plotting\nprocess_long &lt;- process_data %&gt;%\n  pivot_longer(cols = c(before, after), names_to = \"time\", values_to = \"output\") %&gt;%\n  data.table()\n\n# Before/After comparison plot\np1 &lt;- ggplot(data = process_long, mapping = aes(x = time, y = output, group = line_id)) +\n  geom_line(color = \"gray\", alpha = 0.7) +\n  geom_point(mapping = aes(color = time), size = 3) +\n  scale_color_manual(values = c(\"before\" = \"red\", \"after\" = \"blue\")) +\n  labs(\n    x = \"Time Period\",\n    y = \"Output\",\n    title = \"Before/After Process Improvement\",\n    color = \"Period\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Differences histogram\np2 &lt;- ggplot(data = process_data, mapping = aes(x = difference)) +\n  geom_histogram(bins = 8, fill = \"lightblue\", color = \"black\", alpha = 0.7) +\n  geom_vline(xintercept = d_bar, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = 0, color = \"black\", linetype = \"solid\", linewidth = 1) +\n  labs(\n    x = \"Difference (After - Before)\",\n    y = \"Frequency\",\n    title = \"Distribution of Differences\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#inference-on-the-ratio-of-variances-of-two-normal-populations",
    "href": "book/Ch05.html#inference-on-the-ratio-of-variances-of-two-normal-populations",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteF-Test for Equality of Variances\n\n\n\nTo test H₀: σ₁² = σ₂² vs H₁: σ₁² ≠ σ₂², use:\nF_0 = \\frac{S_1^2}{S_2^2}\nUnder H₀, F₀ ~ F_{n₁-1, n₂-1}\nConvention: Put the larger sample variance in the numerator so F₀ ≥ 1.\nProperties of F-distribution:\n\nAlways positive\nRight-skewed\nApproaches 1 as both df → ∞\nF₁₋α,ν₁,ν₂ = 1/F_α,ν₂,ν₁\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the variability in measurements from two instruments:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Instrument Comparison Data Summary:\"\n\n\n     instrument     n      Min       Q1   Median     Mean       Q3      Max\n         &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Instrument_1    16 23.17625 24.45219 25.33319 25.39453 25.69963 28.86313\n2: Instrument_2    21 22.07273 24.18302 25.35367 25.00446 25.81188 26.85136\n         SD      Var\n      &lt;num&gt;    &lt;num&gt;\n1: 1.522413 2.317741\n2: 1.359890 1.849302\n\n\n\n\n\n# Instrument comparison for F-test (variance comparison)\nset.seed(321)\ninstrument_data &lt;- data.table(\n  instrument = rep(c(\"Instrument_1\", \"Instrument_2\"), c(16, 21)),\n  measurement = c(rnorm(16, mean = 25, sd = sqrt(2.5)), rnorm(21, mean = 25.2, sd = sqrt(1.8)))\n)\n\n# Summary statistics\ninstrument_summary &lt;- instrument_data %&gt;% \n  fgroup_by(instrument) %&gt;% \n  fsummarise(\n    n = fnobs(measurement),\n    Min = fmin(measurement),\n    Q1 = fquantile(measurement, 0.25),\n    Median = fmedian(measurement),\n    Mean = fmean(measurement),\n    Q3 = fquantile(measurement, 0.75),\n    Max = fmax(measurement),\n    SD = fsd(measurement),\n    Var = fvar(measurement)\n  )\n\nprint(\"Instrument Comparison Data Summary:\")\nprint(instrument_summary)\n\n# Write data to CSV for download\nfwrite(instrument_data, \"data/instrument_comparison.csv\")\n\n\n\n\n\n\n\nTest H₀: σ₁² = σ₂² vs H₁: σ₁² ≠ σ₂² at α = 0.05.\nManual Calculation:\nGiven: n₁ = 16, s₁² = 2.5, n₂ = 21, s₂² = 1.8\n\nTest Statistic: F_0 = \\frac{2.5}{1.8} = 1.39\nCritical Value: F₀.₀₂₅,₁₅,₂₀ = 2.57\nDecision: F₀ = 1.39 &lt; 2.57, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"F-Test Results for Equality of Variances:\"\n\n\n                          Test_Type    n1    n2 s1_squared s2_squared\n                             &lt;char&gt; &lt;int&gt; &lt;int&gt;      &lt;num&gt;      &lt;num&gt;\n1: F-test for equality of variances    16    21     2.3177     1.8493\n   Larger_Variance F_Statistic   df1   df2 P_Value Alpha F_Critical\n            &lt;char&gt;       &lt;num&gt; &lt;num&gt; &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;\n1:    Instrument_1      1.2533    15    20   0.627  0.05      2.573\n            Decision                             Conclusion\n              &lt;char&gt;                                 &lt;char&gt;\n1: Fail to reject H0 No significant difference in variances\n\n\n[1] \"R's var.test verification:\"\n\n\n\n    F test to compare two variances\n\ndata:  inst1_data$measurement and inst2_data$measurement\nF = 1.2533, num df = 15, denom df = 20, p-value = 0.627\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.4870809 3.4539881\nsample estimates:\nratio of variances \n          1.253306 \n\n\n\n\n\n# F-test for equality of variances\ninst1_data &lt;- instrument_data[instrument == \"Instrument_1\"]\ninst2_data &lt;- instrument_data[instrument == \"Instrument_2\"]\n\nn1_f &lt;- fnobs(inst1_data$measurement)\nn2_f &lt;- fnobs(inst2_data$measurement)\ns1_f_squared &lt;- fvar(inst1_data$measurement)\ns2_f_squared &lt;- fvar(inst2_data$measurement)\n\n# F-test (put larger variance in numerator)\nif(s1_f_squared &gt;= s2_f_squared) {\n  f_stat &lt;- s1_f_squared / s2_f_squared\n  df1_f_test &lt;- n1_f - 1\n  df2_f_test &lt;- n2_f - 1\n  larger_var &lt;- \"Instrument_1\"\n} else {\n  f_stat &lt;- s2_f_squared / s1_f_squared\n  df1_f_test &lt;- n2_f - 1\n  df2_f_test &lt;- n1_f - 1\n  larger_var &lt;- \"Instrument_2\"\n}\n\n# Calculate p-value for two-sided test\np_value_f_test &lt;- 2 * (1 - pf(f_stat, df1_f_test, df2_f_test))\nf_critical &lt;- qf(1 - alpha/2, df1_f_test, df2_f_test)\n\n# Test results\nf_test_results &lt;- data.table(\n  Test_Type = \"F-test for equality of variances\",\n  n1 = n1_f,\n  n2 = n2_f,\n  s1_squared = round(s1_f_squared, 4),\n  s2_squared = round(s2_f_squared, 4),\n  Larger_Variance = larger_var,\n  F_Statistic = round(f_stat, 4),\n  df1 = df1_f_test,\n  df2 = df2_f_test,\n  P_Value = round(p_value_f_test, 4),\n  Alpha = alpha,\n  F_Critical = round(f_critical, 3),\n  Decision = ifelse(f_stat &gt; f_critical, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_f_test &lt; alpha, \n                     \"Variances are significantly different\", \n                     \"No significant difference in variances\")\n)\n\nprint(\"F-Test Results for Equality of Variances:\")\nprint(f_test_results)\n\n# Using R's var.test for verification\nvar_test_r &lt;- var.test(inst1_data$measurement, inst2_data$measurement)\nprint(\"R's var.test verification:\")\nprint(var_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for σ₁²/σ₂²\n\n\n\nA 100(1-α)% confidence interval for σ₁²/σ₂²:\n\\frac{s_1^2/s_2^2}{F_{\\alpha/2,n_1-1,n_2-1}} \\leq \\frac{\\sigma_1^2}{\\sigma_2^2} \\leq \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2,n_1-1,n_2-1}}\nNote: F₁₋α/₂,ν₁,ν₂ = 1/F_α/₂,ν₂,ν₁\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Ratio of Variances:\"\n\n\n   Parameter Confidence_Level Sample_Ratio Lower_Bound Upper_Bound  Width\n      &lt;char&gt;           &lt;char&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:   σ₁²/σ₂²              95%       1.2533      0.4871       3.454 2.9669\n                                           Interpretation\n                                                   &lt;char&gt;\n1: 95% confident that σ₁²/σ₂² is between 0.4871 and 3.454\n\n\n\n\n\n# Confidence interval for ratio of variances\nf_alpha2_lower &lt;- qf(alpha/2, df1_f_test, df2_f_test)\nf_alpha2_upper &lt;- qf(1 - alpha/2, df1_f_test, df2_f_test)\n\n# Note: For CI, use original ratio without putting larger in numerator\nratio_original &lt;- s1_f_squared / s2_f_squared\nci_lower_f &lt;- ratio_original / f_alpha2_upper\nci_upper_f &lt;- ratio_original / f_alpha2_lower\n\nvariance_ratio_ci &lt;- data.table(\n  Parameter = \"σ₁²/σ₂²\",\n  Confidence_Level = \"95%\",\n  Sample_Ratio = round(ratio_original, 4),\n  Lower_Bound = round(ci_lower_f, 4),\n  Upper_Bound = round(ci_upper_f, 4),\n  Width = round(ci_upper_f - ci_lower_f, 4),\n  Interpretation = paste(\"95% confident that σ₁²/σ₂² is between\", \n                        round(ci_lower_f, 4), \"and\", round(ci_upper_f, 4))\n)\n\nprint(\"95% Confidence Interval for Ratio of Variances:\")\nprint(variance_ratio_ci)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#inference-on-two-population-proportions",
    "href": "book/Ch05.html#inference-on-two-population-proportions",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteTwo-Sample Z-Test for Proportions\n\n\n\nTo test H₀: p₁ = p₂, use the pooled estimate:\n\\hat{p} = \\frac{x_1 + x_2}{n_1 + n_2}\nTest Statistic:\nZ_0 = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\nConditions: n₁p̂ ≥ 5, n₁(1-p̂) ≥ 5, n₂p̂ ≥ 5, n₂(1-p̂) ≥ 5\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare defect rates between two production lines:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Defect Rate Comparison Summary:\"\n\n\n     line sample_size defects  prop\n   &lt;char&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;\n1: Line_1         200      16 0.080\n2: Line_2         250      14 0.056\n\n\n[1] \"Overall Summary:\"\n\n\n   Total_Items Total_Defects Line1_Rate Line2_Rate Overall_Rate\n         &lt;num&gt;         &lt;num&gt;      &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:         450            30       0.08      0.056   0.06666667\n\n\n\n\n\n# Defect rate comparison (two-sample proportion test)\ndefect_comparison_data &lt;- data.table(\n  line = c(\"Line_1\", \"Line_2\"),\n  sample_size = c(200, 250),\n  defects = c(16, 14),\n  prop = c(16/200, 14/250)\n)\n\n# Summary\ndefect_summary &lt;- defect_comparison_data %&gt;% \n  fsummarise(\n    Total_Items = fsum(sample_size),\n    Total_Defects = fsum(defects),\n    Line1_Rate = defects[1] / sample_size[1],\n    Line2_Rate = defects[2] / sample_size[2],\n    Overall_Rate = fsum(defects) / fsum(sample_size)\n  )\n\nprint(\"Defect Rate Comparison Summary:\")\nprint(defect_comparison_data)\nprint(\"Overall Summary:\")\nprint(defect_summary)\n\n# Write data to CSV for download\nfwrite(defect_comparison_data, \"data/defect_comparison.csv\")\n\n\n\n\n\n\n\nTest H₀: p₁ = p₂ vs H₁: p₁ ≠ p₂ at α = 0.05.\nManual Calculation:\nGiven: n₁ = 200, x₁ = 16, n₂ = 250, x₂ = 14\n\nSample Proportions:\n\np̂₁ = 16/200 = 0.08\np̂₂ = 14/250 = 0.056\n\nPooled Proportion: \\hat{p} = \\frac{16 + 14}{200 + 250} = \\frac{30}{450} = 0.067\nTest Statistic: Z_0 = \\frac{0.08 - 0.056}{\\sqrt{0.067(0.933)(\\frac{1}{200} + \\frac{1}{250})}} = \\frac{0.024}{0.024} = 1.00\nDecision: |Z₀| = 1.00 &lt; 1.96, so fail to reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Large Sample Conditions:\"\n\n\n     Condition     Value    Met\n        &lt;char&gt;     &lt;num&gt; &lt;lgcl&gt;\n1:     n₁p̂ ≥ 5  13.33333   TRUE\n2: n₁(1-p̂) ≥ 5 186.66667   TRUE\n3:     n₂p̂ ≥ 5  16.66667   TRUE\n4: n₂(1-p̂) ≥ 5 233.33333   TRUE\n\n\n[1] \"Two-Sample Proportion Test Results:\"\n\n\n                    Test_Type    n1    n2    x1    x2 p1_hat p2_hat p_pooled\n                       &lt;char&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;    &lt;num&gt;\n1: Two-sample proportion test   200   250    16    14   0.08  0.056   0.0667\n   SE_Difference Z_Statistic P_Value Alpha Z_Critical          Decision\n           &lt;num&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;            &lt;char&gt;\n1:        0.0237      1.0142  0.3105  0.05       1.96 Fail to reject H0\n                                 Conclusion\n                                     &lt;char&gt;\n1: No significant difference in proportions\n\n\n[1] \"R's prop.test verification:\"\n\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(x1_prop, x2_prop) out of c(n1_prop, n2_prop)\nX-squared = 0.67902, df = 1, p-value = 0.4099\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.02768004  0.07568004\nsample estimates:\nprop 1 prop 2 \n 0.080  0.056 \n\n\n\n\n\n# Two-sample proportion test\nn1_prop &lt;- defect_comparison_data[line == \"Line_1\", sample_size]\nn2_prop &lt;- defect_comparison_data[line == \"Line_2\", sample_size]\nx1_prop &lt;- defect_comparison_data[line == \"Line_1\", defects]\nx2_prop &lt;- defect_comparison_data[line == \"Line_2\", defects]\np1_hat &lt;- x1_prop / n1_prop\np2_hat &lt;- x2_prop / n2_prop\n\n# Pooled proportion\np_pooled &lt;- (x1_prop + x2_prop) / (n1_prop + n2_prop)\n\n# Check conditions\nconditions &lt;- data.table(\n  Condition = c(\"n₁p̂ ≥ 5\", \"n₁(1-p̂) ≥ 5\", \"n₂p̂ ≥ 5\", \"n₂(1-p̂) ≥ 5\"),\n  Value = c(n1_prop * p_pooled, n1_prop * (1 - p_pooled), \n           n2_prop * p_pooled, n2_prop * (1 - p_pooled)),\n  Met = c(n1_prop * p_pooled &gt;= 5, n1_prop * (1 - p_pooled) &gt;= 5,\n         n2_prop * p_pooled &gt;= 5, n2_prop * (1 - p_pooled) &gt;= 5)\n)\n\nprint(\"Large Sample Conditions:\")\nprint(conditions)\n\n# Manual calculation\nse_prop &lt;- sqrt(p_pooled * (1 - p_pooled) * (1/n1_prop + 1/n2_prop))\nz_stat_prop &lt;- (p1_hat - p2_hat) / se_prop\np_value_prop &lt;- 2 * (1 - pnorm(abs(z_stat_prop)))  # Two-sided test\nz_critical_prop &lt;- qnorm(1 - alpha/2)\n\n# Test results\nprop_test_results &lt;- data.table(\n  Test_Type = \"Two-sample proportion test\",\n  n1 = n1_prop,\n  n2 = n2_prop,\n  x1 = x1_prop,\n  x2 = x2_prop,\n  p1_hat = round(p1_hat, 4),\n  p2_hat = round(p2_hat, 4),\n  p_pooled = round(p_pooled, 4),\n  SE_Difference = round(se_prop, 4),\n  Z_Statistic = round(z_stat_prop, 4),\n  P_Value = round(p_value_prop, 4),\n  Alpha = alpha,\n  Z_Critical = round(z_critical_prop, 3),\n  Decision = ifelse(abs(z_stat_prop) &gt; z_critical_prop, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_prop &lt; alpha, \n                     \"Proportions are significantly different\", \n                     \"No significant difference in proportions\")\n)\n\nprint(\"Two-Sample Proportion Test Results:\")\nprint(prop_test_results)\n\n# Using R's prop.test for verification\nprop_test_r &lt;- prop.test(x = c(x1_prop, x2_prop), n = c(n1_prop, n2_prop))\nprint(\"R's prop.test verification:\")\nprint(prop_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Two-Proportion Test\n\n\n\nFor equal sample sizes (n₁ = n₂ = n) and difference δ = |p₁ - p₂|:\nn = \\frac{[z_{\\alpha/2}\\sqrt{2\\bar{p}(1-\\bar{p})} + z_\\beta\\sqrt{p_1(1-p_1) + p_2(1-p_2)}]^2}{\\delta^2}\nwhere \\bar{p} = \\frac{p_1 + p_2}{2}\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for p₁ - p₂\n\n\n\nA 100(1-α)% confidence interval for p₁ - p₂:\n(\\hat{p_1} - \\hat{p_2}) \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p_1}(1-\\hat{p_1})}{n_1} + \\frac{\\hat{p_2}(1-\\hat{p_2})}{n_2}}\nNote: Use individual sample proportions, not the pooled estimate.\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Difference in Proportions:\"\n\n\n   Parameter Confidence_Level Difference Standard_Error Z_Critical Margin_Error\n      &lt;char&gt;           &lt;char&gt;      &lt;num&gt;          &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   p₁ - p₂              95%      0.024         0.0241       1.96       0.0472\n   Lower_Bound Upper_Bound  Width\n         &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     -0.0232      0.0712 0.0944\n                                             Interpretation\n                                                     &lt;char&gt;\n1: 95% confident that p₁ - p₂ is between -0.0232 and 0.0712\n\n\n\n\n\n# Confidence interval for difference in proportions\nse_diff_ci &lt;- sqrt(p1_hat * (1 - p1_hat) / n1_prop + p2_hat * (1 - p2_hat) / n2_prop)\nmargin_error_prop &lt;- z_critical_prop * se_diff_ci\nci_lower_prop &lt;- (p1_hat - p2_hat) - margin_error_prop\nci_upper_prop &lt;- (p1_hat - p2_hat) + margin_error_prop\n\nprop_ci_results &lt;- data.table(\n  Parameter = \"p₁ - p₂\",\n  Confidence_Level = \"95%\",\n  Difference = round(p1_hat - p2_hat, 4),\n  Standard_Error = round(se_diff_ci, 4),\n  Z_Critical = round(z_critical_prop, 3),\n  Margin_Error = round(margin_error_prop, 4),\n  Lower_Bound = round(ci_lower_prop, 4),\n  Upper_Bound = round(ci_upper_prop, 4),\n  Width = round(ci_upper_prop - ci_lower_prop, 4),\n  Interpretation = paste(\"95% confident that p₁ - p₂ is between\", \n                        round(ci_lower_prop, 4), \"and\", round(ci_upper_prop, 4))\n)\n\nprint(\"95% Confidence Interval for Difference in Proportions:\")\nprint(prop_ci_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#summary-tables-for-inference-procedures-for-two-samples",
    "href": "book/Ch05.html#summary-tables-for-inference-procedures-for-two-samples",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "R OutputR Code\n\n\n\n\n\n\nTable 1: Summary of two-sample inference procedures\n\n\n\n\nSummary of Two-Sample Inference Procedures\n\n\nParameter\nTest_Statistic\nDistribution\nConfidence_Interval\nAssumptions\n\n\n\n\nμ₁ - μ₂ (σ known)\nZ = (x̄₁-x̄₂)/√(σ₁²/n₁+σ₂²/n₂)\nN(0,1)\n(x̄₁-x̄₂) ± z_{α/2}√(σ₁²/n₁+σ₂²/n₂)\nNormal populations, σ known\n\n\nμ₁ - μ₂ (σ unknown, equal)\nt = (x̄₁-x̄₂)/(sp√(1/n₁+1/n₂))\nt(n₁+n₂-2)\n(x̄₁-x̄₂) ± t_{α/2}sp√(1/n₁+1/n₂)\nNormal populations, σ₁²=σ₂²\n\n\nμ₁ - μ₂ (σ unknown, unequal)\nt = (x̄₁-x̄₂)/√(s₁²/n₁+s₂²/n₂)\nt(ν)\n(x̄₁-x̄₂) ± t_{α/2}√(s₁²/n₁+s₂²/n₂)\nNormal populations, σ₁²≠σ₂²\n\n\nμ_d (paired)\nt = d̄/(sd/√n)\nt(n-1)\nd̄ ± t_{α/2}sd/√n\nNormal differences\n\n\nσ₁²/σ₂²\nF = s₁²/s₂²\nF(n₁-1,n₂-1)\n(s₁²/s₂²)/F_{α/2} to (s₁²/s₂²)/F_{1-α/2}\nNormal populations\n\n\np₁ - p₂\nZ = (p̂₁-p̂₂)/√(p̂(1-p̂)(1/n₁+1/n₂))\nN(0,1)\n(p̂₁-p̂₂) ± z_{α/2}√(p̂₁(1-p̂₁)/n₁+p̂₂(1-p̂₂)/n₂)\nLarge samples, np≥5\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Summary table of two-sample inference procedures\ninference_procedures &lt;- data.table(\n  Parameter = c(\"μ₁ - μ₂ (σ known)\", \"μ₁ - μ₂ (σ unknown, equal)\", \n               \"μ₁ - μ₂ (σ unknown, unequal)\", \"μ_d (paired)\",\n               \"σ₁²/σ₂²\", \"p₁ - p₂\"),\n  Test_Statistic = c(\"Z = (x̄₁-x̄₂)/√(σ₁²/n₁+σ₂²/n₂)\", \n                    \"t = (x̄₁-x̄₂)/(sp√(1/n₁+1/n₂))\",\n                    \"t = (x̄₁-x̄₂)/√(s₁²/n₁+s₂²/n₂)\",\n                    \"t = d̄/(sd/√n)\",\n                    \"F = s₁²/s₂²\",\n                    \"Z = (p̂₁-p̂₂)/√(p̂(1-p̂)(1/n₁+1/n₂))\"),\n  Distribution = c(\"N(0,1)\", \"t(n₁+n₂-2)\", \"t(ν)\", \"t(n-1)\", \"F(n₁-1,n₂-1)\", \"N(0,1)\"),\n  Confidence_Interval = c(\"(x̄₁-x̄₂) ± z_{α/2}√(σ₁²/n₁+σ₂²/n₂)\",\n                         \"(x̄₁-x̄₂) ± t_{α/2}sp√(1/n₁+1/n₂)\",\n                         \"(x̄₁-x̄₂) ± t_{α/2}√(s₁²/n₁+s₂²/n₂)\",\n                         \"d̄ ± t_{α/2}sd/√n\",\n                         \"(s₁²/s₂²)/F_{α/2} to (s₁²/s₂²)/F_{1-α/2}\",\n                         \"(p̂₁-p̂₂) ± z_{α/2}√(p̂₁(1-p̂₁)/n₁+p̂₂(1-p̂₂)/n₂)\"),\n  Assumptions = c(\"Normal populations, σ known\", \"Normal populations, σ₁²=σ₂²\",\n                 \"Normal populations, σ₁²≠σ₂²\", \"Normal differences\",\n                 \"Normal populations\", \"Large samples, np≥5\")\n)\n\nkbl(inference_procedures, \n    caption = \"Summary of Two-Sample Inference Procedures\") %&gt;%\n  kable_styling() %&gt;%\n  column_spec(2, width = \"3.5cm\") %&gt;%\n  column_spec(3, width = \"2cm\") %&gt;%\n  column_spec(4, width = \"3.5cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\")",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#what-if-we-have-more-than-two-samples",
    "href": "book/Ch05.html#what-if-we-have-more-than-two-samples",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteIntroduction to ANOVA\n\n\n\nWhen comparing more than two groups, multiple t-tests lead to inflated Type I error rates. Analysis of Variance (ANOVA) provides a single test for:\nH₀: μ₁ = μ₂ = … = μₐ\nH₁: At least one mean differs\nOne-Way ANOVA Model:\ny_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\nwhere:\n\ny_{ij} = j-th observation in i-th treatment\nμ = overall mean\nτᵢ = effect of i-th treatment\nεᵢⱼ \\sim N(0, σ²)\n\nF-Statistic:\nF_0 = \\frac{MS_{Treatments}}{MS_{Error}} = \\frac{SS_{Treatments}/(a-1)}{SS_{Error}/(N-a)}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nCompare the yield from four different chemical processes:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Process Yield Data Summary by Process:\"\n\n\n     process     n      Min       Q1   Median     Mean       Q3      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Process_A     6 71.95873 73.73665 75.00213 75.90529 77.54712 81.75850\n2: Process_B     6 74.64733 76.58680 79.26790 78.91072 81.56075 82.26307\n3: Process_C     6 68.06958 68.56533 70.23732 70.18130 71.91936 72.05552\n4: Process_D     6 76.94869 79.26128 81.26787 80.64549 82.49767 82.88851\n         SD       Var\n      &lt;num&gt;     &lt;num&gt;\n1: 3.561280 12.682715\n2: 3.180471 10.115399\n3: 1.927890  3.716761\n4: 2.362796  5.582806\n\n\n[1] \"Overall Summary:\"\n\n\n   Total_n Overall_Mean Overall_SD Overall_Var Between_Process_Range\n     &lt;int&gt;        &lt;num&gt;      &lt;num&gt;       &lt;num&gt;                 &lt;num&gt;\n1:      24      76.4107   4.845237    23.47632               10.4642\n\n\n\n\n\n# One-way ANOVA example (comparing multiple processes)\nset.seed(654)\nprocess_yield_data &lt;- data.table(\n  process = rep(c(\"Process_A\", \"Process_B\", \"Process_C\", \"Process_D\"), each = 6),\n  yield = c(rnorm(6, mean = 75, sd = 4),   # Process A\n           rnorm(6, mean = 78, sd = 4),   # Process B  \n           rnorm(6, mean = 72, sd = 4),   # Process C\n           rnorm(6, mean = 80, sd = 4))   # Process D\n)\n\n# Summary statistics by process\nyield_summary &lt;- process_yield_data %&gt;% \n  fgroup_by(process) %&gt;% \n  fsummarise(\n    n = fnobs(yield),\n    Min = fmin(yield),\n    Q1 = fquantile(yield, 0.25),\n    Median = fmedian(yield),\n    Mean = fmean(yield),\n    Q3 = fquantile(yield, 0.75),\n    Max = fmax(yield),\n    SD = fsd(yield),\n    Var = fvar(yield)\n  )\n\nprint(\"Process Yield Data Summary by Process:\")\nprint(yield_summary)\n\n# Overall summary\noverall_summary &lt;- process_yield_data %&gt;% \n  fsummarise(\n    Total_n = fnobs(yield),\n    Overall_Mean = fmean(yield),\n    Overall_SD = fsd(yield),\n    Overall_Var = fvar(yield),\n    Between_Process_Range = fmax(yield_summary$Mean) - fmin(yield_summary$Mean)\n  )\n\nprint(\"Overall Summary:\")\nprint(overall_summary)\n\n# Write data to CSV for download\nfwrite(process_yield_data, \"data/process_yield.csv\")\n\n\n\n\n\n\n\nANOVA Table:\n\n\n\nSource\nSS\ndf\nMS\nF\nP-value\n\n\n\n\nTotal\nSSₜ\nN-1\n\n\n\n\n\nTreatments\nSSₜᵣ\na-1\nMSₜᵣ\nF₀\nP\n\n\nError\nSSₑ\nN-a\nMSₑ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"One-Way ANOVA Table:\"\n\n\n       Source     SS    df     MS F_Statistic P_Value\n       &lt;char&gt;  &lt;num&gt; &lt;num&gt;  &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Treatments 379.47     3 126.49      15.763       0\n2:      Error 160.49    20   8.02          NA      NA\n3:      Total 539.96    23     NA          NA      NA\n\n\n[1] \"ANOVA Test Results:\"\n\n\n   F_Statistic F_Critical P_Value Alpha  Decision\n         &lt;num&gt;      &lt;num&gt;   &lt;num&gt; &lt;num&gt;    &lt;char&gt;\n1:      15.763      3.098       0  0.05 Reject H0\n                          Conclusion\n                              &lt;char&gt;\n1: At least one process mean differs\n\n\n[1] \"R's ANOVA verification:\"\n\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nprocess      3  379.5  126.49   15.76 1.7e-05 ***\nResiduals   20  160.5    8.02                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# One-way ANOVA calculations (manual and R)\n# ANOVA calculations\na &lt;- length(unique(process_yield_data$process))  # number of treatments\nN &lt;- fnobs(process_yield_data$yield)  # total observations\nn_per_group &lt;- N / a  # observations per group (balanced design)\n\n# Calculate sum of squares\ngrand_mean &lt;- fmean(process_yield_data$yield)\n\n# Treatment sum of squares (SST)\ngroup_means &lt;- yield_summary$Mean\nSST &lt;- n_per_group * sum((group_means - grand_mean)^2)\n\n# Error sum of squares (SSE) \nSSE &lt;- sum(yield_summary$Var * (yield_summary$n - 1))\n\n# Total sum of squares (SS_Total)\nSS_Total &lt;- sum((process_yield_data$yield - grand_mean)^2)\n\n# Degrees of freedom\ndf_treatments &lt;- a - 1\ndf_error &lt;- N - a\ndf_total &lt;- N - 1\n\n# Mean squares\nMS_treatments &lt;- SST / df_treatments\nMS_error &lt;- SSE / df_error\n\n# F-statistic\nf_stat_anova &lt;- MS_treatments / MS_error\np_value_anova &lt;- 1 - pf(f_stat_anova, df_treatments, df_error)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Treatments\", \"Error\", \"Total\"),\n  SS = c(round(SST, 2), round(SSE, 2), round(SS_Total, 2)),\n  df = c(df_treatments, df_error, df_total),\n  MS = c(round(MS_treatments, 2), round(MS_error, 2), NA),\n  F_Statistic = c(round(f_stat_anova, 4), NA, NA),\n  P_Value = c(round(p_value_anova, 4), NA, NA)\n)\n\nprint(\"One-Way ANOVA Table:\")\nprint(anova_table)\n\n# ANOVA conclusion\nalpha_anova &lt;- 0.05\nf_critical_anova &lt;- qf(1 - alpha_anova, df_treatments, df_error)\nanova_conclusion &lt;- data.table(\n  F_Statistic = round(f_stat_anova, 4),\n  F_Critical = round(f_critical_anova, 3),\n  P_Value = round(p_value_anova, 4),\n  Alpha = alpha_anova,\n  Decision = ifelse(f_stat_anova &gt; f_critical_anova, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_anova &lt; alpha_anova,\n                     \"At least one process mean differs\",\n                     \"No significant difference between process means\")\n)\n\nprint(\"ANOVA Test Results:\")\nprint(anova_conclusion)\n\n# Using R's aov function for verification\nanova_r &lt;- aov(yield ~ process, data = process_yield_data)\nanova_summary_r &lt;- summary(anova_r)\nprint(\"R's ANOVA verification:\")\nprint(anova_summary_r)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: One-way ANOVA visualization\n\n\n\n\n\n\n\n\n# One-way ANOVA visualization\n# Box plot by process\np1 &lt;- ggplot(data = process_yield_data, mapping = aes(x = process, y = yield, fill = process)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.6) +\n  stat_summary(fun = mean, geom = \"point\", shape = 18, size = 3, color = \"red\") +\n  geom_hline(yintercept = grand_mean, linetype = \"dashed\", color = \"blue\") +\n  labs(\n    x = \"Process\",\n    y = \"Yield\",\n    title = \"Yield by Process\",\n    subtitle = \"Red diamonds = group means, blue line = grand mean\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"none\"\n  )\n\n# Residuals vs fitted plot\nfitted_values &lt;- rep(group_means, each = 6)\nresiduals &lt;- process_yield_data$yield - fitted_values\nresidual_data &lt;- data.table(fitted = fitted_values, residuals = residuals, process = process_yield_data$process)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(mapping = aes(color = process), size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\",\n    color = \"Process\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandomized Complete Block Design\n\n\n\nWhen experimental units are heterogeneous, blocking reduces error variance:\nModel:\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}\nwhere:\n\nτᵢ = treatment effect\nβⱼ = block effect\nOne observation per treatment-block combination\n\nAdvantages:\n\nControls for known sources of variation\nIncreases precision of treatment comparisons\nReduces experimental error\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nTest four coating methods on different types of metal substrates (blocks):\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Coating Experiment Data Summary by Coating:\"\n\n\n     coating     n     Mean       SD      Min      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: Coating_A     5 85.34018 5.821115 78.63352 93.27538\n2: Coating_B     5 85.93684 6.051341 79.31872 91.13382\n3: Coating_C     5 81.46412 7.616026 72.13510 91.27308\n4: Coating_D     5 90.18421 4.988870 82.43243 95.37133\n\n\n[1] \"Summary by Substrate (Block):\"\n\n\n   substrate     n     Mean       SD      Min      Max\n      &lt;char&gt; &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:  Aluminum     4 78.79708 4.624031 72.13510 82.43243\n2:    Copper     4 91.60979 2.779976 88.66094 95.37133\n3:    Nickel     4 80.94072 6.287063 75.63415 90.04449\n4:     Steel     4 86.92283 2.550402 84.62341 89.42896\n5:  Titanium     4 90.38625 4.640854 83.65485 93.64384\n\n\n\n\n\n# Randomized complete block design example (coating experiment)\nset.seed(987)\ncoating_data &lt;- data.table(\n  coating = rep(c(\"Coating_A\", \"Coating_B\", \"Coating_C\", \"Coating_D\"), 5),\n  substrate = rep(c(\"Steel\", \"Aluminum\", \"Copper\", \"Titanium\", \"Nickel\"), each = 4),\n  adhesion = c(\n    # Steel substrate\n    rnorm(4, mean = c(85, 88, 82, 90), sd = 3),\n    # Aluminum substrate  \n    rnorm(4, mean = c(78, 82, 75, 85), sd = 3),\n    # Copper substrate\n    rnorm(4, mean = c(92, 95, 89, 98), sd = 3),\n    # Titanium substrate\n    rnorm(4, mean = c(88, 91, 85, 94), sd = 3),\n    # Nickel substrate\n    rnorm(4, mean = c(80, 83, 77, 87), sd = 3)\n  )\n)\n\n# Summary by coating\ncoating_summary &lt;- coating_data %&gt;% \n  fgroup_by(coating) %&gt;% \n  fsummarise(\n    n = fnobs(adhesion),\n    Mean = fmean(adhesion),\n    SD = fsd(adhesion),\n    Min = fmin(adhesion),\n    Max = fmax(adhesion)\n  )\n\n# Summary by substrate (block)\nsubstrate_summary &lt;- coating_data %&gt;% \n  fgroup_by(substrate) %&gt;% \n  fsummarise(\n    n = fnobs(adhesion),\n    Mean = fmean(adhesion),\n    SD = fsd(adhesion),\n    Min = fmin(adhesion),\n    Max = fmax(adhesion)\n  )\n\nprint(\"Coating Experiment Data Summary by Coating:\")\nprint(coating_summary)\nprint(\"Summary by Substrate (Block):\")\nprint(substrate_summary)\n\n# Write data to CSV for download\nfwrite(coating_data, \"data/coating_experiment.csv\")\n\n\n\n\n\n\n\nANOVA Table for RCBD:\n\n\n\nSource\nSS\ndf\nMS\nF\nP-value\n\n\n\n\nTotal\nSSₜ\nab-1\n\n\n\n\n\nTreatments\nSSₜᵣ\na-1\nMSₜᵣ\nMSₜᵣ/MSₑ\nP₁\n\n\nBlocks\nSSᵦₗ\nb-1\nMSᵦₗ\nMSᵦₗ/MSₑ\nP₂\n\n\nError\nSSₑ\n(a-1)(b-1)\nMSₑ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"RCBD ANOVA Table:\"\n\n\n       Source     SS    df     MS F_Statistic P_Value\n       &lt;char&gt;  &lt;num&gt; &lt;num&gt;  &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1:   Coatings 191.16     3  63.72      7.7335  0.0039\n2: Substrates 514.71     4 128.68     15.6170  0.0001\n3:      Error  98.88    12   8.24          NA      NA\n4:      Total 804.75    19     NA          NA      NA\n\n\n[1] \"RCBD Test Results:\"\n\n\n             Effect F_Statistic P_Value  Decision\n             &lt;char&gt;       &lt;num&gt;   &lt;num&gt;    &lt;char&gt;\n1:   Coating Effect      7.7335  0.0039 Reject H0\n2: Substrate Effect     15.6170  0.0001 Reject H0\n                             Conclusion\n                                 &lt;char&gt;\n1:   Coating means differ significantly\n2: Substrate means differ significantly\n\n\n[1] \"R's RCBD ANOVA verification:\"\n\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncoating      3  191.2   63.72   7.733 0.003871 ** \nsubstrate    4  514.7  128.68  15.617 0.000106 ***\nResiduals   12   98.9    8.24                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# RCBD ANOVA calculations\na_rcbd &lt;- length(unique(coating_data$coating))  # treatments\nb_rcbd &lt;- length(unique(coating_data$substrate))  # blocks  \nN_rcbd &lt;- fnobs(coating_data$adhesion)\n\n# Calculate sum of squares for RCBD\ngrand_mean_rcbd &lt;- fmean(coating_data$adhesion)\n\n# Treatment sum of squares\ntreatment_means &lt;- coating_summary$Mean\nSS_treatments_rcbd &lt;- b_rcbd * sum((treatment_means - grand_mean_rcbd)^2)\n\n# Block sum of squares  \nblock_means &lt;- substrate_summary$Mean\nSS_blocks_rcbd &lt;- a_rcbd * sum((block_means - grand_mean_rcbd)^2)\n\n# Total sum of squares\nSS_total_rcbd &lt;- sum((coating_data$adhesion - grand_mean_rcbd)^2)\n\n# Error sum of squares (by subtraction)\nSS_error_rcbd &lt;- SS_total_rcbd - SS_treatments_rcbd - SS_blocks_rcbd\n\n# Degrees of freedom\ndf_treatments_rcbd &lt;- a_rcbd - 1\ndf_blocks_rcbd &lt;- b_rcbd - 1  \ndf_error_rcbd &lt;- (a_rcbd - 1) * (b_rcbd - 1)\ndf_total_rcbd &lt;- N_rcbd - 1\n\n# Mean squares\nMS_treatments_rcbd &lt;- SS_treatments_rcbd / df_treatments_rcbd\nMS_blocks_rcbd &lt;- SS_blocks_rcbd / df_blocks_rcbd\nMS_error_rcbd &lt;- SS_error_rcbd / df_error_rcbd\n\n# F-statistics\nf_treatments_rcbd &lt;- MS_treatments_rcbd / MS_error_rcbd\nf_blocks_rcbd &lt;- MS_blocks_rcbd / MS_error_rcbd\n\n# P-values\np_treatments_rcbd &lt;- 1 - pf(f_treatments_rcbd, df_treatments_rcbd, df_error_rcbd)\np_blocks_rcbd &lt;- 1 - pf(f_blocks_rcbd, df_blocks_rcbd, df_error_rcbd)\n\n# RCBD ANOVA table\nrcbd_anova_table &lt;- data.table(\n  Source = c(\"Coatings\", \"Substrates\", \"Error\", \"Total\"),\n  SS = c(round(SS_treatments_rcbd, 2), round(SS_blocks_rcbd, 2), \n         round(SS_error_rcbd, 2), round(SS_total_rcbd, 2)),\n  df = c(df_treatments_rcbd, df_blocks_rcbd, df_error_rcbd, df_total_rcbd),\n  MS = c(round(MS_treatments_rcbd, 2), round(MS_blocks_rcbd, 2), \n         round(MS_error_rcbd, 2), NA),\n  F_Statistic = c(round(f_treatments_rcbd, 4), round(f_blocks_rcbd, 4), NA, NA),\n  P_Value = c(round(p_treatments_rcbd, 4), round(p_blocks_rcbd, 4), NA, NA)\n)\n\nprint(\"RCBD ANOVA Table:\")\nprint(rcbd_anova_table)\n\n# RCBD conclusions\nrcbd_conclusions &lt;- data.table(\n  Effect = c(\"Coating Effect\", \"Substrate Effect\"),\n  F_Statistic = c(round(f_treatments_rcbd, 4), round(f_blocks_rcbd, 4)),\n  P_Value = c(round(p_treatments_rcbd, 4), round(p_blocks_rcbd, 4)),\n  Decision = c(\n    ifelse(p_treatments_rcbd &lt; alpha_anova, \"Reject H0\", \"Fail to reject H0\"),\n    ifelse(p_blocks_rcbd &lt; alpha_anova, \"Reject H0\", \"Fail to reject H0\")\n  ),\n  Conclusion = c(\n    ifelse(p_treatments_rcbd &lt; alpha_anova, \"Coating means differ significantly\", \"No significant coating effect\"),\n    ifelse(p_blocks_rcbd &lt; alpha_anova, \"Substrate means differ significantly\", \"No significant substrate effect\")\n  )\n)\n\nprint(\"RCBD Test Results:\")\nprint(rcbd_conclusions)\n\n# Using R's aov for verification\nrcbd_r &lt;- aov(adhesion ~ coating + substrate, data = coating_data)\nrcbd_summary_r &lt;- summary(rcbd_r)\nprint(\"R's RCBD ANOVA verification:\")\nprint(rcbd_summary_r)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 4: Randomized complete block design visualization\n\n\n\n\n\n\n\n\n# RCBD visualization\n# Interaction plot\ninteraction_plot_data &lt;- coating_data %&gt;%\n  fgroup_by(coating, substrate) %&gt;%\n  fsummarise(mean_adhesion = fmean(adhesion))\n\np1_rcbd &lt;- ggplot(data = interaction_plot_data, mapping = aes(x = coating, y = mean_adhesion, \n                                                             color = substrate, group = substrate)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  labs(\n    x = \"Coating Type\",\n    y = \"Mean Adhesion\",\n    color = \"Substrate\",\n    title = \"Interaction Plot: Coating × Substrate\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Box plot by coating with substrate grouping\np2_rcbd &lt;- ggplot(data = coating_data, mapping = aes(x = coating, y = adhesion, fill = substrate)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    x = \"Coating Type\",\n    y = \"Adhesion\",\n    fill = \"Substrate\",\n    title = \"Adhesion by Coating and Substrate\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Combine plots\ngrid.arrange(p1_rcbd, p2_rcbd, nrow = 2, ncol = 1)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#comprehensive-example-quality-improvement-study",
    "href": "book/Ch05.html#comprehensive-example-quality-improvement-study",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteComprehensive Example\n\n\n\nA manufacturing company conducts a comprehensive quality study comparing two suppliers, testing before/after process improvements, and evaluating multiple quality metrics:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Comprehensive Quality Study Summary by Supplier:\"\n\n\n     supplier     n Mean_Before Mean_After Mean_Improvement SD_Before SD_After\n       &lt;char&gt; &lt;int&gt;       &lt;num&gt;      &lt;num&gt;            &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1: Supplier_A    30    83.49488   92.93163         9.436750  8.242329 5.746158\n2: Supplier_B    30    82.31349   87.60750         5.294008 11.587749 8.813704\n   SD_Improvement Defect_Rate_Before Defect_Rate_After Var_Before Var_After\n            &lt;num&gt;              &lt;num&gt;             &lt;num&gt;      &lt;num&gt;     &lt;num&gt;\n1:       12.23185         0.16666667        0.00000000   67.93598  33.01834\n2:       15.98305         0.06666667        0.03333333  134.27592  77.68137\n\n\n[1] \"Overall Quality Summary:\"\n\n\n   Total_n Overall_Mean_Before Overall_Mean_After Overall_Improvement\n     &lt;int&gt;               &lt;num&gt;              &lt;num&gt;               &lt;num&gt;\n1:      60            82.90418           90.26956            7.365379\n   Overall_Defect_Before Overall_Defect_After\n                   &lt;num&gt;                &lt;num&gt;\n1:             0.1166667           0.01666667\n\n\n\n\n\n# Comprehensive quality improvement study\nset.seed(111)\ncomprehensive_quality_data &lt;- data.table(\n  part_id = 1:60,\n  supplier = rep(c(\"Supplier_A\", \"Supplier_B\"), each = 30),\n  before_improvement = c(rnorm(30, mean = 85, sd = 8), rnorm(30, mean = 82, sd = 9)),\n  after_improvement = c(rnorm(30, mean = 92, sd = 7), rnorm(30, mean = 88, sd = 8)),\n  defective_before = sample(c(0, 1), 60, replace = TRUE, prob = c(0.88, 0.12)),\n  defective_after = sample(c(0, 1), 60, replace = TRUE, prob = c(0.94, 0.06))\n)\n\n# Calculate improvements\ncomprehensive_quality_data[, improvement := after_improvement - before_improvement]\n\n# Comprehensive summary\nquality_comprehensive_summary &lt;- comprehensive_quality_data %&gt;% \n  fgroup_by(supplier) %&gt;% \n  fsummarise(\n    n = fnobs(part_id),\n    # Before/After means\n    Mean_Before = fmean(before_improvement),\n    Mean_After = fmean(after_improvement),\n    Mean_Improvement = fmean(improvement),\n    # Standard deviations\n    SD_Before = fsd(before_improvement),\n    SD_After = fsd(after_improvement),\n    SD_Improvement = fsd(improvement),\n    # Defect rates\n    Defect_Rate_Before = fmean(defective_before),\n    Defect_Rate_After = fmean(defective_after),\n    # Variance comparison\n    Var_Before = fvar(before_improvement),\n    Var_After = fvar(after_improvement)\n  )\n\nprint(\"Comprehensive Quality Study Summary by Supplier:\")\nprint(quality_comprehensive_summary)\n\n# Overall summary\noverall_quality_summary &lt;- comprehensive_quality_data %&gt;% \n  fsummarise(\n    Total_n = fnobs(part_id),\n    Overall_Mean_Before = fmean(before_improvement),\n    Overall_Mean_After = fmean(after_improvement),\n    Overall_Improvement = fmean(improvement),\n    Overall_Defect_Before = fmean(defective_before),\n    Overall_Defect_After = fmean(defective_after)\n  )\n\nprint(\"Overall Quality Summary:\")\nprint(overall_quality_summary)\n\n# Write data to CSV for download\nfwrite(comprehensive_quality_data, \"data/comprehensive_quality.csv\")\n\n\n\n\n\n\n\nComplete Analysis:\n\nCompare supplier means (independent samples)\nTest variance equality between suppliers\nBefore/after comparison (paired test)\nCompare defect proportions\nMulti-factor ANOVA\n\n\nR OutputR Code\n\n\n\n\nError in select(., part_id, supplier, before_improvement, after_improvement): could not find function \"select\"\n\n\nError: object 'before_after_long' not found\n\n\nError in select(., supplier, before_improvement, after_improvement): could not find function \"select\"\n\n\nError: object 'variance_data' not found\n\n\nError in select(., supplier, Rate_Before, Rate_After): could not find function \"select\"\n\n\nError: object 'defect_summary_comp' not found\n\n\nError: object 'p2_comp' not found\n\n\n\n\n\n# Comprehensive quality analysis dashboard\n\n# 1. Supplier comparison (before improvement)\np1_comp &lt;- ggplot(data = comprehensive_quality_data, \n                  mapping = aes(x = supplier, y = before_improvement, fill = supplier)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_point(position = position_jitter(width = 0.2), alpha = 0.4) +\n  labs(x = \"Supplier\", y = \"Quality Score\", title = \"Quality Before Improvement\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5), legend.position = \"none\")\n\n# 2. Before/After comparison (paired)\nbefore_after_long &lt;- comprehensive_quality_data %&gt;%\n  select(part_id, supplier, before_improvement, after_improvement) %&gt;%\n  pivot_longer(cols = c(before_improvement, after_improvement), \n               names_to = \"period\", values_to = \"quality\") %&gt;%\n  data.table()\n\np2_comp &lt;- ggplot(data = before_after_long, \n                  mapping = aes(x = period, y = quality, group = part_id)) +\n  geom_line(alpha = 0.3) +\n  geom_point(mapping = aes(color = period), alpha = 0.6) +\n  facet_wrap(~supplier) +\n  labs(x = \"Period\", y = \"Quality Score\", title = \"Before/After Comparison by Supplier\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Variance comparison\nvariance_data &lt;- comprehensive_quality_data %&gt;%\n  select(supplier, before_improvement, after_improvement) %&gt;%\n  pivot_longer(cols = c(before_improvement, after_improvement),\n               names_to = \"period\", values_to = \"quality\") %&gt;%\n  data.table()\n\np3_comp &lt;- ggplot(data = variance_data, mapping = aes(x = quality, fill = period)) +\n  geom_density(alpha = 0.6) +\n  facet_wrap(~supplier) +\n  labs(x = \"Quality Score\", y = \"Density\", title = \"Distribution Comparison\", fill = \"Period\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Defect rate comparison\ndefect_summary_comp &lt;- comprehensive_quality_data %&gt;%\n  fgroup_by(supplier) %&gt;%\n  fsummarise(\n    Before = fsum(defective_before),\n    After = fsum(defective_after),\n    n = fnobs(part_id)\n  ) %&gt;%\n  mutate(\n    Rate_Before = Before / n,\n    Rate_After = After / n\n  ) %&gt;%\n  select(supplier, Rate_Before, Rate_After) %&gt;%\n  pivot_longer(cols = c(Rate_Before, Rate_After), names_to = \"period\", values_to = \"rate\") %&gt;%\n  data.table()\n\np4_comp &lt;- ggplot(data = defect_summary_comp, \n                  mapping = aes(x = supplier, y = rate, fill = period)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  labs(x = \"Supplier\", y = \"Defect Rate\", title = \"Defect Rate Comparison\", fill = \"Period\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine all plots\ngrid.arrange(p1_comp, p2_comp, p3_comp, p4_comp, nrow = 2, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#key-formulas-and-decision-framework",
    "href": "book/Ch05.html#key-formulas-and-decision-framework",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "NoteEssential Formulas Summary\n\n\n\nTwo-Sample Z-Test (σ known):\nZ_0 = \\frac{(\\overline{X_1} - \\overline{X_2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\nTwo-Sample t-Test (σ unknown, equal variances):\nT_0 = \\frac{\\overline{X_1} - \\overline{X_2}}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\nPaired t-Test:\nT_0 = \\frac{\\overline{d} - \\mu_d}{S_d/\\sqrt{n}}\nF-Test for Variances:\nF_0 = \\frac{S_1^2}{S_2^2}\nTwo-Proportion Z-Test:\nZ_0 = \\frac{\\hat{p_1} - \\hat{p_2}}{\\sqrt{\\hat{p}(1-\\hat{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\nOne-Way ANOVA:\nF_0 = \\frac{MS_{Treatments}}{MS_{Error}}\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 3: Decision framework for choosing appropriate test\n\n\n\n\nDecision Framework for Choosing Appropriate Two-Sample Test\n\n\nSituation\nTest_Method\nKey_Assumptions\nWhen_to_Use\n\n\n\n\nTwo independent groups, σ known\nTwo-sample Z-test\nNormal populations, σ₁, σ₂ known\nPopulation SDs known from historical data\n\n\nTwo independent groups, σ unknown, equal variances\nTwo-sample t-test (pooled)\nNormal populations, σ₁² = σ₂²\nMost common two-sample situation\n\n\nTwo independent groups, σ unknown, unequal variances\nWelch's t-test\nNormal populations\nWhen variances clearly unequal\n\n\nPaired/matched observations\nPaired t-test\nNormal differences\nBefore/after, matched pairs\n\n\nCompare two variances\nF-test\nNormal populations\nCheck equal variance assumption\n\n\nCompare two proportions\nTwo-proportion Z-test\nLarge samples\nCompare success rates\n\n\nCompare more than two groups\nOne-way ANOVA\nNormal populations, equal variances\nMultiple groups to compare\n\n\nCompare groups with blocking\nRCBD ANOVA\nNormal populations, additive model\nControl for known nuisance factors\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4\n\n\n# Decision framework for choosing appropriate test\ndecision_framework &lt;- data.table(\n  Situation = c(\"Two independent groups, σ known\", \"Two independent groups, σ unknown, equal variances\",\n               \"Two independent groups, σ unknown, unequal variances\", \"Paired/matched observations\",\n               \"Compare two variances\", \"Compare two proportions\",\n               \"Compare more than two groups\", \"Compare groups with blocking\"),\n  Test_Method = c(\"Two-sample Z-test\", \"Two-sample t-test (pooled)\", \n                 \"Welch's t-test\", \"Paired t-test\",\n                 \"F-test\", \"Two-proportion Z-test\",\n                 \"One-way ANOVA\", \"RCBD ANOVA\"),\n  Key_Assumptions = c(\"Normal populations, σ₁, σ₂ known\", \"Normal populations, σ₁² = σ₂²\",\n                     \"Normal populations\", \"Normal differences\",\n                     \"Normal populations\", \"Large samples\",\n                     \"Normal populations, equal variances\", \"Normal populations, additive model\"),\n  When_to_Use = c(\"Population SDs known from historical data\", \"Most common two-sample situation\",\n                 \"When variances clearly unequal\", \"Before/after, matched pairs\",\n                 \"Check equal variance assumption\", \"Compare success rates\",\n                 \"Multiple groups to compare\", \"Control for known nuisance factors\")\n)\n\nkbl(decision_framework, \n    caption = \"Decision Framework for Choosing Appropriate Two-Sample Test\") %&gt;%\n  kable_styling() %&gt;%\n  column_spec(1, width = \"3cm\") %&gt;%\n  column_spec(2, width = \"2.5cm\") %&gt;%\n  column_spec(3, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"3cm\")",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#sample-size-and-power-analysis",
    "href": "book/Ch05.html#sample-size-and-power-analysis",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "R OutputR Code\n\n\n\n\n[1] \"Sample Size Requirements for Two-Sample Tests:\"\n\n\n             Test_Type Alpha Power      Effect_Size Required_n_per_group\n                &lt;char&gt; &lt;num&gt; &lt;num&gt;           &lt;char&gt;                &lt;num&gt;\n1:   Two-sample t-test  0.05   0.8        δ=5, σ=10                   63\n2:   Two-sample t-test  0.05   0.8        δ=3, σ=10                  175\n3:   Two-sample t-test  0.05   0.8        δ=1, σ=10                 1570\n4: Two-proportion test  0.05   0.8   p₁=0.2, p₂=0.3                  294\n5: Two-proportion test  0.05   0.8  p₁=0.1, p₂=0.15                  686\n6: Two-proportion test  0.05   0.8 p₁=0.05, p₂=0.10                  435\n\n\n\n\n\n# Sample size determination for various two-sample tests\n\n# Function for two-sample t-test sample size\nsample_size_two_sample_t &lt;- function(alpha, beta, delta, sigma_pooled) {\n  t_alpha2 &lt;- qt(1 - alpha/2, df = Inf)  # Use normal approximation for large samples\n  t_beta &lt;- qt(1 - beta, df = Inf)\n  n &lt;- 2 * ((t_alpha2 + t_beta) * sigma_pooled / delta)^2\n  return(ceiling(n))\n}\n\n# Function for two-proportion test sample size\nsample_size_two_prop &lt;- function(alpha, beta, p1, p2) {\n  z_alpha2 &lt;- qnorm(1 - alpha/2)\n  z_beta &lt;- qnorm(1 - beta)\n  p_bar &lt;- (p1 + p2) / 2\n  delta &lt;- abs(p1 - p2)\n  \n  n &lt;- (z_alpha2 * sqrt(2 * p_bar * (1 - p_bar)) + z_beta * sqrt(p1 * (1 - p1) + p2 * (1 - p2)))^2 / delta^2\n  return(ceiling(n))\n}\n\n# Sample size examples\nsample_size_examples &lt;- data.table(\n  Test_Type = c(\"Two-sample t-test\", \"Two-sample t-test\", \"Two-sample t-test\",\n               \"Two-proportion test\", \"Two-proportion test\", \"Two-proportion test\"),\n  Alpha = rep(0.05, 6),\n  Power = rep(0.80, 6),\n  Effect_Size = c(\"δ=5, σ=10\", \"δ=3, σ=10\", \"δ=1, σ=10\",\n                 \"p₁=0.2, p₂=0.3\", \"p₁=0.1, p₂=0.15\", \"p₁=0.05, p₂=0.10\"),\n  Required_n_per_group = c(\n    sample_size_two_sample_t(0.05, 0.20, 5, 10),\n    sample_size_two_sample_t(0.05, 0.20, 3, 10),\n    sample_size_two_sample_t(0.05, 0.20, 1, 10),\n    sample_size_two_prop(0.05, 0.20, 0.2, 0.3),\n    sample_size_two_prop(0.05, 0.20, 0.1, 0.15),\n    sample_size_two_prop(0.05, 0.20, 0.05, 0.10)\n  )\n)\n\nprint(\"Sample Size Requirements for Two-Sample Tests:\")\nprint(sample_size_examples)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#effect-size-analysis",
    "href": "book/Ch05.html#effect-size-analysis",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "R OutputR Code\n\n\n\n\nError in parse(text = input): &lt;text&gt;:17:3: unexpected symbol\n16:   ),\n17: F value\n      ^\n\n\n\n\n\n# Calculate effect sizes for all examples in the chapter\n\neffect_size_summary &lt;- data.table(\n  Example = c(\"Cable Strength (Z-test)\", \"Plastic Strength (t-test)\", \n             \"Process Improvement (Paired)\", \"Defect Rate Comparison\", \n             \"Instrument Variance\", \"Process Yield (ANOVA)\"),\n  Effect_Type = c(\"Cohen's d\", \"Cohen's d\", \"Cohen's d\", \n                 \"Difference in proportions\", \"Variance ratio\", \"Eta-squared\"),\n  Calculated_Effect = c(\n    abs(xbar1 - xbar2) / sqrt((sigma1^2 + sigma2^2)/2),  # Cable strength\n    abs(xbar1_t - xbar2_t) / sp,                         # Plastic strength\n    abs(d_bar) / s_d,                                    # Process improvement\n    abs(p1_hat - p2_hat),                                # Defect rates\n    max(s1_f_squared, s2_f_squared) / min(s1_f_squared, s2_f_squared), # Variance ratio\n    SST / SS_Total                                       # ANOVA eta-squared\n  ),\nF value`[1]) &lt; 0.001\n  )\n)\n\nprint(\"Validation of Manual vs R Calculations:\")\nprint(validation_checks)\n\n# Check if all validations passed\nall_passed &lt;- all(validation_checks$Match)\nprint(paste(\"All validation checks passed:\", all_passed))\n\nif (!all_passed) {\n  print(\"WARNING: Some manual calculations do not match R functions\")\n  print(\"Please review the calculations above\")\n} else {\n  print(\"SUCCESS: All manual calculations match R function results\")\n}\n\nprint(paste0(\"\\n\", strrep(\"=\", 80)))\nprint(\"CHAPTER 5 COMPLETE - ALL EXAMPLES VERIFIED\")\nprint(strrep(\"=\", 80))",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/Ch05.html#chapter-summary",
    "href": "book/Ch05.html#chapter-summary",
    "title": "1 Decision Making for Two Samples",
    "section": "",
    "text": "This chapter covered the fundamental concepts of two-sample comparisons in engineering statistics:\n\nTwo-sample Z-tests when population variances are known\nTwo-sample t-tests for unknown variances (equal and unequal)\nPaired t-tests for dependent observations\nF-tests for comparing variances\nTwo-proportion tests for comparing success rates\nANOVA for comparing multiple groups\nExperimental design concepts (CRD and RCBD)\n\nThe key is choosing the appropriate method based on:\n\nData structure (independent vs. paired)\nKnowledge of population parameters\nAssumptions about variance equality\nType of data (continuous vs. categorical)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for Two Samples"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nMontgomery, D. C., Runger, G. C., & Hubele, N. F. (2014). Engineering statistics. Wiley.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "References"
    ]
  },
  {
    "objectID": "book/index.html",
    "href": "book/index.html",
    "title": "Preface",
    "section": "",
    "text": "Preface\nThis booklet provides the class notes for Math-3020: Statistics for Science & Engineering, based on Engineering Statistics by Montgomery, Runger, and Hubele (please refer to the textbook for complete and comprehensive details). The following topics are covered:\n\nThe Role of Statistics in Engineering\nData Summary and Presentation\nRandom Variables and Probability Distributions\nDecision Making for a Single Sample\nDecision Making for Two Samples\nBuilding Empirical Models\n\nMontgomery, D. C., Runger, G. C., & Hubele, N. F. (2014). Engineering statistics. Wiley.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Preface"
    ]
  },
  {
    "objectID": "book/Ch02.html",
    "href": "book/Ch02.html",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteMajor Themes of Chapter 2\n\n\n\n\nDescriptive Statistics: Learn to compute and interpret measures of central tendency (mean, median) and variability (variance, standard deviation, range)\nData Visualization: Master key graphical displays including histograms, box plots, stem-and-leaf diagrams, and scatter plots\nDistribution Analysis: Understand how to assess the shape, center, spread, and outliers in datasets\nTime Series Patterns: Recognize trends, cycles, and other temporal patterns in engineering data\nMultivariate Relationships: Explore relationships between multiple variables using correlation analysis and scatter plot matrices\nQuality Tools: Apply Pareto charts and other specialized plots for engineering problem-solving\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nCompute and interpret the sample mean, sample variance, sample standard deviation, sample median, and sample range.\nExplain the concepts of sample mean, sample variance, population mean, and population variance.\nConstruct and interpret visual data displays, including the stem-and-leaf display, the histogram, and the box plot and understand how these graphical techniques are useful in uncovering and summarizing patterns in data.\nExplain how to use box plots and other data displays to visually compare two or more samples of data.\nKnow how to use simple time series plots to visually display the important features of time-oriented data.\nConstruct scatter plots and compute and interpret a sample correlation coefficient.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Mean\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, the sample mean is\n\\begin{aligned}\n\\overline{X} & = \\frac{X_{1} + X_{2} + \\cdots + X_{n}}{n} \\\\\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\nNotePopulation Mean\n\n\n\nWhen there is a finite number of observations (say, N) in the population, the population mean is\n\\begin{aligned}\n\\mu & = \\frac{\\sum_{i=1}^{N} X_{i}}{N}\n\\end{aligned}\n\n\nThe sample mean, \\overline{X}, is a reasonable estimate of the population mean, \\mu.\n\n\n\n\n\n\n\n\n\nNoteExample: O-Ring Tensile Strength\n\n\n\nEngineering Context: This example demonstrates how the sample mean serves as a measure of central tendency in materials testing. Understanding tensile strength is crucial for ensuring component reliability in aerospace applications.\nConsider the O-ring tensile strength experiment described in Chapter 1. The data from the modified rubber compound (1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073) are shown in the dot diagram (Figure 1 and Figure 2).\nManual Calculation: The sample mean strength (psi) for the eight observations is:\n\\begin{aligned}\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n} \\\\\n\\overline{X} & = \\frac{1037 + 1047 + \\cdots + 1073}{8} \\\\\n\\overline{X} & = \\frac{8440}{8} = 1055  \\text{ psi}\n\\end{aligned}\nPhysical Interpretation: The sample mean \\overline{X} = 1055 can be thought of as a “balance point.” If each observation represents 1 pound of mass placed at the point on the x-axis, a fulcrum located at \\overline{X} would exactly balance this system of weights.\nEngineering Significance: This mean value provides a single representative measure that engineers can use to compare different rubber compounds or assess whether the material meets specifications.\n\n\n\n\n\n\nFigure 1: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\nR OutputR Code\n\n\n\n\n\nO-Ring Tensile Strength Statistics\n\n\nn\nsum\nmean\n\n\n\n\n8\n8440\n1055\n\n\n\n\n\n\n\n\n# Load required libraries\nlibrary(fastverse)\nlibrary(kit)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(car)\nlibrary(GGally)\n\n# O-ring tensile strength example\ntensile_data &lt;-\n  data.table(TS = c(1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073))\n\ntensile_data_summary1 &lt;-\n  tensile_data %&gt;%\n  fsummarise(\n    n = fnobs(TS),\n    sum = fsum(TS),\n    mean = fmean(TS)\n  )\n\ntensile_data_summary1 %&gt;%\n  kbl(caption = \"O-Ring Tensile Strength Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\n\n\n\n\n# Dot diagram showing sample mean as balance point\nggplot(data = tensile_data, mapping = aes(x = TS)) +\n  geom_dotplot(binwidth = 1, stackdir = \"up\", dotsize = 1) +\n  geom_vline(aes(xintercept = fmean(TS)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(x = \"Tensile Strength (psi)\", y = NULL, title = \"Sample Mean as Balance Point\") +\n  theme_classic() +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Variance and Sample Standard Deviation\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, then the sample variance is\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n\\end{aligned}\nThe sample standard deviation, S, is the positive square root of the sample variance.\n\n\nEngineering Perspective: The units of measurement for the sample variance are the square of the original units. Thus, if X is measured in psi, the variance units are (\\text{psi})^2. The standard deviation has the desirable property of measuring variability in the original units (psi), making it more interpretable for engineers.\n\n\n\n\n\n\n\n\n\nNoteExample: Variability in O-Ring Strength\n\n\n\nWhy Variability Matters: In engineering applications, understanding variability is often more critical than knowing the average. High variability can indicate process instability, quality issues, or the need for tighter manufacturing controls.\nThe numerator of S^2 is (See Table 1): \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2} = 1348\nSample Variance Calculation:\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n= \\frac{1348}{8-1} = 192.57 \\text{ psi}^2 \\end{aligned}\nSample Standard Deviation:\n\\begin{aligned}\nS & = \\sqrt{192.57} = 13.9 \\text{ psi} \\end{aligned}\nEngineering Interpretation: A standard deviation of 13.9 psi means that most O-ring tensile strengths fall within about 14 psi of the mean (1055 psi). This gives engineers insight into the consistency of the manufacturing process.\nComputational Note: \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}=\\sum_{i=1}^{n}X_{i}^{2}-\\frac{\\left(\\sum_{i=1}^{n}X_{i}\\right)^{2}}{n} (computational formula).\n\nR OutputR Code\n\n\n\n\n\nO-Ring Tensile Strength Statistics\n\n\nvar\nsd\n\n\n\n\n192.57\n13.88\n\n\n\n\n\n\n\n\ntensile_data_summary2 &lt;-\n  tensile_data %&gt;%\n  fsummarise(\n    var = fvar(TS),\n    sd = fsd(TS)\n  )\n\ntensile_data_summary2 %&gt;%\n  kbl(caption = \"O-Ring Tensile Strength Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\nTable 1: Variance Calculations\n\n\n\n\nVariance Calculation Terms\n\n\ni\nXi\nXi - X̄\n(Xi - X̄)²\n\n\n\n\n1\n1048\n-7\n49\n\n\n2\n1059\n4\n16\n\n\n3\n1047\n-8\n64\n\n\n4\n1066\n11\n121\n\n\n5\n1040\n-15\n225\n\n\n6\n1070\n15\n225\n\n\n7\n1037\n-18\n324\n\n\n8\n1073\n18\n324\n\n\nTotal\n8440\n0\n1348\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUnderstanding Variability Measurement\n\n\n\nEngineering Insight: To see how the sample variance measures dispersion or variability, refer to Figure 3, which shows the deviations X_{i}-\\overline{X} for the O-ring tensile strength data.\nKey Concepts:\n\nGreater variability → larger absolute deviations from the mean\nDeviations X_{i}-\\overline{X} always sum to zero\nSquaring eliminates negative values and emphasizes larger deviations\nSmall S^2 indicates consistent process; large S^2 suggests high variability\n\nProcess Control Implications: Engineers use variance to monitor process stability. Increasing variance over time may signal equipment wear, material inconsistencies, or environmental factors affecting production.\n\n\n\n\n\n\nFigure 3: How the sample variance measures variability through the deviations X_{i}-\\overline{X}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePopulation Variance\n\n\n\nWhen the population is finite and consists of N values, we may define the population variance as\n\\begin{aligned}\n\\sigma^2 & = \\frac{\\sum_{i=1}^{N}\\left(X_{i}-\\mu\\right)^{2}}{N}\n\\end{aligned}\n\n\nThe sample variance, S^2, is a reasonable estimate of the population variance, \\sigma^2.\n\n\n\n\n\n\n\n\n\n\n\nNoteStem-and-Leaf Diagram\n\n\n\nEngineering Application: Stem-and-leaf diagrams are particularly useful for initial data exploration in engineering. They preserve actual data values while showing distribution shape, making them ideal for quality control and process analysis.\nA stem-and-leaf diagram provides an informative visual display of a data set X_{1},X_{2},\\ldots,X_{n}, where each number X_{i} consists of at least two digits.\n\n\n\n\n\n\nTipSteps for Constructing a Stem-and-Leaf Diagram\n\n\n\n\nDivide each number X_{i} into two parts: a stem (leading digits) and a leaf (remaining digit)\nList the stem values in a vertical column\nRecord the leaf for each observation beside its stem\nWrite the units for stems and leaves on the display\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Aluminum-Lithium Alloy Compressive Strength\n\n\n\nEngineering Context: This example analyzes compressive strength data from 80 aluminum-lithium alloy specimens. Such analysis is crucial for aerospace applications where weight reduction and strength are both critical.\nData Overview: The compressive strength measurements (in psi) from Table 2 represent quality control testing of a lightweight alloy used in aircraft components.\n\n\n\nTable 2: Compressive Strength of 80 Aluminum-Lithium Alloy Specimens\n\n\n\n\n\n105\n221\n183\n186\n121\n181\n180\n143\n\n\n97\n154\n153\n174\n120\n168\n167\n141\n\n\n245\n228\n174\n199\n181\n158\n176\n110\n\n\n163\n131\n154\n115\n160\n208\n158\n133\n\n\n207\n180\n190\n193\n194\n133\n156\n123\n\n\n134\n178\n76\n167\n184\n135\n229\n146\n\n\n218\n157\n101\n171\n165\n172\n158\n169\n\n\n199\n151\n142\n163\n145\n171\n148\n158\n\n\n160\n175\n149\n87\n160\n237\n150\n135\n\n\n196\n201\n200\n176\n150\n170\n118\n149\n\n\n\n\n\n\nWe select stem values as the numbers 7, 8, 9, \\ldots, 24 (representing tens place).\n\nR OutputR CodeDownload Data\n\n\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   7 | 6\n   8 | 7\n   9 | 7\n  10 | 15\n  11 | 058\n  12 | 013\n  13 | 133455\n  14 | 12356899\n  15 | 001344678888\n  16 | 0003357789\n  17 | 0112445668\n  18 | 0011346\n  19 | 034699\n  20 | 0178\n  21 | 8\n  22 | 189\n  23 | 7\n  24 | 5\n\n\n\n\n\n# Stem-and-leaf diagram for compressive strength data\ncompressive_data &lt;- fread(\"./data/Exmp2.4.csv\")\nstem(compressive_data[, CS], scale = 2, width = 80)\n\n\n\n\n\n\n\nEngineering Interpretation: The stem-and-leaf display reveals:\n\nMost compressive strengths lie between 110 and 200 psi\nCentral value is approximately 150-160 psi\nDistribution is approximately symmetric\nNo obvious outliers or unusual patterns\nThe alloy appears to have consistent strength properties\n\nQuality Control Insights: This symmetric distribution suggests the manufacturing process is well-controlled. Engineers can use this information to set specification limits and monitor future production.\n\n\n\n\n\n\n\n\n\n\nNoteHistogram\n\n\n\nEngineering Definition: A histogram is a graphical representation of data distribution, showing frequency or relative frequency within specified intervals (bins). Histograms are essential tools for:\n\nProcess capability analysis\nQuality control monitoring\nDesign specification validation\nStatistical process control\n\nEach bin represents a range of values, and bar height reflects the number (or proportion) of data points in that range.\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Golf Ball Distance Testing\n\n\n\nEngineering Application: The USGA tests golf balls to ensure conformance to regulations. This standardized testing demonstrates how consistent measurement protocols are essential in engineering quality control.\nTesting Protocol: Balls are tested using “Iron Byron,” a mechanical device that eliminates human variability, ensuring reproducible test conditions. This exemplifies the engineering principle of controlling sources of variation.\n\n\n\nTable 3: Golf Ball Distance Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n291.5\n274.4\n290.2\n276.4\n272.0\n268.7\n281.6\n281.6\n276.3\n285.9\n\n\n269.6\n266.6\n283.6\n269.6\n277.8\n287.8\n267.6\n292.6\n273.4\n284.4\n\n\n270.7\n274.0\n285.2\n275.5\n272.1\n261.3\n274.0\n279.3\n281.0\n293.1\n\n\n277.5\n278.0\n272.5\n271.7\n280.8\n265.6\n260.1\n272.5\n281.3\n263.0\n\n\n279.0\n267.3\n283.5\n271.2\n268.5\n277.1\n266.2\n266.4\n271.5\n280.3\n\n\n267.8\n272.1\n269.7\n278.5\n277.3\n280.5\n270.8\n267.7\n255.1\n276.4\n\n\n283.7\n281.7\n282.2\n274.1\n264.5\n281.0\n273.2\n274.4\n281.6\n273.7\n\n\n271.0\n271.5\n289.7\n271.1\n256.9\n274.5\n286.2\n273.9\n268.5\n262.6\n\n\n261.9\n258.9\n293.2\n267.1\n255.0\n269.7\n281.9\n269.6\n279.8\n269.9\n\n\n282.6\n270.0\n265.2\n277.7\n275.5\n272.2\n270.0\n271.0\n284.3\n268.4\n\n\n\n\n\n\nHistogram Design Principles: With 100 observations, \\sqrt{100} = 10 suggests about 10 bins for optimal visualization. The analysis reveals important distribution characteristics for engineering decision-making.\nEngineering Insights from Histograms:\n\nDistribution Shape: The reasonably symmetric, bell-shaped distribution suggests the manufacturing process is well-controlled\nProcess Capability: The spread indicates natural process variation\nSpecification Conformance: Engineers can assess what percentage of balls meet distance requirements\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram with 9 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram with 16 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A relative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A cumulative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\nFigure 4: Histogram for the golf ball distance data\n\n\n\n\n\n\n# Golf ball distance histograms\ngolf_data &lt;- fread(\"./data/Exmp2.6.csv\")\n\n# Basic histogram with 9 bins\np1 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 9, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 9 bins\") +\n  theme_classic()\n\n# Histogram with 16 bins\np2 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 16, fill = \"lightgreen\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 16 bins\") +\n  theme_classic()\n\n# Relative frequency histogram\np3 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(count / sum(count))),\n    bins = 9,\n    fill = \"orange\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), labels = percent_format()) +\n  labs(x = \"Distance (yards)\", y = \"Relative Frequency\", title = \"Relative Frequency\") +\n  theme_classic()\n\n# Cumulative frequency histogram\np4 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(cumsum(count))),\n    bins = 10,\n    fill = \"pink\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Cumulative Frequency\", title = \"Cumulative Frequency\") +\n  theme_classic()\n\nprint(p1)\nprint(p2)\nprint(p3)\nprint(p4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePareto Chart\n\n\n\nEngineering Application: Pareto charts are fundamental tools in quality engineering and process improvement, embodying the “80-20 rule” or “vital few vs. trivial many” principle. They help engineers prioritize improvement efforts by identifying the most significant causes of problems.\nKey Features:\n\nCategories ordered by frequency (highest to lowest)\nFocuses attention on major contributors\nGuides resource allocation for maximum impact\nEssential for root cause analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Aircraft Safety Analysis\n\n\n\nEngineering Context: This analysis of aircraft accident data demonstrates how Pareto charts help engineers identify critical safety issues and prioritize design improvements.\nData Source: Aircraft accident rates from The Wall Street Journal analysis, showing hull losses per million departures between 1959-1999 for 22 aircraft types.\n\n\n\nTable 4: Aircraft Accident Data\n\n\n\n\n\n\n\n\n\n\nAircraft Type\nActual Number of Hull Losses\nHull Losses/Million Departures\n\n\n\n\nMD-11\n5\n6.54\n\n\n707/720\n115\n6.46\n\n\nDC-8\n71\n5.84\n\n\nF-28\n32\n3.94\n\n\nBAC 1-11\n22\n2.64\n\n\nDC-10\n20\n2.57\n\n\n747-Early\n21\n1.9\n\n\n\n\n\n\nEngineering Analysis: The top three aircraft types account for the majority of incidents on a per-million-departures basis. Notably:\n\n707/720 and DC-8: Mid-1950s designs, largely out of service\nMD-11: 1990s design with concerning safety record (5 losses out of 198 aircraft)\n\nProcess Improvement Insights: This Pareto analysis helps aviation engineers:\n\nFocus safety improvements on high-risk aircraft designs\nUnderstand the relationship between design era and safety performance\nAllocate research resources to address the “vital few” risk factors\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 5: Pareto chart for the aircraft accident data\n\n\n\n\n\n\n\n\n# Pareto chart for aircraft accident data\naircraft_data &lt;- fread(\"./data/Exmp2.8.csv\")\n\naircraft_data %&gt;%\n  fmutate(Type = factor(Type, levels = Type[order(-HLMD)])) %&gt;%\n  ggplot(aes(x = Type, y = HLMD)) +\n  geom_col(fill = \"steelblue\", color = \"black\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  labs(\n    x = \"Aircraft Type\",\n    y = \"Hull Losses per Million Departures\",\n    title = \"Pareto Chart: Aircraft Accident Rates\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBox Plot\n\n\n\nEngineering Applications: Box plots are powerful tools for engineering data analysis, providing comprehensive information about data distribution in a compact visual format.\nWhat Box Plots Reveal:\n\nMeasure of Central Tendency (median)\nMeasure of Variability (IQR, range)\nMeasure of Symmetry (distribution shape)\nOutliers (unusual observations)\nGroup Comparisons (process/design alternatives)\n\n\n Box Whisker Plot\n\nStatistical Definitions:\n\n\\begin{align*}\n\\text{Lower Inner Fence} & =Q_{1}-1.5\\times\\text{IQR}\\\\\n\\text{Upper Inner Fence} & =Q_{3}+1.5\\times\\text{IQR}\\\\\n\\text{Lower Outer Fence} & =Q_{1}-3\\times\\text{IQR}\\\\\n\\text{Upper Outer Fence} & =Q_{3}+3\\times\\text{IQR}\n\\end{align*}\n\nOutlier Classification:\n\nMild Outlier: Beyond inner fence (but inside outer fence)\nExtreme Outlier: Beyond outer fence\n\nEngineering Interpretation: Whiskers extend to the most extreme values that are not outliers, helping engineers identify unusual measurements that may indicate process problems or measurement errors.\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\nBox Plot Statistics\n\n\n\n\nq1\n\n\nmedian\n\n\nq3\n\n\niqr\n\n\nlower_fence\n\n\nupper_fence\n\n\n\n\n\n\n144.5\n\n\n161.5\n\n\n181\n\n\n36.5\n\n\n89.75\n\n\n235.75\n\n\n\n\n\n\nFigure 6: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n\n\nFigure 7: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n# Box plot for compressive strength data\nggplot(compressive_data, aes(y = CS)) +\n  geom_boxplot(width = 0.3, fill = \"lightblue\") +\n  scale_y_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(\n    y = \"Compressive Strength (psi)\", x = NULL,\n    title = \"Box Plot: Compressive Strength Distribution\"\n  ) +\n  theme_classic() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())\n\nbox_stats &lt;- compressive_data %&gt;%\n  fsummarise(\n    q1 = fquantile(CS, 0.25),\n    median = fmedian(CS),\n    q3 = fquantile(CS, 0.75),\n    iqr = fquantile(CS, 0.75) - fquantile(CS, 0.25),\n    lower_fence = fquantile(CS, 0.25) - 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25)),\n    upper_fence = fquantile(CS, 0.75) + 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25))\n  )\n\nbox_stats %&gt;%\n  kbl(caption = \"Box Plot Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Comparative box plots of a quality index at three plants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTime Series Plots\n\n\n\nEngineering Importance: Time series analysis is critical in engineering for monitoring processes, identifying trends, and predicting future behavior. Understanding temporal patterns helps engineers:\n\nDetect process drift or shifts\nIdentify cyclical behaviors\nPlan maintenance schedules\nOptimize production timing\n\nKey Concepts:\n\nTime Series: Data recorded in sequential order over time\nTrend: Long-term increase or decrease in values\nCycles: Regular patterns that repeat over time\nSeasonality: Predictable patterns related to calendar periods\n\nEngineering Applications:\n\nEquipment performance monitoring\nQuality control charts\nProduction scheduling\nPredictive maintenance\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Company sales by year\n\n\n\n\n\n\n\n\n\n\n\n(b) Company sales by quarter\n\n\n\n\n\n\n\nFigure 9: Company sales\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Annual sales trend\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Quarterly sales pattern\n\n\n\n\n\n\n\nFigure 10: Time series plots showing trends and patterns\n\n\n\n\n\n\n# Time series example (simulated company sales data)\nset.seed(123)\ntime_data &lt;-\n  data.table(\n    year = 2015:2024,\n    quarter = rep(1:4, length.out = 40),\n    sales = 100 + 0.02 * (1:40)^2 + 10 * sin(2 * pi * (1:40) / 4) + rnorm(40, 0, 5)\n  ) %&gt;%\n  fmutate(time_period = year + (quarter - 1) / 4)\n\n# Annual sales trend\np5 &lt;-\n  time_data %&gt;%\n  fgroup_by(year) %&gt;%\n  fsummarise(annual_sales = fmean(sales)) %&gt;%\n  ggplot(aes(x = year, y = annual_sales)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(size = 3) +\n  labs(x = \"Year\", y = \"Average Sales\", title = \"Annual Sales Trend\") +\n  theme_classic()\n\n# Quarterly sales pattern\np6 &lt;-\n  ggplot(time_data, aes(x = time_period, y = sales)) +\n  geom_line(color = \"red\", linewidth = 1) +\n  geom_point(size = 2) +\n  labs(x = \"Time Period\", y = \"Sales\", title = \"Quarterly Sales Pattern\") +\n  theme_classic()\n\nprint(p5)\nprint(p6)\n\nfwrite(time_data, \"data/time_series_example.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: A digidot plot of the compressive strength data.\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 12: Digital dot plot of compressive strength data\n\n\n\n\n\n\n\n\n# Digital dot plot\ncompressive_data %&gt;%\n  fmutate(digit = round(CS / 10) * 10, count = 1) %&gt;%\n  fgroup_by(digit) %&gt;%\n  fsummarise(frequency = fsum(count)) %&gt;%\n  ggplot(aes(x = digit, y = frequency)) +\n  geom_segment(aes(xend = digit, yend = 0), linewidth = 1) +\n  geom_point(size = 3, color = \"red\") +\n  labs(\n    x = \"Compressive Strength (grouped by 10s)\", y = \"Frequency\",\n    title = \"Digital Dot Plot\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMultivariate Data\n\n\n\nEngineering Reality: Most engineering problems involve multiple variables simultaneously. Understanding relationships between variables is essential for:\n\nProcess optimization\nDesign improvement\nRoot cause analysis\nPredictive modeling\n\nKey Concepts:\n\nUnivariate Analysis: Single variable (mean, variance, distribution)\nMultivariate Analysis: Multiple variables and their relationships\nCorrelation: Strength of linear relationship between variables\nCausation vs. Correlation: Important distinction for engineering decisions\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Wire Bond Strength Analysis\n\n\n\nEngineering Context: Wire bonding is a critical process in semiconductor manufacturing. Understanding how process parameters affect bond strength is essential for reliable electronic devices.\nProcess Variables:\n\nPull Strength (Y): Response variable (quality measure)\nWire Length (X1): Process parameter\nDie Height (X2): Design parameter\n\n\n\n\nTable 5: Wire Bond Data\n\n\n\n\n\nObs No\nPull Strength (Y)\nWire Length (X1)\nDie Height (X2)\n\n\n\n\n1\n9.95\n2\n50\n\n\n2\n24.45\n8\n110\n\n\n3\n31.75\n11\n120\n\n\n4\n35.00\n10\n550\n\n\n5\n25.02\n8\n295\n\n\n6\n16.86\n4\n200\n\n\n7\n14.38\n2\n375\n\n\n8\n9.60\n2\n52\n\n\n9\n24.35\n9\n100\n\n\n10\n27.50\n8\n300\n\n\n11\n17.08\n4\n412\n\n\n12\n37.00\n11\n400\n\n\n13\n41.95\n12\n500\n\n\n14\n11.66\n2\n360\n\n\n15\n21.65\n4\n205\n\n\n16\n17.89\n4\n400\n\n\n17\n69.00\n20\n600\n\n\n18\n10.30\n1\n585\n\n\n19\n34.93\n10\n540\n\n\n20\n46.59\n15\n250\n\n\n21\n44.88\n15\n290\n\n\n22\n54.12\n16\n510\n\n\n23\n56.63\n17\n590\n\n\n24\n22.13\n6\n100\n\n\n25\n21.15\n5\n400\n\n\n\n\n\n\nEngineering Questions:\n\nWhich parameter most strongly affects bond strength?\nAre the parameters independent of each other?\nCan we predict strength from parameter settings?\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scatter plot for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot for Pull strength versus die height\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scatter and box plots for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(d) Scatter and box plots for Pull strength versus die height\n\n\n\n\n\n\n\nFigure 13: Scatter diagrams and box plots for the wire bond pull strength data\n\n\n\n\n\n\n# Wire bond data analysis\nwire_data &lt;- fread(\"./data/Table2.9.csv\")\n\n# Scatter plot: Pull strength vs Wire length\np7 &lt;-\n  ggplot(wire_data, aes(x = X1, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"Wire Length\", y = \"Pull Strength\", title = \"Pull Strength vs Wire Length\") +\n  theme_classic()\n\n# Scatter plot: Pull strength vs Die height\np8 &lt;-\n  ggplot(wire_data, aes(x = X2, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Die Height\", y = \"Pull Strength\", title = \"Pull Strength vs Die Height\") +\n  theme_classic()\n\nprint(p7)\nprint(p8)\n\nlibrary(car)\nscatterplot(\n  formula = Y ~ X1,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Length\",\n  ylab = \"Strength\"\n)\n\nscatterplot(\n  formula = Y ~ X2,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Die height\",\n  ylab = \"Strength\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Correlation Coefficient\n\n\n\nEngineering Definition: The correlation coefficient quantifies the strength and direction of linear relationships between variables, crucial for understanding process interactions.\nGiven n pairs of data \\left(X_{1},Y_{1}\\right),\\left(X_{2},Y_{2}\\right),\\ldots,\\left(X_{n},Y_{n}\\right):\n\nr=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(Y_{i}-\\overline{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\overline{Y}\\right)^{2}}}\n\nwhere -1\\leq r\\leq+1.\nEngineering Interpretation:\n\nr ≈ +1: Strong positive relationship (as X increases, Y increases)\nr ≈ -1: Strong negative relationship (as X increases, Y decreases)\nr ≈ 0: Weak linear relationship (may still have nonlinear relationship)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) r is near +1\n\n\n\n\n\n\n\n\n\n\n\n(b) r is near -1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) r is near 0, Y and X are unrelated\n\n\n\n\n\n\n\n\n\n\n\n(d) r is near 0, Y and X are nonlinearly related\n\n\n\n\n\n\n\nFigure 14: Scatter diagrams for different values of the sample correlation coefficient r.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Shampoo Quality Analysis\n\n\n\nEngineering Context: This multivariate analysis demonstrates how engineers evaluate product quality across multiple sensory and performance characteristics. Understanding these relationships helps optimize formulations.\nQuality Variables:\n\nFoam: Lathering performance\nScent: Fragrance intensity\nColor: Visual appeal\nResidue: Cleansing effectiveness\nRegion: Market segment\nQuality: Overall consumer rating\n\n\n\n\nTable 6: Data on Shampoo\n\n\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\n6.3\n5.3\n4.8\n3.1\n1\n91\n\n\n4.4\n4.9\n3.5\n3.9\n1\n87\n\n\n3.9\n5.3\n4.8\n4.7\n1\n82\n\n\n5.1\n4.2\n3.1\n3.6\n1\n83\n\n\n5.6\n5.1\n5.5\n5.1\n1\n83\n\n\n4.6\n4.7\n5.1\n4.1\n1\n84\n\n\n4.8\n4.8\n4.8\n3.3\n1\n90\n\n\n6.5\n4.5\n4.3\n5.2\n1\n84\n\n\n8.7\n4.3\n3.9\n2.9\n1\n97\n\n\n8.3\n3.9\n4.7\n3.9\n1\n93\n\n\n5.1\n4.3\n4.5\n3.6\n1\n82\n\n\n3.3\n5.4\n4.3\n3.6\n1\n84\n\n\n5.9\n5.7\n7.2\n4.1\n2\n87\n\n\n7.7\n6.6\n6.7\n5.6\n2\n80\n\n\n7.1\n4.4\n5.8\n4.1\n2\n84\n\n\n5.5\n5.6\n5.6\n4.4\n2\n84\n\n\n6.3\n5.4\n4.8\n4.6\n2\n82\n\n\n4.3\n5.5\n5.5\n4.1\n2\n79\n\n\n4.6\n4.1\n4.3\n3.1\n2\n81\n\n\n3.4\n5.0\n3.4\n3.4\n2\n83\n\n\n6.4\n5.4\n6.6\n4.8\n2\n81\n\n\n5.5\n5.3\n5.3\n3.8\n2\n84\n\n\n4.7\n4.1\n5.0\n3.7\n2\n83\n\n\n4.1\n4.0\n4.1\n4.0\n2\n80\n\n\n\n\n\n\nEngineering Analysis Goals:\n\nIdentify which characteristics most influence overall quality\nUnderstand relationships between formulation parameters\nOptimize product design for target markets\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\nTable 7: Correlation Matrix\n\n\n\n\nShampoo Data Correlation Matrix\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\nFoam\n1.000\n0.002\n0.328\n0.193\n-0.032\n0.512\n\n\nScent\n0.002\n1.000\n0.599\n0.500\n0.278\n-0.252\n\n\nColor\n0.328\n0.599\n1.000\n0.524\n0.458\n-0.194\n\n\nResidue\n0.193\n0.500\n0.524\n1.000\n0.165\n-0.489\n\n\nRegion\n-0.032\n0.278\n0.458\n0.165\n1.000\n-0.507\n\n\nQuality\n0.512\n-0.252\n-0.194\n-0.489\n-0.507\n1.000\n\n\n\n\n\n\nShampoo Data Summary Statistics\n\n\nFoam_mean\nFoam_sd\nFoam_min\nFoam_max\nScent_mean\nScent_sd\nScent_min\nScent_max\nColor_mean\nColor_sd\nColor_min\nColor_max\nResidue_mean\nResidue_sd\nResidue_min\nResidue_max\nRegion_mean\nRegion_sd\nRegion_min\nRegion_max\nQuality_mean\nQuality_sd\nQuality_min\nQuality_max\n\n\n\n\n5.5\n1.45\n3.3\n8.7\n4.91\n0.67\n3.9\n6.6\n4.9\n1.02\n3.1\n7.2\n4.03\n0.7\n2.9\n5.6\n1.5\n0.51\n1\n2\n84.5\n4.36\n79\n97\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8\n\n\n# Shampoo data correlation matrix\nshampoo_data &lt;- fread(\"./data/Table2.11.csv\")\n\ncor_matrix &lt;- cor(shampoo_data)\ncor_matrix %&gt;%\n  round(3) %&gt;%\n  kbl(caption = \"Shampoo Data Correlation Matrix\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\nshampoo_summary &lt;-\n  shampoo_data %&gt;%\n  fsummarise(\n    across(\n      .cols = is.numeric,\n      .fns = list(\n        mean = fmean,\n        sd = fsd,\n        min = fmin,\n        max = fmax\n      )\n    )\n  )\n\nshampoo_summary %&gt;%\n  kbl(caption = \"Shampoo Data Summary Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 15: Matrix of scatter plots for the shampoo data\n\n\n\n\n\n\n\n\n# Matrix of scatter plots for shampoo data\nggpairs(shampoo_data,\n  title = \"Shampoo Quality Data: Pairwise Relationships\",\n  upper = list(continuous = wrap(\"cor\", size = 3))\n) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis chapter provided essential tools for engineering data analysis:\n\n\n\nMeasures of Central Tendency: Sample mean as balance point for process centering\nMeasures of Variability: Standard deviation for process control and capability analysis\nDistribution Analysis: Understanding natural process variation patterns\n\n\n\n\n\nHistograms: Assess process capability and distribution normality\nBox Plots: Identify outliers and compare process alternatives\nStem-and-Leaf: Quick distribution assessment preserving actual values\nPareto Charts: Prioritize improvement efforts using 80-20 principle\n\n\n\n\n\nTime Series: Monitor trends, detect shifts, identify cycles\nCorrelation Analysis: Understand variable relationships for optimization\nMultivariate Methods: Analyze complex engineering systems\n\n\n\n\n\nQuality control and process improvement\nDesign optimization and parameter selection\nRoot cause analysis and problem solving\nSpecification setting and capability assessment\n\nThese statistical tools form the foundation for evidence-based engineering decisions and continuous improvement initiatives.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#data-summary-and-display",
    "href": "book/Ch02.html#data-summary-and-display",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteSample Mean\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, the sample mean is\n\\begin{aligned}\n\\overline{X} & = \\frac{X_{1} + X_{2} + \\cdots + X_{n}}{n} \\\\\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\nNotePopulation Mean\n\n\n\nWhen there is a finite number of observations (say, N) in the population, the population mean is\n\\begin{aligned}\n\\mu & = \\frac{\\sum_{i=1}^{N} X_{i}}{N}\n\\end{aligned}\n\n\nThe sample mean, \\overline{X}, is a reasonable estimate of the population mean, \\mu.\n\n\n\n\n\n\n\n\n\nNoteExample: O-Ring Tensile Strength\n\n\n\nEngineering Context: This example demonstrates how the sample mean serves as a measure of central tendency in materials testing. Understanding tensile strength is crucial for ensuring component reliability in aerospace applications.\nConsider the O-ring tensile strength experiment described in Chapter 1. The data from the modified rubber compound (1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073) are shown in the dot diagram (Figure 1 and Figure 2).\nManual Calculation: The sample mean strength (psi) for the eight observations is:\n\\begin{aligned}\n\\overline{X} & = \\frac{\\sum_{i=1}^{n} X_{i}}{n} \\\\\n\\overline{X} & = \\frac{1037 + 1047 + \\cdots + 1073}{8} \\\\\n\\overline{X} & = \\frac{8440}{8} = 1055  \\text{ psi}\n\\end{aligned}\nPhysical Interpretation: The sample mean \\overline{X} = 1055 can be thought of as a “balance point.” If each observation represents 1 pound of mass placed at the point on the x-axis, a fulcrum located at \\overline{X} would exactly balance this system of weights.\nEngineering Significance: This mean value provides a single representative measure that engineers can use to compare different rubber compounds or assess whether the material meets specifications.\n\n\n\n\n\n\nFigure 1: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\nR OutputR Code\n\n\n\n\n\nO-Ring Tensile Strength Statistics\n\n\nn\nsum\nmean\n\n\n\n\n8\n8440\n1055\n\n\n\n\n\n\n\n\n# Load required libraries\nlibrary(fastverse)\nlibrary(kit)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(car)\nlibrary(GGally)\n\n# O-ring tensile strength example\ntensile_data &lt;-\n  data.table(TS = c(1048, 1059, 1047, 1066, 1040, 1070, 1037, 1073))\n\ntensile_data_summary1 &lt;-\n  tensile_data %&gt;%\n  fsummarise(\n    n = fnobs(TS),\n    sum = fsum(TS),\n    mean = fmean(TS)\n  )\n\ntensile_data_summary1 %&gt;%\n  kbl(caption = \"O-Ring Tensile Strength Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Dot diagram of O-ring tensile strength. The sample mean is shown as a balance point for a system of weights.\n\n\n\n\n\n\n\n\n# Dot diagram showing sample mean as balance point\nggplot(data = tensile_data, mapping = aes(x = TS)) +\n  geom_dotplot(binwidth = 1, stackdir = \"up\", dotsize = 1) +\n  geom_vline(aes(xintercept = fmean(TS)), color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(x = \"Tensile Strength (psi)\", y = NULL, title = \"Sample Mean as Balance Point\") +\n  theme_classic() +\n  theme(axis.ticks.y = element_blank(), axis.text.y = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Variance and Sample Standard Deviation\n\n\n\nIf the n observations in a sample are denoted by X_{1},X_{2},\\ldots,X_{n}, then the sample variance is\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n\\end{aligned}\nThe sample standard deviation, S, is the positive square root of the sample variance.\n\n\nEngineering Perspective: The units of measurement for the sample variance are the square of the original units. Thus, if X is measured in psi, the variance units are (\\text{psi})^2. The standard deviation has the desirable property of measuring variability in the original units (psi), making it more interpretable for engineers.\n\n\n\n\n\n\n\n\n\nNoteExample: Variability in O-Ring Strength\n\n\n\nWhy Variability Matters: In engineering applications, understanding variability is often more critical than knowing the average. High variability can indicate process instability, quality issues, or the need for tighter manufacturing controls.\nThe numerator of S^2 is (See Table 1): \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2} = 1348\nSample Variance Calculation:\n\\begin{aligned}\nS^2 & = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{n-1}\n= \\frac{1348}{8-1} = 192.57 \\text{ psi}^2 \\end{aligned}\nSample Standard Deviation:\n\\begin{aligned}\nS & = \\sqrt{192.57} = 13.9 \\text{ psi} \\end{aligned}\nEngineering Interpretation: A standard deviation of 13.9 psi means that most O-ring tensile strengths fall within about 14 psi of the mean (1055 psi). This gives engineers insight into the consistency of the manufacturing process.\nComputational Note: \\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}=\\sum_{i=1}^{n}X_{i}^{2}-\\frac{\\left(\\sum_{i=1}^{n}X_{i}\\right)^{2}}{n} (computational formula).\n\nR OutputR Code\n\n\n\n\n\nO-Ring Tensile Strength Statistics\n\n\nvar\nsd\n\n\n\n\n192.57\n13.88\n\n\n\n\n\n\n\n\ntensile_data_summary2 &lt;-\n  tensile_data %&gt;%\n  fsummarise(\n    var = fvar(TS),\n    sd = fsd(TS)\n  )\n\ntensile_data_summary2 %&gt;%\n  kbl(caption = \"O-Ring Tensile Strength Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\nTable 1: Variance Calculations\n\n\n\n\nVariance Calculation Terms\n\n\ni\nXi\nXi - X̄\n(Xi - X̄)²\n\n\n\n\n1\n1048\n-7\n49\n\n\n2\n1059\n4\n16\n\n\n3\n1047\n-8\n64\n\n\n4\n1066\n11\n121\n\n\n5\n1040\n-15\n225\n\n\n6\n1070\n15\n225\n\n\n7\n1037\n-18\n324\n\n\n8\n1073\n18\n324\n\n\nTotal\n8440\n0\n1348\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteUnderstanding Variability Measurement\n\n\n\nEngineering Insight: To see how the sample variance measures dispersion or variability, refer to Figure 3, which shows the deviations X_{i}-\\overline{X} for the O-ring tensile strength data.\nKey Concepts:\n\nGreater variability → larger absolute deviations from the mean\nDeviations X_{i}-\\overline{X} always sum to zero\nSquaring eliminates negative values and emphasizes larger deviations\nSmall S^2 indicates consistent process; large S^2 suggests high variability\n\nProcess Control Implications: Engineers use variance to monitor process stability. Increasing variance over time may signal equipment wear, material inconsistencies, or environmental factors affecting production.\n\n\n\n\n\n\nFigure 3: How the sample variance measures variability through the deviations X_{i}-\\overline{X}.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePopulation Variance\n\n\n\nWhen the population is finite and consists of N values, we may define the population variance as\n\\begin{aligned}\n\\sigma^2 & = \\frac{\\sum_{i=1}^{N}\\left(X_{i}-\\mu\\right)^{2}}{N}\n\\end{aligned}\n\n\nThe sample variance, S^2, is a reasonable estimate of the population variance, \\sigma^2.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#stem-and-leaf-diagram",
    "href": "book/Ch02.html#stem-and-leaf-diagram",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteStem-and-Leaf Diagram\n\n\n\nEngineering Application: Stem-and-leaf diagrams are particularly useful for initial data exploration in engineering. They preserve actual data values while showing distribution shape, making them ideal for quality control and process analysis.\nA stem-and-leaf diagram provides an informative visual display of a data set X_{1},X_{2},\\ldots,X_{n}, where each number X_{i} consists of at least two digits.\n\n\n\n\n\n\nTipSteps for Constructing a Stem-and-Leaf Diagram\n\n\n\n\nDivide each number X_{i} into two parts: a stem (leading digits) and a leaf (remaining digit)\nList the stem values in a vertical column\nRecord the leaf for each observation beside its stem\nWrite the units for stems and leaves on the display\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Aluminum-Lithium Alloy Compressive Strength\n\n\n\nEngineering Context: This example analyzes compressive strength data from 80 aluminum-lithium alloy specimens. Such analysis is crucial for aerospace applications where weight reduction and strength are both critical.\nData Overview: The compressive strength measurements (in psi) from Table 2 represent quality control testing of a lightweight alloy used in aircraft components.\n\n\n\nTable 2: Compressive Strength of 80 Aluminum-Lithium Alloy Specimens\n\n\n\n\n\n105\n221\n183\n186\n121\n181\n180\n143\n\n\n97\n154\n153\n174\n120\n168\n167\n141\n\n\n245\n228\n174\n199\n181\n158\n176\n110\n\n\n163\n131\n154\n115\n160\n208\n158\n133\n\n\n207\n180\n190\n193\n194\n133\n156\n123\n\n\n134\n178\n76\n167\n184\n135\n229\n146\n\n\n218\n157\n101\n171\n165\n172\n158\n169\n\n\n199\n151\n142\n163\n145\n171\n148\n158\n\n\n160\n175\n149\n87\n160\n237\n150\n135\n\n\n196\n201\n200\n176\n150\n170\n118\n149\n\n\n\n\n\n\nWe select stem values as the numbers 7, 8, 9, \\ldots, 24 (representing tens place).\n\nR OutputR CodeDownload Data\n\n\n\n\n\n  The decimal point is 1 digit(s) to the right of the |\n\n   7 | 6\n   8 | 7\n   9 | 7\n  10 | 15\n  11 | 058\n  12 | 013\n  13 | 133455\n  14 | 12356899\n  15 | 001344678888\n  16 | 0003357789\n  17 | 0112445668\n  18 | 0011346\n  19 | 034699\n  20 | 0178\n  21 | 8\n  22 | 189\n  23 | 7\n  24 | 5\n\n\n\n\n\n# Stem-and-leaf diagram for compressive strength data\ncompressive_data &lt;- fread(\"./data/Exmp2.4.csv\")\nstem(compressive_data[, CS], scale = 2, width = 80)\n\n\n\n\n\n\n\nEngineering Interpretation: The stem-and-leaf display reveals:\n\nMost compressive strengths lie between 110 and 200 psi\nCentral value is approximately 150-160 psi\nDistribution is approximately symmetric\nNo obvious outliers or unusual patterns\nThe alloy appears to have consistent strength properties\n\nQuality Control Insights: This symmetric distribution suggests the manufacturing process is well-controlled. Engineers can use this information to set specification limits and monitor future production.\n\n\n\n\n\n\n\n\n\n\nNoteHistogram\n\n\n\nEngineering Definition: A histogram is a graphical representation of data distribution, showing frequency or relative frequency within specified intervals (bins). Histograms are essential tools for:\n\nProcess capability analysis\nQuality control monitoring\nDesign specification validation\nStatistical process control\n\nEach bin represents a range of values, and bar height reflects the number (or proportion) of data points in that range.\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Golf Ball Distance Testing\n\n\n\nEngineering Application: The USGA tests golf balls to ensure conformance to regulations. This standardized testing demonstrates how consistent measurement protocols are essential in engineering quality control.\nTesting Protocol: Balls are tested using “Iron Byron,” a mechanical device that eliminates human variability, ensuring reproducible test conditions. This exemplifies the engineering principle of controlling sources of variation.\n\n\n\nTable 3: Golf Ball Distance Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n291.5\n274.4\n290.2\n276.4\n272.0\n268.7\n281.6\n281.6\n276.3\n285.9\n\n\n269.6\n266.6\n283.6\n269.6\n277.8\n287.8\n267.6\n292.6\n273.4\n284.4\n\n\n270.7\n274.0\n285.2\n275.5\n272.1\n261.3\n274.0\n279.3\n281.0\n293.1\n\n\n277.5\n278.0\n272.5\n271.7\n280.8\n265.6\n260.1\n272.5\n281.3\n263.0\n\n\n279.0\n267.3\n283.5\n271.2\n268.5\n277.1\n266.2\n266.4\n271.5\n280.3\n\n\n267.8\n272.1\n269.7\n278.5\n277.3\n280.5\n270.8\n267.7\n255.1\n276.4\n\n\n283.7\n281.7\n282.2\n274.1\n264.5\n281.0\n273.2\n274.4\n281.6\n273.7\n\n\n271.0\n271.5\n289.7\n271.1\n256.9\n274.5\n286.2\n273.9\n268.5\n262.6\n\n\n261.9\n258.9\n293.2\n267.1\n255.0\n269.7\n281.9\n269.6\n279.8\n269.9\n\n\n282.6\n270.0\n265.2\n277.7\n275.5\n272.2\n270.0\n271.0\n284.3\n268.4\n\n\n\n\n\n\nHistogram Design Principles: With 100 observations, \\sqrt{100} = 10 suggests about 10 bins for optimal visualization. The analysis reveals important distribution characteristics for engineering decision-making.\nEngineering Insights from Histograms:\n\nDistribution Shape: The reasonably symmetric, bell-shaped distribution suggests the manufacturing process is well-controlled\nProcess Capability: The spread indicates natural process variation\nSpecification Conformance: Engineers can assess what percentage of balls meet distance requirements\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram with 9 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram with 16 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A relative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A cumulative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\nFigure 4: Histogram for the golf ball distance data\n\n\n\n\n\n\n# Golf ball distance histograms\ngolf_data &lt;- fread(\"./data/Exmp2.6.csv\")\n\n# Basic histogram with 9 bins\np1 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 9, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 9 bins\") +\n  theme_classic()\n\n# Histogram with 16 bins\np2 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 16, fill = \"lightgreen\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 16 bins\") +\n  theme_classic()\n\n# Relative frequency histogram\np3 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(count / sum(count))),\n    bins = 9,\n    fill = \"orange\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), labels = percent_format()) +\n  labs(x = \"Distance (yards)\", y = \"Relative Frequency\", title = \"Relative Frequency\") +\n  theme_classic()\n\n# Cumulative frequency histogram\np4 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(cumsum(count))),\n    bins = 10,\n    fill = \"pink\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Cumulative Frequency\", title = \"Cumulative Frequency\") +\n  theme_classic()\n\nprint(p1)\nprint(p2)\nprint(p3)\nprint(p4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePareto Chart\n\n\n\nEngineering Application: Pareto charts are fundamental tools in quality engineering and process improvement, embodying the “80-20 rule” or “vital few vs. trivial many” principle. They help engineers prioritize improvement efforts by identifying the most significant causes of problems.\nKey Features:\n\nCategories ordered by frequency (highest to lowest)\nFocuses attention on major contributors\nGuides resource allocation for maximum impact\nEssential for root cause analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Aircraft Safety Analysis\n\n\n\nEngineering Context: This analysis of aircraft accident data demonstrates how Pareto charts help engineers identify critical safety issues and prioritize design improvements.\nData Source: Aircraft accident rates from The Wall Street Journal analysis, showing hull losses per million departures between 1959-1999 for 22 aircraft types.\n\n\n\nTable 4: Aircraft Accident Data\n\n\n\n\n\n\n\n\n\n\nAircraft Type\nActual Number of Hull Losses\nHull Losses/Million Departures\n\n\n\n\nMD-11\n5\n6.54\n\n\n707/720\n115\n6.46\n\n\nDC-8\n71\n5.84\n\n\nF-28\n32\n3.94\n\n\nBAC 1-11\n22\n2.64\n\n\nDC-10\n20\n2.57\n\n\n747-Early\n21\n1.9\n\n\n\n\n\n\nEngineering Analysis: The top three aircraft types account for the majority of incidents on a per-million-departures basis. Notably:\n\n707/720 and DC-8: Mid-1950s designs, largely out of service\nMD-11: 1990s design with concerning safety record (5 losses out of 198 aircraft)\n\nProcess Improvement Insights: This Pareto analysis helps aviation engineers:\n\nFocus safety improvements on high-risk aircraft designs\nUnderstand the relationship between design era and safety performance\nAllocate research resources to address the “vital few” risk factors\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 5: Pareto chart for the aircraft accident data\n\n\n\n\n\n\n\n\n# Pareto chart for aircraft accident data\naircraft_data &lt;- fread(\"./data/Exmp2.8.csv\")\n\naircraft_data %&gt;%\n  fmutate(Type = factor(Type, levels = Type[order(-HLMD)])) %&gt;%\n  ggplot(aes(x = Type, y = HLMD)) +\n  geom_col(fill = \"steelblue\", color = \"black\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  labs(\n    x = \"Aircraft Type\",\n    y = \"Hull Losses per Million Departures\",\n    title = \"Pareto Chart: Aircraft Accident Rates\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBox Plot\n\n\n\nEngineering Applications: Box plots are powerful tools for engineering data analysis, providing comprehensive information about data distribution in a compact visual format.\nWhat Box Plots Reveal:\n\nMeasure of Central Tendency (median)\nMeasure of Variability (IQR, range)\nMeasure of Symmetry (distribution shape)\nOutliers (unusual observations)\nGroup Comparisons (process/design alternatives)\n\n\n Box Whisker Plot\n\nStatistical Definitions:\n\n\\begin{align*}\n\\text{Lower Inner Fence} & =Q_{1}-1.5\\times\\text{IQR}\\\\\n\\text{Upper Inner Fence} & =Q_{3}+1.5\\times\\text{IQR}\\\\\n\\text{Lower Outer Fence} & =Q_{1}-3\\times\\text{IQR}\\\\\n\\text{Upper Outer Fence} & =Q_{3}+3\\times\\text{IQR}\n\\end{align*}\n\nOutlier Classification:\n\nMild Outlier: Beyond inner fence (but inside outer fence)\nExtreme Outlier: Beyond outer fence\n\nEngineering Interpretation: Whiskers extend to the most extreme values that are not outliers, helping engineers identify unusual measurements that may indicate process problems or measurement errors.\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\nBox Plot Statistics\n\n\n\n\nq1\n\n\nmedian\n\n\nq3\n\n\niqr\n\n\nlower_fence\n\n\nupper_fence\n\n\n\n\n\n\n144.5\n\n\n161.5\n\n\n181\n\n\n36.5\n\n\n89.75\n\n\n235.75\n\n\n\n\n\n\nFigure 6: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n\n\nFigure 7: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n# Box plot for compressive strength data\nggplot(compressive_data, aes(y = CS)) +\n  geom_boxplot(width = 0.3, fill = \"lightblue\") +\n  scale_y_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(\n    y = \"Compressive Strength (psi)\", x = NULL,\n    title = \"Box Plot: Compressive Strength Distribution\"\n  ) +\n  theme_classic() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())\n\nbox_stats &lt;- compressive_data %&gt;%\n  fsummarise(\n    q1 = fquantile(CS, 0.25),\n    median = fmedian(CS),\n    q3 = fquantile(CS, 0.75),\n    iqr = fquantile(CS, 0.75) - fquantile(CS, 0.25),\n    lower_fence = fquantile(CS, 0.25) - 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25)),\n    upper_fence = fquantile(CS, 0.75) + 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25))\n  )\n\nbox_stats %&gt;%\n  kbl(caption = \"Box Plot Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Comparative box plots of a quality index at three plants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTime Series Plots\n\n\n\nEngineering Importance: Time series analysis is critical in engineering for monitoring processes, identifying trends, and predicting future behavior. Understanding temporal patterns helps engineers:\n\nDetect process drift or shifts\nIdentify cyclical behaviors\nPlan maintenance schedules\nOptimize production timing\n\nKey Concepts:\n\nTime Series: Data recorded in sequential order over time\nTrend: Long-term increase or decrease in values\nCycles: Regular patterns that repeat over time\nSeasonality: Predictable patterns related to calendar periods\n\nEngineering Applications:\n\nEquipment performance monitoring\nQuality control charts\nProduction scheduling\nPredictive maintenance\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Company sales by year\n\n\n\n\n\n\n\n\n\n\n\n(b) Company sales by quarter\n\n\n\n\n\n\n\nFigure 9: Company sales\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Annual sales trend\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Quarterly sales pattern\n\n\n\n\n\n\n\nFigure 10: Time series plots showing trends and patterns\n\n\n\n\n\n\n# Time series example (simulated company sales data)\nset.seed(123)\ntime_data &lt;-\n  data.table(\n    year = 2015:2024,\n    quarter = rep(1:4, length.out = 40),\n    sales = 100 + 0.02 * (1:40)^2 + 10 * sin(2 * pi * (1:40) / 4) + rnorm(40, 0, 5)\n  ) %&gt;%\n  fmutate(time_period = year + (quarter - 1) / 4)\n\n# Annual sales trend\np5 &lt;-\n  time_data %&gt;%\n  fgroup_by(year) %&gt;%\n  fsummarise(annual_sales = fmean(sales)) %&gt;%\n  ggplot(aes(x = year, y = annual_sales)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(size = 3) +\n  labs(x = \"Year\", y = \"Average Sales\", title = \"Annual Sales Trend\") +\n  theme_classic()\n\n# Quarterly sales pattern\np6 &lt;-\n  ggplot(time_data, aes(x = time_period, y = sales)) +\n  geom_line(color = \"red\", linewidth = 1) +\n  geom_point(size = 2) +\n  labs(x = \"Time Period\", y = \"Sales\", title = \"Quarterly Sales Pattern\") +\n  theme_classic()\n\nprint(p5)\nprint(p6)\n\nfwrite(time_data, \"data/time_series_example.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: A digidot plot of the compressive strength data.\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 12: Digital dot plot of compressive strength data\n\n\n\n\n\n\n\n\n# Digital dot plot\ncompressive_data %&gt;%\n  fmutate(digit = round(CS / 10) * 10, count = 1) %&gt;%\n  fgroup_by(digit) %&gt;%\n  fsummarise(frequency = fsum(count)) %&gt;%\n  ggplot(aes(x = digit, y = frequency)) +\n  geom_segment(aes(xend = digit, yend = 0), linewidth = 1) +\n  geom_point(size = 3, color = \"red\") +\n  labs(\n    x = \"Compressive Strength (grouped by 10s)\", y = \"Frequency\",\n    title = \"Digital Dot Plot\"\n  ) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMultivariate Data\n\n\n\nEngineering Reality: Most engineering problems involve multiple variables simultaneously. Understanding relationships between variables is essential for:\n\nProcess optimization\nDesign improvement\nRoot cause analysis\nPredictive modeling\n\nKey Concepts:\n\nUnivariate Analysis: Single variable (mean, variance, distribution)\nMultivariate Analysis: Multiple variables and their relationships\nCorrelation: Strength of linear relationship between variables\nCausation vs. Correlation: Important distinction for engineering decisions\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Wire Bond Strength Analysis\n\n\n\nEngineering Context: Wire bonding is a critical process in semiconductor manufacturing. Understanding how process parameters affect bond strength is essential for reliable electronic devices.\nProcess Variables:\n\nPull Strength (Y): Response variable (quality measure)\nWire Length (X1): Process parameter\nDie Height (X2): Design parameter\n\n\n\n\nTable 5: Wire Bond Data\n\n\n\n\n\nObs No\nPull Strength (Y)\nWire Length (X1)\nDie Height (X2)\n\n\n\n\n1\n9.95\n2\n50\n\n\n2\n24.45\n8\n110\n\n\n3\n31.75\n11\n120\n\n\n4\n35.00\n10\n550\n\n\n5\n25.02\n8\n295\n\n\n6\n16.86\n4\n200\n\n\n7\n14.38\n2\n375\n\n\n8\n9.60\n2\n52\n\n\n9\n24.35\n9\n100\n\n\n10\n27.50\n8\n300\n\n\n11\n17.08\n4\n412\n\n\n12\n37.00\n11\n400\n\n\n13\n41.95\n12\n500\n\n\n14\n11.66\n2\n360\n\n\n15\n21.65\n4\n205\n\n\n16\n17.89\n4\n400\n\n\n17\n69.00\n20\n600\n\n\n18\n10.30\n1\n585\n\n\n19\n34.93\n10\n540\n\n\n20\n46.59\n15\n250\n\n\n21\n44.88\n15\n290\n\n\n22\n54.12\n16\n510\n\n\n23\n56.63\n17\n590\n\n\n24\n22.13\n6\n100\n\n\n25\n21.15\n5\n400\n\n\n\n\n\n\nEngineering Questions:\n\nWhich parameter most strongly affects bond strength?\nAre the parameters independent of each other?\nCan we predict strength from parameter settings?\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scatter plot for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot for Pull strength versus die height\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scatter and box plots for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(d) Scatter and box plots for Pull strength versus die height\n\n\n\n\n\n\n\nFigure 13: Scatter diagrams and box plots for the wire bond pull strength data\n\n\n\n\n\n\n# Wire bond data analysis\nwire_data &lt;- fread(\"./data/Table2.9.csv\")\n\n# Scatter plot: Pull strength vs Wire length\np7 &lt;-\n  ggplot(wire_data, aes(x = X1, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"Wire Length\", y = \"Pull Strength\", title = \"Pull Strength vs Wire Length\") +\n  theme_classic()\n\n# Scatter plot: Pull strength vs Die height\np8 &lt;-\n  ggplot(wire_data, aes(x = X2, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Die Height\", y = \"Pull Strength\", title = \"Pull Strength vs Die Height\") +\n  theme_classic()\n\nprint(p7)\nprint(p8)\n\nlibrary(car)\nscatterplot(\n  formula = Y ~ X1,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Length\",\n  ylab = \"Strength\"\n)\n\nscatterplot(\n  formula = Y ~ X2,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Die height\",\n  ylab = \"Strength\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Correlation Coefficient\n\n\n\nEngineering Definition: The correlation coefficient quantifies the strength and direction of linear relationships between variables, crucial for understanding process interactions.\nGiven n pairs of data \\left(X_{1},Y_{1}\\right),\\left(X_{2},Y_{2}\\right),\\ldots,\\left(X_{n},Y_{n}\\right):\n\nr=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(Y_{i}-\\overline{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\overline{Y}\\right)^{2}}}\n\nwhere -1\\leq r\\leq+1.\nEngineering Interpretation:\n\nr ≈ +1: Strong positive relationship (as X increases, Y increases)\nr ≈ -1: Strong negative relationship (as X increases, Y decreases)\nr ≈ 0: Weak linear relationship (may still have nonlinear relationship)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) r is near +1\n\n\n\n\n\n\n\n\n\n\n\n(b) r is near -1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) r is near 0, Y and X are unrelated\n\n\n\n\n\n\n\n\n\n\n\n(d) r is near 0, Y and X are nonlinearly related\n\n\n\n\n\n\n\nFigure 14: Scatter diagrams for different values of the sample correlation coefficient r.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Shampoo Quality Analysis\n\n\n\nEngineering Context: This multivariate analysis demonstrates how engineers evaluate product quality across multiple sensory and performance characteristics. Understanding these relationships helps optimize formulations.\nQuality Variables:\n\nFoam: Lathering performance\nScent: Fragrance intensity\nColor: Visual appeal\nResidue: Cleansing effectiveness\nRegion: Market segment\nQuality: Overall consumer rating\n\n\n\n\nTable 6: Data on Shampoo\n\n\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\n6.3\n5.3\n4.8\n3.1\n1\n91\n\n\n4.4\n4.9\n3.5\n3.9\n1\n87\n\n\n3.9\n5.3\n4.8\n4.7\n1\n82\n\n\n5.1\n4.2\n3.1\n3.6\n1\n83\n\n\n5.6\n5.1\n5.5\n5.1\n1\n83\n\n\n4.6\n4.7\n5.1\n4.1\n1\n84\n\n\n4.8\n4.8\n4.8\n3.3\n1\n90\n\n\n6.5\n4.5\n4.3\n5.2\n1\n84\n\n\n8.7\n4.3\n3.9\n2.9\n1\n97\n\n\n8.3\n3.9\n4.7\n3.9\n1\n93\n\n\n5.1\n4.3\n4.5\n3.6\n1\n82\n\n\n3.3\n5.4\n4.3\n3.6\n1\n84\n\n\n5.9\n5.7\n7.2\n4.1\n2\n87\n\n\n7.7\n6.6\n6.7\n5.6\n2\n80\n\n\n7.1\n4.4\n5.8\n4.1\n2\n84\n\n\n5.5\n5.6\n5.6\n4.4\n2\n84\n\n\n6.3\n5.4\n4.8\n4.6\n2\n82\n\n\n4.3\n5.5\n5.5\n4.1\n2\n79\n\n\n4.6\n4.1\n4.3\n3.1\n2\n81\n\n\n3.4\n5.0\n3.4\n3.4\n2\n83\n\n\n6.4\n5.4\n6.6\n4.8\n2\n81\n\n\n5.5\n5.3\n5.3\n3.8\n2\n84\n\n\n4.7\n4.1\n5.0\n3.7\n2\n83\n\n\n4.1\n4.0\n4.1\n4.0\n2\n80\n\n\n\n\n\n\nEngineering Analysis Goals:\n\nIdentify which characteristics most influence overall quality\nUnderstand relationships between formulation parameters\nOptimize product design for target markets\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\nTable 7: Correlation Matrix\n\n\n\n\nShampoo Data Correlation Matrix\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\nFoam\n1.000\n0.002\n0.328\n0.193\n-0.032\n0.512\n\n\nScent\n0.002\n1.000\n0.599\n0.500\n0.278\n-0.252\n\n\nColor\n0.328\n0.599\n1.000\n0.524\n0.458\n-0.194\n\n\nResidue\n0.193\n0.500\n0.524\n1.000\n0.165\n-0.489\n\n\nRegion\n-0.032\n0.278\n0.458\n0.165\n1.000\n-0.507\n\n\nQuality\n0.512\n-0.252\n-0.194\n-0.489\n-0.507\n1.000\n\n\n\n\n\n\nShampoo Data Summary Statistics\n\n\nFoam_mean\nFoam_sd\nFoam_min\nFoam_max\nScent_mean\nScent_sd\nScent_min\nScent_max\nColor_mean\nColor_sd\nColor_min\nColor_max\nResidue_mean\nResidue_sd\nResidue_min\nResidue_max\nRegion_mean\nRegion_sd\nRegion_min\nRegion_max\nQuality_mean\nQuality_sd\nQuality_min\nQuality_max\n\n\n\n\n5.5\n1.45\n3.3\n8.7\n4.91\n0.67\n3.9\n6.6\n4.9\n1.02\n3.1\n7.2\n4.03\n0.7\n2.9\n5.6\n1.5\n0.51\n1\n2\n84.5\n4.36\n79\n97\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8\n\n\n# Shampoo data correlation matrix\nshampoo_data &lt;- fread(\"./data/Table2.11.csv\")\n\ncor_matrix &lt;- cor(shampoo_data)\ncor_matrix %&gt;%\n  round(3) %&gt;%\n  kbl(caption = \"Shampoo Data Correlation Matrix\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\nshampoo_summary &lt;-\n  shampoo_data %&gt;%\n  fsummarise(\n    across(\n      .cols = is.numeric,\n      .fns = list(\n        mean = fmean,\n        sd = fsd,\n        min = fmin,\n        max = fmax\n      )\n    )\n  )\n\nshampoo_summary %&gt;%\n  kbl(caption = \"Shampoo Data Summary Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 15: Matrix of scatter plots for the shampoo data\n\n\n\n\n\n\n\n\n# Matrix of scatter plots for shampoo data\nggpairs(shampoo_data,\n  title = \"Shampoo Quality Data: Pairwise Relationships\",\n  upper = list(continuous = wrap(\"cor\", size = 3))\n) +\n  theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis chapter provided essential tools for engineering data analysis:\n\n\n\nMeasures of Central Tendency: Sample mean as balance point for process centering\nMeasures of Variability: Standard deviation for process control and capability analysis\nDistribution Analysis: Understanding natural process variation patterns\n\n\n\n\n\nHistograms: Assess process capability and distribution normality\nBox Plots: Identify outliers and compare process alternatives\nStem-and-Leaf: Quick distribution assessment preserving actual values\nPareto Charts: Prioritize improvement efforts using 80-20 principle\n\n\n\n\n\nTime Series: Monitor trends, detect shifts, identify cycles\nCorrelation Analysis: Understand variable relationships for optimization\nMultivariate Methods: Analyze complex engineering systems\n\n\n\n\n\nQuality control and process improvement\nDesign optimization and parameter selection\nRoot cause analysis and problem solving\nSpecification setting and capability assessment\n\nThese statistical tools form the foundation for evidence-based engineering decisions and continuous improvement initiatives.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#histograms",
    "href": "book/Ch02.html#histograms",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteHistogram\n\n\n\nEngineering Definition: A histogram is a graphical representation of data distribution, showing frequency or relative frequency within specified intervals (bins). Histograms are essential tools for:\n\nProcess capability analysis\nQuality control monitoring\nDesign specification validation\nStatistical process control\n\nEach bin represents a range of values, and bar height reflects the number (or proportion) of data points in that range.\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Golf Ball Distance Testing\n\n\n\nEngineering Application: The USGA tests golf balls to ensure conformance to regulations. This standardized testing demonstrates how consistent measurement protocols are essential in engineering quality control.\nTesting Protocol: Balls are tested using “Iron Byron,” a mechanical device that eliminates human variability, ensuring reproducible test conditions. This exemplifies the engineering principle of controlling sources of variation.\n\n\n\nTable 3: Golf Ball Distance Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n291.5\n274.4\n290.2\n276.4\n272.0\n268.7\n281.6\n281.6\n276.3\n285.9\n\n\n269.6\n266.6\n283.6\n269.6\n277.8\n287.8\n267.6\n292.6\n273.4\n284.4\n\n\n270.7\n274.0\n285.2\n275.5\n272.1\n261.3\n274.0\n279.3\n281.0\n293.1\n\n\n277.5\n278.0\n272.5\n271.7\n280.8\n265.6\n260.1\n272.5\n281.3\n263.0\n\n\n279.0\n267.3\n283.5\n271.2\n268.5\n277.1\n266.2\n266.4\n271.5\n280.3\n\n\n267.8\n272.1\n269.7\n278.5\n277.3\n280.5\n270.8\n267.7\n255.1\n276.4\n\n\n283.7\n281.7\n282.2\n274.1\n264.5\n281.0\n273.2\n274.4\n281.6\n273.7\n\n\n271.0\n271.5\n289.7\n271.1\n256.9\n274.5\n286.2\n273.9\n268.5\n262.6\n\n\n261.9\n258.9\n293.2\n267.1\n255.0\n269.7\n281.9\n269.6\n279.8\n269.9\n\n\n282.6\n270.0\n265.2\n277.7\n275.5\n272.2\n270.0\n271.0\n284.3\n268.4\n\n\n\n\n\n\nHistogram Design Principles: With 100 observations, \\sqrt{100} = 10 suggests about 10 bins for optimal visualization. The analysis reveals important distribution characteristics for engineering decision-making.\nEngineering Insights from Histograms:\n\nDistribution Shape: The reasonably symmetric, bell-shaped distribution suggests the manufacturing process is well-controlled\nProcess Capability: The spread indicates natural process variation\nSpecification Conformance: Engineers can assess what percentage of balls meet distance requirements\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Histogram with 9 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Histogram with 16 bins for the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) A relative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) A cumulative frequency plot of the golf ball distance data\n\n\n\n\n\n\n\nFigure 4: Histogram for the golf ball distance data\n\n\n\n\n\n\n# Golf ball distance histograms\ngolf_data &lt;- fread(\"./data/Exmp2.6.csv\")\n\n# Basic histogram with 9 bins\np1 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 9, fill = \"lightblue\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 9 bins\") +\n  theme_classic()\n\n# Histogram with 16 bins\np2 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(bins = 16, fill = \"lightgreen\", color = \"black\") +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Frequency\", title = \"Histogram with 16 bins\") +\n  theme_classic()\n\n# Relative frequency histogram\np3 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(count / sum(count))),\n    bins = 9,\n    fill = \"orange\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA), labels = percent_format()) +\n  labs(x = \"Distance (yards)\", y = \"Relative Frequency\", title = \"Relative Frequency\") +\n  theme_classic()\n\n# Cumulative frequency histogram\np4 &lt;-\n  ggplot(golf_data, aes(x = Distance)) +\n  geom_histogram(aes(y = after_stat(cumsum(count))),\n    bins = 10,\n    fill = \"pink\", color = \"black\"\n  ) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  labs(x = \"Distance (yards)\", y = \"Cumulative Frequency\", title = \"Cumulative Frequency\") +\n  theme_classic()\n\nprint(p1)\nprint(p2)\nprint(p3)\nprint(p4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePareto Chart\n\n\n\nEngineering Application: Pareto charts are fundamental tools in quality engineering and process improvement, embodying the “80-20 rule” or “vital few vs. trivial many” principle. They help engineers prioritize improvement efforts by identifying the most significant causes of problems.\nKey Features:\n\nCategories ordered by frequency (highest to lowest)\nFocuses attention on major contributors\nGuides resource allocation for maximum impact\nEssential for root cause analysis\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Aircraft Safety Analysis\n\n\n\nEngineering Context: This analysis of aircraft accident data demonstrates how Pareto charts help engineers identify critical safety issues and prioritize design improvements.\nData Source: Aircraft accident rates from The Wall Street Journal analysis, showing hull losses per million departures between 1959-1999 for 22 aircraft types.\n\n\n\nTable 4: Aircraft Accident Data\n\n\n\n\n\n\n\n\n\n\nAircraft Type\nActual Number of Hull Losses\nHull Losses/Million Departures\n\n\n\n\nMD-11\n5\n6.54\n\n\n707/720\n115\n6.46\n\n\nDC-8\n71\n5.84\n\n\nF-28\n32\n3.94\n\n\nBAC 1-11\n22\n2.64\n\n\nDC-10\n20\n2.57\n\n\n747-Early\n21\n1.9\n\n\n\n\n\n\nEngineering Analysis: The top three aircraft types account for the majority of incidents on a per-million-departures basis. Notably:\n\n707/720 and DC-8: Mid-1950s designs, largely out of service\nMD-11: 1990s design with concerning safety record (5 losses out of 198 aircraft)\n\nProcess Improvement Insights: This Pareto analysis helps aviation engineers:\n\nFocus safety improvements on high-risk aircraft designs\nUnderstand the relationship between design era and safety performance\nAllocate research resources to address the “vital few” risk factors\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 5: Pareto chart for the aircraft accident data\n\n\n\n\n\n\n\n\n# Pareto chart for aircraft accident data\naircraft_data &lt;- fread(\"./data/Exmp2.8.csv\")\n\naircraft_data %&gt;%\n  fmutate(Type = factor(Type, levels = Type[order(-HLMD)])) %&gt;%\n  ggplot(aes(x = Type, y = HLMD)) +\n  geom_col(fill = \"steelblue\", color = \"black\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, NA)) +\n  scale_x_discrete(guide = guide_axis(n.dodge = 2)) +\n  labs(\n    x = \"Aircraft Type\",\n    y = \"Hull Losses per Million Departures\",\n    title = \"Pareto Chart: Aircraft Accident Rates\"\n  ) +\n  theme_classic()",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#box-plot",
    "href": "book/Ch02.html#box-plot",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteBox Plot\n\n\n\nEngineering Applications: Box plots are powerful tools for engineering data analysis, providing comprehensive information about data distribution in a compact visual format.\nWhat Box Plots Reveal:\n\nMeasure of Central Tendency (median)\nMeasure of Variability (IQR, range)\nMeasure of Symmetry (distribution shape)\nOutliers (unusual observations)\nGroup Comparisons (process/design alternatives)\n\n\n Box Whisker Plot\n\nStatistical Definitions:\n\n\\begin{align*}\n\\text{Lower Inner Fence} & =Q_{1}-1.5\\times\\text{IQR}\\\\\n\\text{Upper Inner Fence} & =Q_{3}+1.5\\times\\text{IQR}\\\\\n\\text{Lower Outer Fence} & =Q_{1}-3\\times\\text{IQR}\\\\\n\\text{Upper Outer Fence} & =Q_{3}+3\\times\\text{IQR}\n\\end{align*}\n\nOutlier Classification:\n\nMild Outlier: Beyond inner fence (but inside outer fence)\nExtreme Outlier: Beyond outer fence\n\nEngineering Interpretation: Whiskers extend to the most extreme values that are not outliers, helping engineers identify unusual measurements that may indicate process problems or measurement errors.\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\nBox Plot Statistics\n\n\n\n\nq1\n\n\nmedian\n\n\nq3\n\n\niqr\n\n\nlower_fence\n\n\nupper_fence\n\n\n\n\n\n\n144.5\n\n\n161.5\n\n\n181\n\n\n36.5\n\n\n89.75\n\n\n235.75\n\n\n\n\n\n\nFigure 6: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n\n\nFigure 7: Box plot for compressive strength data\n\n\n\n\n\n\n\n\n# Box plot for compressive strength data\nggplot(compressive_data, aes(y = CS)) +\n  geom_boxplot(width = 0.3, fill = \"lightblue\") +\n  scale_y_continuous(breaks = pretty_breaks(n = 8)) +\n  labs(\n    y = \"Compressive Strength (psi)\", x = NULL,\n    title = \"Box Plot: Compressive Strength Distribution\"\n  ) +\n  theme_classic() +\n  theme(axis.ticks.x = element_blank(), axis.text.x = element_blank())\n\nbox_stats &lt;- compressive_data %&gt;%\n  fsummarise(\n    q1 = fquantile(CS, 0.25),\n    median = fmedian(CS),\n    q3 = fquantile(CS, 0.75),\n    iqr = fquantile(CS, 0.75) - fquantile(CS, 0.25),\n    lower_fence = fquantile(CS, 0.25) - 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25)),\n    upper_fence = fquantile(CS, 0.75) + 1.5 * (fquantile(CS, 0.75) - fquantile(CS, 0.25))\n  )\n\nbox_stats %&gt;%\n  kbl(caption = \"Box Plot Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Comparative box plots of a quality index at three plants.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#time-series-plots",
    "href": "book/Ch02.html#time-series-plots",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteTime Series Plots\n\n\n\nEngineering Importance: Time series analysis is critical in engineering for monitoring processes, identifying trends, and predicting future behavior. Understanding temporal patterns helps engineers:\n\nDetect process drift or shifts\nIdentify cyclical behaviors\nPlan maintenance schedules\nOptimize production timing\n\nKey Concepts:\n\nTime Series: Data recorded in sequential order over time\nTrend: Long-term increase or decrease in values\nCycles: Regular patterns that repeat over time\nSeasonality: Predictable patterns related to calendar periods\n\nEngineering Applications:\n\nEquipment performance monitoring\nQuality control charts\nProduction scheduling\nPredictive maintenance\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Company sales by year\n\n\n\n\n\n\n\n\n\n\n\n(b) Company sales by quarter\n\n\n\n\n\n\n\nFigure 9: Company sales\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Annual sales trend\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Quarterly sales pattern\n\n\n\n\n\n\n\nFigure 10: Time series plots showing trends and patterns\n\n\n\n\n\n\n# Time series example (simulated company sales data)\nset.seed(123)\ntime_data &lt;-\n  data.table(\n    year = 2015:2024,\n    quarter = rep(1:4, length.out = 40),\n    sales = 100 + 0.02 * (1:40)^2 + 10 * sin(2 * pi * (1:40) / 4) + rnorm(40, 0, 5)\n  ) %&gt;%\n  fmutate(time_period = year + (quarter - 1) / 4)\n\n# Annual sales trend\np5 &lt;-\n  time_data %&gt;%\n  fgroup_by(year) %&gt;%\n  fsummarise(annual_sales = fmean(sales)) %&gt;%\n  ggplot(aes(x = year, y = annual_sales)) +\n  geom_line(color = \"blue\", linewidth = 1) +\n  geom_point(size = 3) +\n  labs(x = \"Year\", y = \"Average Sales\", title = \"Annual Sales Trend\") +\n  theme_classic()\n\n# Quarterly sales pattern\np6 &lt;-\n  ggplot(time_data, aes(x = time_period, y = sales)) +\n  geom_line(color = \"red\", linewidth = 1) +\n  geom_point(size = 2) +\n  labs(x = \"Time Period\", y = \"Sales\", title = \"Quarterly Sales Pattern\") +\n  theme_classic()\n\nprint(p5)\nprint(p6)\n\nfwrite(time_data, \"data/time_series_example.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: A digidot plot of the compressive strength data.\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 12: Digital dot plot of compressive strength data\n\n\n\n\n\n\n\n\n# Digital dot plot\ncompressive_data %&gt;%\n  fmutate(digit = round(CS / 10) * 10, count = 1) %&gt;%\n  fgroup_by(digit) %&gt;%\n  fsummarise(frequency = fsum(count)) %&gt;%\n  ggplot(aes(x = digit, y = frequency)) +\n  geom_segment(aes(xend = digit, yend = 0), linewidth = 1) +\n  geom_point(size = 3, color = \"red\") +\n  labs(\n    x = \"Compressive Strength (grouped by 10s)\", y = \"Frequency\",\n    title = \"Digital Dot Plot\"\n  ) +\n  theme_classic()",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#multivariate-data",
    "href": "book/Ch02.html#multivariate-data",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "NoteMultivariate Data\n\n\n\nEngineering Reality: Most engineering problems involve multiple variables simultaneously. Understanding relationships between variables is essential for:\n\nProcess optimization\nDesign improvement\nRoot cause analysis\nPredictive modeling\n\nKey Concepts:\n\nUnivariate Analysis: Single variable (mean, variance, distribution)\nMultivariate Analysis: Multiple variables and their relationships\nCorrelation: Strength of linear relationship between variables\nCausation vs. Correlation: Important distinction for engineering decisions\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Wire Bond Strength Analysis\n\n\n\nEngineering Context: Wire bonding is a critical process in semiconductor manufacturing. Understanding how process parameters affect bond strength is essential for reliable electronic devices.\nProcess Variables:\n\nPull Strength (Y): Response variable (quality measure)\nWire Length (X1): Process parameter\nDie Height (X2): Design parameter\n\n\n\n\nTable 5: Wire Bond Data\n\n\n\n\n\nObs No\nPull Strength (Y)\nWire Length (X1)\nDie Height (X2)\n\n\n\n\n1\n9.95\n2\n50\n\n\n2\n24.45\n8\n110\n\n\n3\n31.75\n11\n120\n\n\n4\n35.00\n10\n550\n\n\n5\n25.02\n8\n295\n\n\n6\n16.86\n4\n200\n\n\n7\n14.38\n2\n375\n\n\n8\n9.60\n2\n52\n\n\n9\n24.35\n9\n100\n\n\n10\n27.50\n8\n300\n\n\n11\n17.08\n4\n412\n\n\n12\n37.00\n11\n400\n\n\n13\n41.95\n12\n500\n\n\n14\n11.66\n2\n360\n\n\n15\n21.65\n4\n205\n\n\n16\n17.89\n4\n400\n\n\n17\n69.00\n20\n600\n\n\n18\n10.30\n1\n585\n\n\n19\n34.93\n10\n540\n\n\n20\n46.59\n15\n250\n\n\n21\n44.88\n15\n290\n\n\n22\n54.12\n16\n510\n\n\n23\n56.63\n17\n590\n\n\n24\n22.13\n6\n100\n\n\n25\n21.15\n5\n400\n\n\n\n\n\n\nEngineering Questions:\n\nWhich parameter most strongly affects bond strength?\nAre the parameters independent of each other?\nCan we predict strength from parameter settings?\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Scatter plot for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(b) Scatter plot for Pull strength versus die height\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Scatter and box plots for Pull strength versus length\n\n\n\n\n\n\n\n\n\n\n\n(d) Scatter and box plots for Pull strength versus die height\n\n\n\n\n\n\n\nFigure 13: Scatter diagrams and box plots for the wire bond pull strength data\n\n\n\n\n\n\n# Wire bond data analysis\nwire_data &lt;- fread(\"./data/Table2.9.csv\")\n\n# Scatter plot: Pull strength vs Wire length\np7 &lt;-\n  ggplot(wire_data, aes(x = X1, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(x = \"Wire Length\", y = \"Pull Strength\", title = \"Pull Strength vs Wire Length\") +\n  theme_classic()\n\n# Scatter plot: Pull strength vs Die height\np8 &lt;-\n  ggplot(wire_data, aes(x = X2, y = Y)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  labs(x = \"Die Height\", y = \"Pull Strength\", title = \"Pull Strength vs Die Height\") +\n  theme_classic()\n\nprint(p7)\nprint(p8)\n\nlibrary(car)\nscatterplot(\n  formula = Y ~ X1,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Length\",\n  ylab = \"Strength\"\n)\n\nscatterplot(\n  formula = Y ~ X2,\n  data = wire_data,\n  smooth = FALSE,\n  regLine = FALSE,\n  xlab = \"Die height\",\n  ylab = \"Strength\"\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Correlation Coefficient\n\n\n\nEngineering Definition: The correlation coefficient quantifies the strength and direction of linear relationships between variables, crucial for understanding process interactions.\nGiven n pairs of data \\left(X_{1},Y_{1}\\right),\\left(X_{2},Y_{2}\\right),\\ldots,\\left(X_{n},Y_{n}\\right):\n\nr=\\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(Y_{i}-\\overline{Y}\\right)}{\\sqrt{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}\\sum_{i=1}^{n}\\left(Y_{i}-\\overline{Y}\\right)^{2}}}\n\nwhere -1\\leq r\\leq+1.\nEngineering Interpretation:\n\nr ≈ +1: Strong positive relationship (as X increases, Y increases)\nr ≈ -1: Strong negative relationship (as X increases, Y decreases)\nr ≈ 0: Weak linear relationship (may still have nonlinear relationship)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) r is near +1\n\n\n\n\n\n\n\n\n\n\n\n(b) r is near -1\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) r is near 0, Y and X are unrelated\n\n\n\n\n\n\n\n\n\n\n\n(d) r is near 0, Y and X are nonlinearly related\n\n\n\n\n\n\n\nFigure 14: Scatter diagrams for different values of the sample correlation coefficient r.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Shampoo Quality Analysis\n\n\n\nEngineering Context: This multivariate analysis demonstrates how engineers evaluate product quality across multiple sensory and performance characteristics. Understanding these relationships helps optimize formulations.\nQuality Variables:\n\nFoam: Lathering performance\nScent: Fragrance intensity\nColor: Visual appeal\nResidue: Cleansing effectiveness\nRegion: Market segment\nQuality: Overall consumer rating\n\n\n\n\nTable 6: Data on Shampoo\n\n\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\n6.3\n5.3\n4.8\n3.1\n1\n91\n\n\n4.4\n4.9\n3.5\n3.9\n1\n87\n\n\n3.9\n5.3\n4.8\n4.7\n1\n82\n\n\n5.1\n4.2\n3.1\n3.6\n1\n83\n\n\n5.6\n5.1\n5.5\n5.1\n1\n83\n\n\n4.6\n4.7\n5.1\n4.1\n1\n84\n\n\n4.8\n4.8\n4.8\n3.3\n1\n90\n\n\n6.5\n4.5\n4.3\n5.2\n1\n84\n\n\n8.7\n4.3\n3.9\n2.9\n1\n97\n\n\n8.3\n3.9\n4.7\n3.9\n1\n93\n\n\n5.1\n4.3\n4.5\n3.6\n1\n82\n\n\n3.3\n5.4\n4.3\n3.6\n1\n84\n\n\n5.9\n5.7\n7.2\n4.1\n2\n87\n\n\n7.7\n6.6\n6.7\n5.6\n2\n80\n\n\n7.1\n4.4\n5.8\n4.1\n2\n84\n\n\n5.5\n5.6\n5.6\n4.4\n2\n84\n\n\n6.3\n5.4\n4.8\n4.6\n2\n82\n\n\n4.3\n5.5\n5.5\n4.1\n2\n79\n\n\n4.6\n4.1\n4.3\n3.1\n2\n81\n\n\n3.4\n5.0\n3.4\n3.4\n2\n83\n\n\n6.4\n5.4\n6.6\n4.8\n2\n81\n\n\n5.5\n5.3\n5.3\n3.8\n2\n84\n\n\n4.7\n4.1\n5.0\n3.7\n2\n83\n\n\n4.1\n4.0\n4.1\n4.0\n2\n80\n\n\n\n\n\n\nEngineering Analysis Goals:\n\nIdentify which characteristics most influence overall quality\nUnderstand relationships between formulation parameters\nOptimize product design for target markets\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\nTable 7: Correlation Matrix\n\n\n\n\nShampoo Data Correlation Matrix\n\n\n\nFoam\nScent\nColor\nResidue\nRegion\nQuality\n\n\n\n\nFoam\n1.000\n0.002\n0.328\n0.193\n-0.032\n0.512\n\n\nScent\n0.002\n1.000\n0.599\n0.500\n0.278\n-0.252\n\n\nColor\n0.328\n0.599\n1.000\n0.524\n0.458\n-0.194\n\n\nResidue\n0.193\n0.500\n0.524\n1.000\n0.165\n-0.489\n\n\nRegion\n-0.032\n0.278\n0.458\n0.165\n1.000\n-0.507\n\n\nQuality\n0.512\n-0.252\n-0.194\n-0.489\n-0.507\n1.000\n\n\n\n\n\n\nShampoo Data Summary Statistics\n\n\nFoam_mean\nFoam_sd\nFoam_min\nFoam_max\nScent_mean\nScent_sd\nScent_min\nScent_max\nColor_mean\nColor_sd\nColor_min\nColor_max\nResidue_mean\nResidue_sd\nResidue_min\nResidue_max\nRegion_mean\nRegion_sd\nRegion_min\nRegion_max\nQuality_mean\nQuality_sd\nQuality_min\nQuality_max\n\n\n\n\n5.5\n1.45\n3.3\n8.7\n4.91\n0.67\n3.9\n6.6\n4.9\n1.02\n3.1\n7.2\n4.03\n0.7\n2.9\n5.6\n1.5\n0.51\n1\n2\n84.5\n4.36\n79\n97\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 8\n\n\n# Shampoo data correlation matrix\nshampoo_data &lt;- fread(\"./data/Table2.11.csv\")\n\ncor_matrix &lt;- cor(shampoo_data)\ncor_matrix %&gt;%\n  round(3) %&gt;%\n  kbl(caption = \"Shampoo Data Correlation Matrix\") %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\nshampoo_summary &lt;-\n  shampoo_data %&gt;%\n  fsummarise(\n    across(\n      .cols = is.numeric,\n      .fns = list(\n        mean = fmean,\n        sd = fsd,\n        min = fmin,\n        max = fmax\n      )\n    )\n  )\n\nshampoo_summary %&gt;%\n  kbl(caption = \"Shampoo Data Summary Statistics\", digits = 2) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n\n\n\n\n\n\n\nR OutputR CodeDownload Data\n\n\n\n\n\n\n\n\n\n\nFigure 15: Matrix of scatter plots for the shampoo data\n\n\n\n\n\n\n\n\n# Matrix of scatter plots for shampoo data\nggpairs(shampoo_data,\n  title = \"Shampoo Quality Data: Pairwise Relationships\",\n  upper = list(continuous = wrap(\"cor\", size = 3))\n) +\n  theme_classic()",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch02.html#key-engineering-takeaways",
    "href": "book/Ch02.html#key-engineering-takeaways",
    "title": "1 Data Summary and Presentation",
    "section": "",
    "text": "This chapter provided essential tools for engineering data analysis:\n\n\n\nMeasures of Central Tendency: Sample mean as balance point for process centering\nMeasures of Variability: Standard deviation for process control and capability analysis\nDistribution Analysis: Understanding natural process variation patterns\n\n\n\n\n\nHistograms: Assess process capability and distribution normality\nBox Plots: Identify outliers and compare process alternatives\nStem-and-Leaf: Quick distribution assessment preserving actual values\nPareto Charts: Prioritize improvement efforts using 80-20 principle\n\n\n\n\n\nTime Series: Monitor trends, detect shifts, identify cycles\nCorrelation Analysis: Understand variable relationships for optimization\nMultivariate Methods: Analyze complex engineering systems\n\n\n\n\n\nQuality control and process improvement\nDesign optimization and parameter selection\nRoot cause analysis and problem solving\nSpecification setting and capability assessment\n\nThese statistical tools form the foundation for evidence-based engineering decisions and continuous improvement initiatives.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Data Summary and Presentation"
    ]
  },
  {
    "objectID": "book/Ch061.html",
    "href": "book/Ch061.html",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "TipMajor Themes of Chapter 6\n\n\n\n\nSimple Linear Regression: Modeling linear relationships between two variables\nLeast Squares Estimation: Finding best-fitting lines through data\nStatistical Inference: Testing significance and constructing intervals\nModel Adequacy: Checking assumptions and validating models\nCorrelation Analysis: Measuring strength of linear relationships\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nFit simple linear regression models using least squares estimation.\nConduct hypothesis tests for regression parameters.\nConstruct confidence intervals and prediction intervals.\nCheck model adequacy using residual analysis.\nCalculate and interpret correlation coefficients.\nApply regression and correlation in engineering contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSimple Linear Regression Model\n\n\n\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\nwhere: - y_i = response variable - x_i = predictor variable\n- \\beta_0 = intercept parameter - \\beta_1 = slope parameter - \\epsilon_i \\sim N(0, \\sigma^2) = random error\nAssumptions: - Linear relationship - Independent observations - Constant variance - Normal errors\n\n\n\n\n\n\n\n\n\n\n\nNoteParameter Estimates\n\n\n\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{aligned}\nFitted Model: \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\nResiduals: e_i = y_i - \\hat{y}_i\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.1: Steel Hardness vs Tensile Strength\n\n\n\n\nAnalysisR Code\n\n\n\n\n[1] \"Steel Strength vs Hardness Data Summary:\"\n\n\n       n Hardness_Mean Hardness_SD Hardness_Min Hardness_Max Strength_Mean\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:    20      62.03234    12.53884     41.68238     78.27333      169.2171\n   Strength_SD Strength_Min Strength_Max\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:     29.0103     120.2922     220.4809\n\n\n[1] \"First 10 observations:\"\n\n\n    specimen hardness strength\n       &lt;int&gt;    &lt;num&gt;    &lt;num&gt;\n 1:        1 51.50310 153.5504\n 2:        2 71.53221 196.7090\n 3:        3 56.35908 159.1039\n 4:        4 75.32070 204.1872\n 5:        5 77.61869 204.6000\n 6:        6 41.82226 133.8510\n 7:        7 61.12422 171.7934\n 8:        8 75.69676 188.5090\n 9:        9 62.05740 175.7543\n10:       10 58.26459 156.8791\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Steel strength vs hardness data (simple linear regression)\nset.seed(123)\nsteel_data &lt;- data.table(\n  specimen = 1:20,\n  hardness = runif(20, min = 40, max = 80),\n  strength = NULL\n)\n\n# Create realistic relationship with some noise\nsteel_data[, strength := 15 + 2.5 * hardness + rnorm(20, mean = 0, sd = 8)]\n\n# Summary statistics\nsteel_summary &lt;- steel_data %&gt;%\n  fsummarise(\n    n = fnobs(hardness),\n    Hardness_Mean = fmean(hardness),\n    Hardness_SD = fsd(hardness),\n    Hardness_Min = fmin(hardness),\n    Hardness_Max = fmax(hardness),\n    Strength_Mean = fmean(strength),\n    Strength_SD = fsd(strength),\n    Strength_Min = fmin(strength),\n    Strength_Max = fmax(strength)\n  )\n\nprint(\"Steel Strength vs Hardness Data Summary:\")\nprint(steel_summary)\n\n# Detailed data view\nprint(\"First 10 observations:\")\nprint(head(steel_data, 10))\n\n# Create data directory if needed\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\n# Write data to CSV\nfwrite(steel_data, \"data/steel_strength.csv\")\n\n\n\n\n\nVisualizationR Code\n\n\n\n\nError: object 'beta0_hat' not found\n\n\nError: object 'beta0_hat' not found\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'residual_data' not found\n\n\nError: object 'p1' not found\n\n\n\n\n\n# Simple linear regression visualization\np1 &lt;- ggplot(data = steel_data, mapping = aes(x = hardness, y = strength)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Simple Linear Regression: Tensile Strength vs Hardness\",\n    subtitle = paste(\"ŷ =\", round(beta0_hat, 2), \"+\", round(beta1_hat, 2), \"x, R² =\", round(R_squared, 3))\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n# Residuals vs fitted\nfitted_values &lt;- beta0_hat + beta1_hat * steel_data$hardness\nresiduals &lt;- steel_data$strength - fitted_values\n\nresidual_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  hardness = steel_data$hardness\n)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTests for Regression Parameters\n\n\n\nTest for Slope: H_0: \\beta_1 = 0 \\text{ vs } H_1: \\beta_1 \\neq 0\nTest Statistic: t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} \\sim t_{n-2}\nANOVA F-Test: F = \\frac{MS_R}{MS_E} \\sim F_{1,n-2}\n\n\n\n\n\n\n\n\n\n\n\nNoteInterval Estimation\n\n\n\nConfidence Interval for β₁: \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot SE(\\hat{\\beta}_1)\nPrediction Interval at x₀: \\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nCoefficient of Determination: R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_E}{SS_T}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.2: Steel Model Inference\n\n\n\n\nStatistical TestsR Code\n\n\n\n\n[1] \"Manual Simple Linear Regression Results:\"\n\n\n        Parameter Manual_Estimate                               Interpretation\n           &lt;char&gt;           &lt;num&gt;                                       &lt;char&gt;\n1: β₀ (Intercept)         30.4493            Strength when hardness = 0: 30.45\n2:     β₁ (Slope)          2.2370 Strength increases by 2.24 per unit hardness\n3:             σ²         57.8584                        Error variance: 57.86\n4:             R²          0.9349                93.5 % of variation explained\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = strength ~ hardness, data = steel_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2758  -5.0561  -0.6948   5.4929  15.1408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.4493     8.7991   3.461  0.00279 ** \nhardness      2.2370     0.1392  16.074 4.03e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.606 on 18 degrees of freedom\nMultiple R-squared:  0.9349,    Adjusted R-squared:  0.9313 \nF-statistic: 258.4 on 1 and 18 DF,  p-value: 4.031e-12\n\n\n[1] \"Manual vs R Function Comparison:\"\n\n\n     Parameter    Manual R_Function   Difference\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   Intercept 30.449343  30.449343 7.815970e-14\n2:       Slope  2.237023   2.237023 8.881784e-16\n3:   R-squared  0.934870   0.934870 0.000000e+00\n4: Residual SE  7.606471   7.606471 2.664535e-15\n\n\n\n\n\n# Simple linear regression - manual calculations\nn &lt;- fnobs(steel_data$hardness)\nx_bar &lt;- fmean(steel_data$hardness)\ny_bar &lt;- fmean(steel_data$strength)\n\n# Calculate sums of squares and cross-products\nx &lt;- steel_data$hardness\ny &lt;- steel_data$strength\n\n# Manual calculation of sums\nSxx &lt;- sum((x - x_bar)^2)\nSyy &lt;- sum((y - y_bar)^2)\nSxy &lt;- sum((x - x_bar) * (y - y_bar))\n\n# Parameter estimates\nbeta1_hat &lt;- Sxy / Sxx\nbeta0_hat &lt;- y_bar - beta1_hat * x_bar\n\n# Sum of squares\nSSR &lt;- beta1_hat * Sxy # Regression sum of squares\nSSE &lt;- Syy - beta1_hat * Sxy # Error sum of squares\nSST &lt;- Syy # Total sum of squares\n\n# Mean squares\nMSR &lt;- SSR / 1 # df = 1 for simple regression\nMSE &lt;- SSE / (n - 2) # df = n - 2\ns_squared &lt;- MSE\n\n# R-squared\nR_squared &lt;- SSR / SST\n\n# Manual calculations summary\nmanual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Slope)\", \"σ²\", \"R²\"),\n  Manual_Estimate = c(\n    round(beta0_hat, 4),\n    round(beta1_hat, 4),\n    round(s_squared, 4),\n    round(R_squared, 4)\n  ),\n  Interpretation = c(\n    paste(\"Strength when hardness = 0:\", round(beta0_hat, 2)),\n    paste(\"Strength increases by\", round(beta1_hat, 2), \"per unit hardness\"),\n    paste(\"Error variance:\", round(s_squared, 2)),\n    paste(round(R_squared * 100, 1), \"% of variation explained\")\n  )\n)\n\nprint(\"Manual Simple Linear Regression Results:\")\nprint(manual_results)\n\n# Verification with R's lm function\nlm_model &lt;- lm(strength ~ hardness, data = steel_data)\nlm_summary &lt;- summary(lm_model)\n\nprint(\"R's lm() verification:\")\nprint(lm_summary)\n\n# Compare manual vs R calculations\ncomparison &lt;- data.table(\n  Parameter = c(\"Intercept\", \"Slope\", \"R-squared\", \"Residual SE\"),\n  Manual = c(beta0_hat, beta1_hat, R_squared, sqrt(MSE)),\n  R_Function = c(\n    coef(lm_model)[1], coef(lm_model)[2],\n    summary(lm_model)$r.squared, summary(lm_model)$sigma\n  ),\n  Difference = c(\n    abs(beta0_hat - coef(lm_model)[1]),\n    abs(beta1_hat - coef(lm_model)[2]),\n    abs(R_squared - summary(lm_model)$r.squared),\n    abs(sqrt(MSE) - summary(lm_model)$sigma)\n  )\n)\n\nprint(\"Manual vs R Function Comparison:\")\nprint(comparison)\n\n\n\n\n\nIntervalsR Code\n\n\n\n\n[1] \"Hypothesis Test Results:\"\n\n\n                   Test Null_Hypothesis Test_Statistic P_Value Critical_Value\n                 &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;          &lt;num&gt;\n1:         Slope β₁ = 0          β₁ = 0        16.0739  0.0000          2.101\n2:     Intercept β₀ = 0          β₀ = 0         3.4605  0.0028          2.101\n3: Overall Model F-test          β₁ = 0       258.3704  0.0000          4.414\n    Decision                               Conclusion\n      &lt;char&gt;                                   &lt;char&gt;\n1: Reject H₀          Significant linear relationship\n2: Reject H₀ Intercept significantly different from 0\n3: Reject H₀                     Model is significant\n\n\n[1] \"ANOVA Table:\"\n\n\n       Source       SS    df       MS F_Statistic P_Value\n       &lt;char&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 14948.90     1 14948.90    258.3704       0\n2:      Error  1041.45    18    57.86          NA      NA\n3:      Total 15990.35    19       NA          NA      NA\n\n\n\n\n\n# Hypothesis testing for regression parameters\nalpha &lt;- 0.05\ndf_error &lt;- n - 2\nt_critical &lt;- qt(1 - alpha / 2, df_error)\n\n# Standard errors\nSE_beta1 &lt;- sqrt(MSE / Sxx)\nSE_beta0 &lt;- sqrt(MSE * (1 / n + x_bar^2 / Sxx))\n\n# Test for slope (β₁ = 0)\nt_stat_beta1 &lt;- beta1_hat / SE_beta1\np_value_beta1 &lt;- 2 * (1 - pt(abs(t_stat_beta1), df_error))\n\n# Test for intercept (β₀ = 0)\nt_stat_beta0 &lt;- beta0_hat / SE_beta0\np_value_beta0 &lt;- 2 * (1 - pt(abs(t_stat_beta0), df_error))\n\n# F-test for overall significance\nF_stat &lt;- MSR / MSE\np_value_F &lt;- 1 - pf(F_stat, 1, df_error)\n\n# Hypothesis test results\nhypothesis_tests &lt;- data.table(\n  Test = c(\"Slope β₁ = 0\", \"Intercept β₀ = 0\", \"Overall Model F-test\"),\n  Null_Hypothesis = c(\"β₁ = 0\", \"β₀ = 0\", \"β₁ = 0\"),\n  Test_Statistic = c(round(t_stat_beta1, 4), round(t_stat_beta0, 4), round(F_stat, 4)),\n  P_Value = c(round(p_value_beta1, 4), round(p_value_beta0, 4), round(p_value_F, 4)),\n  Critical_Value = c(\n    round(t_critical, 3), round(t_critical, 3),\n    round(qf(1 - alpha, 1, df_error), 3)\n  ),\n  Decision = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_F &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Conclusion = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Significant linear relationship\", \"No significant relationship\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Intercept significantly different from 0\", \"Intercept not significant\"),\n    ifelse(p_value_F &lt; alpha, \"Model is significant\", \"Model is not significant\")\n  )\n)\n\nprint(\"Hypothesis Test Results:\")\nprint(hypothesis_tests)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR, 2), round(SSE, 2), round(SST, 2)),\n  df = c(1, df_error, n - 1),\n  MS = c(round(MSR, 2), round(MSE, 2), NA),\n  F_Statistic = c(round(F_stat, 4), NA, NA),\n  P_Value = c(round(p_value_F, 4), NA, NA)\n)\n\nprint(\"ANOVA Table:\")\nprint(anova_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDiagnostic Procedures\n\n\n\nKey Plots: - Residuals vs Fitted Values - Normal Q-Q Plot\n- Scale-Location Plot - Residuals vs Order\nTests: - Shapiro-Wilk (normality) - Durbin-Watson (independence)\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.3: Residual Analysis\n\n\n\n\nDiagnosticsR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Mean Response:\"\n\n\n   Hardness Predicted CI_Lower CI_Upper CI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   116.75   130.63    13.88\n2: 50.83012    144.16   139.31   149.00     9.69\n3: 59.97786    164.62   161.00   168.24     7.25\n4: 69.12560    185.08   180.95   189.22     8.26\n5: 78.27333    205.55   199.61   211.49    11.89\n\n\n[1] \"95% Prediction Intervals for Individual Observations:\"\n\n\n   Hardness Predicted PI_Lower PI_Upper PI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   106.27   141.12    34.85\n2: 50.83012    144.16   127.46   160.86    33.40\n3: 59.97786    164.62   148.23   181.01    32.77\n4: 69.12560    185.08   168.58   201.59    33.01\n5: 78.27333    205.55   188.50   222.60    34.10\n\n\n[1] \"Interval Width Comparison:\"\n\n\n   Hardness CI_Width PI_Width PI_vs_CI_Ratio\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1: 41.68238    13.88    34.85           2.51\n2: 50.83012     9.69    33.40           3.45\n3: 59.97786     7.25    32.77           4.52\n4: 69.12560     8.26    33.01           4.00\n5: 78.27333    11.89    34.10           2.87\n\n\n\n\n\n# Confidence and prediction intervals\n# Choose some specific x values for demonstration\nx_new &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 5)\n\n# Confidence intervals for mean response\nCI_results &lt;- data.table()\nPI_results &lt;- data.table()\n\nfor (x0 in x_new) {\n  # Predicted value\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval for mean response\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_CI &lt;- t_critical * SE_mean\n  CI_lower &lt;- y_hat - margin_CI\n  CI_upper &lt;- y_hat + margin_CI\n\n  # Prediction interval for individual observation\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_PI &lt;- t_critical * SE_pred\n  PI_lower &lt;- y_hat - margin_PI\n  PI_upper &lt;- y_hat + margin_PI\n\n  CI_results &lt;- rbind(CI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    CI_Lower = round(CI_lower, 2),\n    CI_Upper = round(CI_upper, 2),\n    CI_Width = round(CI_upper - CI_lower, 2)\n  ))\n\n  PI_results &lt;- rbind(PI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    PI_Lower = round(PI_lower, 2),\n    PI_Upper = round(PI_upper, 2),\n    PI_Width = round(PI_upper - PI_lower, 2)\n  ))\n}\n\nprint(\"95% Confidence Intervals for Mean Response:\")\nprint(CI_results)\n\nprint(\"95% Prediction Intervals for Individual Observations:\")\nprint(PI_results)\n\n# Compare CI and PI widths\ninterval_comparison &lt;- data.table(\n  Hardness = x_new,\n  CI_Width = CI_results$CI_Width,\n  PI_Width = PI_results$PI_Width,\n  PI_vs_CI_Ratio = round(PI_results$PI_Width / CI_results$CI_Width, 2)\n)\n\nprint(\"Interval Width Comparison:\")\nprint(interval_comparison)\n\n\n\n\n\nPlotsR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Model diagnostic plots\n\n\n\n\n\n\n\n\n# Confidence and prediction intervals visualization\n# Create fine grid for smooth curves\nx_grid &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 100)\ninterval_data &lt;- data.table()\n\nfor (x0 in x_grid) {\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  CI_lower &lt;- y_hat - t_critical * SE_mean\n  CI_upper &lt;- y_hat + t_critical * SE_mean\n\n  # Prediction interval\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  PI_lower &lt;- y_hat - t_critical * SE_pred\n  PI_upper &lt;- y_hat + t_critical * SE_pred\n\n  interval_data &lt;- rbind(interval_data, data.table(\n    hardness = x0,\n    fitted = y_hat,\n    CI_lower = CI_lower,\n    CI_upper = CI_upper,\n    PI_lower = PI_lower,\n    PI_upper = PI_upper\n  ))\n}\n\nggplot() +\n  # Prediction intervals (wider, lighter)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = PI_lower, ymax = PI_upper),\n    alpha = 0.2, fill = \"red\"\n  ) +\n  # Confidence intervals (narrower, darker)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = CI_lower, ymax = CI_upper),\n    alpha = 0.3, fill = \"blue\"\n  ) +\n  # Regression line\n  geom_line(data = interval_data, aes(x = hardness, y = fitted), color = \"black\", linewidth = 1) +\n  # Data points\n  geom_point(data = steel_data, aes(x = hardness, y = strength), size = 3, alpha = 0.7) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Confidence and Prediction Intervals\",\n    subtitle = \"Blue = 95% CI for mean response, Red = 95% PI for individual observations\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Correlation\n\n\n\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\nProperties: - -1 \\leq r \\leq 1 - r^2 = R^2 (simple regression) - Scale invariant\nTest for Significance: t = \\frac{r}{\\sqrt{\\frac{1-r^2}{n-2}}} \\sim t_{n-2}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.4: Steel Data Correlation\n\n\n\n\nCorrelationR Code\n\n\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'residuals_data' not found\n\n\n[1] \"Residual Analysis Summary:\"\n\n\nError: object 'residual_summary' not found\n\n\nError: object 'residuals_data' not found\n\n\nError: object 'outliers' not found\n\n\nError in shapiro.test(residuals): is.numeric(x) is not TRUE\n\n\n[1] \"Shapiro-Wilk normality test for residuals:\"\n\n\nError: object 'shapiro_test' not found\n\n\nError in x[0L]: object of type 'closure' is not subsettable\n\n\nError: object 'dw_stat' not found\n\n\n[1] \"(Values near 2 indicate no autocorrelation)\"\n\n\n\n\n\n# Model adequacy checking - residual analysis\n# Calculate residuals and standardized residuals\nresiduals_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  standardized_residuals = residuals / sqrt(MSE),\n  observation = 1:n,\n  hardness = steel_data$hardness\n)\n\n# Summary statistics for residuals\nresidual_summary &lt;- residuals_data %&gt;%\n  fsummarise(\n    Mean_Residuals = fmean(residuals),\n    SD_Residuals = fsd(residuals),\n    Min_Residuals = fmin(residuals),\n    Max_Residuals = fmax(residuals),\n    Mean_Std_Residuals = fmean(standardized_residuals),\n    SD_Std_Residuals = fsd(standardized_residuals)\n  )\n\nprint(\"Residual Analysis Summary:\")\nprint(residual_summary)\n\n# Check for outliers (|standardized residual| &gt; 2)\noutliers &lt;- residuals_data[abs(standardized_residuals) &gt; 2]\nif (nrow(outliers) &gt; 0) {\n  print(\"Potential outliers (|std residual| &gt; 2):\")\n  print(outliers)\n} else {\n  print(\"No obvious outliers detected (|std residual| &gt; 2)\")\n}\n\n# Normality test for residuals\nshapiro_test &lt;- shapiro.test(residuals)\nprint(\"Shapiro-Wilk normality test for residuals:\")\nprint(shapiro_test)\n\n# Durbin-Watson test for independence (if data has natural order)\ndw_stat &lt;- sum(diff(residuals)^2) / sum(residuals^2)\nprint(paste(\"Durbin-Watson statistic:\", round(dw_stat, 4)))\nprint(\"(Values near 2 indicate no autocorrelation)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Engineering Applications\n\n\n\nSimple Linear Regression: - Process optimization - Quality control - Predictive modeling\nCorrelation Analysis:\n- Variable screening - Relationship assessment - Data exploration\nModel Validation: - Assumption checking - Outlier detection - Prediction accuracy"
  },
  {
    "objectID": "book/Ch061.html#simple-linear-regression",
    "href": "book/Ch061.html#simple-linear-regression",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteSimple Linear Regression Model\n\n\n\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\nwhere: - y_i = response variable - x_i = predictor variable\n- \\beta_0 = intercept parameter - \\beta_1 = slope parameter - \\epsilon_i \\sim N(0, \\sigma^2) = random error\nAssumptions: - Linear relationship - Independent observations - Constant variance - Normal errors\n\n\n\n\n\n\n\n\n\n\n\nNoteParameter Estimates\n\n\n\n\\begin{aligned}\n\\hat{\\beta}_1 &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x}\n\\end{aligned}\nFitted Model: \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\nResiduals: e_i = y_i - \\hat{y}_i\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.1: Steel Hardness vs Tensile Strength\n\n\n\n\nAnalysisR Code\n\n\n\n\n[1] \"Steel Strength vs Hardness Data Summary:\"\n\n\n       n Hardness_Mean Hardness_SD Hardness_Min Hardness_Max Strength_Mean\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:    20      62.03234    12.53884     41.68238     78.27333      169.2171\n   Strength_SD Strength_Min Strength_Max\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:     29.0103     120.2922     220.4809\n\n\n[1] \"First 10 observations:\"\n\n\n    specimen hardness strength\n       &lt;int&gt;    &lt;num&gt;    &lt;num&gt;\n 1:        1 51.50310 153.5504\n 2:        2 71.53221 196.7090\n 3:        3 56.35908 159.1039\n 4:        4 75.32070 204.1872\n 5:        5 77.61869 204.6000\n 6:        6 41.82226 133.8510\n 7:        7 61.12422 171.7934\n 8:        8 75.69676 188.5090\n 9:        9 62.05740 175.7543\n10:       10 58.26459 156.8791\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Steel strength vs hardness data (simple linear regression)\nset.seed(123)\nsteel_data &lt;- data.table(\n  specimen = 1:20,\n  hardness = runif(20, min = 40, max = 80),\n  strength = NULL\n)\n\n# Create realistic relationship with some noise\nsteel_data[, strength := 15 + 2.5 * hardness + rnorm(20, mean = 0, sd = 8)]\n\n# Summary statistics\nsteel_summary &lt;- steel_data %&gt;%\n  fsummarise(\n    n = fnobs(hardness),\n    Hardness_Mean = fmean(hardness),\n    Hardness_SD = fsd(hardness),\n    Hardness_Min = fmin(hardness),\n    Hardness_Max = fmax(hardness),\n    Strength_Mean = fmean(strength),\n    Strength_SD = fsd(strength),\n    Strength_Min = fmin(strength),\n    Strength_Max = fmax(strength)\n  )\n\nprint(\"Steel Strength vs Hardness Data Summary:\")\nprint(steel_summary)\n\n# Detailed data view\nprint(\"First 10 observations:\")\nprint(head(steel_data, 10))\n\n# Create data directory if needed\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\n# Write data to CSV\nfwrite(steel_data, \"data/steel_strength.csv\")\n\n\n\n\n\nVisualizationR Code\n\n\n\n\nError: object 'beta0_hat' not found\n\n\nError: object 'beta0_hat' not found\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'residual_data' not found\n\n\nError: object 'p1' not found\n\n\n\n\n\n# Simple linear regression visualization\np1 &lt;- ggplot(data = steel_data, mapping = aes(x = hardness, y = strength)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Simple Linear Regression: Tensile Strength vs Hardness\",\n    subtitle = paste(\"ŷ =\", round(beta0_hat, 2), \"+\", round(beta1_hat, 2), \"x, R² =\", round(R_squared, 3))\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n# Residuals vs fitted\nfitted_values &lt;- beta0_hat + beta1_hat * steel_data$hardness\nresiduals &lt;- steel_data$strength - fitted_values\n\nresidual_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  hardness = steel_data$hardness\n)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)"
  },
  {
    "objectID": "book/Ch061.html#statistical-inference",
    "href": "book/Ch061.html#statistical-inference",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteTests for Regression Parameters\n\n\n\nTest for Slope: H_0: \\beta_1 = 0 \\text{ vs } H_1: \\beta_1 \\neq 0\nTest Statistic: t = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} \\sim t_{n-2}\nANOVA F-Test: F = \\frac{MS_R}{MS_E} \\sim F_{1,n-2}\n\n\n\n\n\n\n\n\n\n\n\nNoteInterval Estimation\n\n\n\nConfidence Interval for β₁: \\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\cdot SE(\\hat{\\beta}_1)\nPrediction Interval at x₀: \\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nCoefficient of Determination: R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_E}{SS_T}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.2: Steel Model Inference\n\n\n\n\nStatistical TestsR Code\n\n\n\n\n[1] \"Manual Simple Linear Regression Results:\"\n\n\n        Parameter Manual_Estimate                               Interpretation\n           &lt;char&gt;           &lt;num&gt;                                       &lt;char&gt;\n1: β₀ (Intercept)         30.4493            Strength when hardness = 0: 30.45\n2:     β₁ (Slope)          2.2370 Strength increases by 2.24 per unit hardness\n3:             σ²         57.8584                        Error variance: 57.86\n4:             R²          0.9349                93.5 % of variation explained\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = strength ~ hardness, data = steel_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2758  -5.0561  -0.6948   5.4929  15.1408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.4493     8.7991   3.461  0.00279 ** \nhardness      2.2370     0.1392  16.074 4.03e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.606 on 18 degrees of freedom\nMultiple R-squared:  0.9349,    Adjusted R-squared:  0.9313 \nF-statistic: 258.4 on 1 and 18 DF,  p-value: 4.031e-12\n\n\n[1] \"Manual vs R Function Comparison:\"\n\n\n     Parameter    Manual R_Function   Difference\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   Intercept 30.449343  30.449343 7.815970e-14\n2:       Slope  2.237023   2.237023 8.881784e-16\n3:   R-squared  0.934870   0.934870 0.000000e+00\n4: Residual SE  7.606471   7.606471 2.664535e-15\n\n\n\n\n\n# Simple linear regression - manual calculations\nn &lt;- fnobs(steel_data$hardness)\nx_bar &lt;- fmean(steel_data$hardness)\ny_bar &lt;- fmean(steel_data$strength)\n\n# Calculate sums of squares and cross-products\nx &lt;- steel_data$hardness\ny &lt;- steel_data$strength\n\n# Manual calculation of sums\nSxx &lt;- sum((x - x_bar)^2)\nSyy &lt;- sum((y - y_bar)^2)\nSxy &lt;- sum((x - x_bar) * (y - y_bar))\n\n# Parameter estimates\nbeta1_hat &lt;- Sxy / Sxx\nbeta0_hat &lt;- y_bar - beta1_hat * x_bar\n\n# Sum of squares\nSSR &lt;- beta1_hat * Sxy # Regression sum of squares\nSSE &lt;- Syy - beta1_hat * Sxy # Error sum of squares\nSST &lt;- Syy # Total sum of squares\n\n# Mean squares\nMSR &lt;- SSR / 1 # df = 1 for simple regression\nMSE &lt;- SSE / (n - 2) # df = n - 2\ns_squared &lt;- MSE\n\n# R-squared\nR_squared &lt;- SSR / SST\n\n# Manual calculations summary\nmanual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Slope)\", \"σ²\", \"R²\"),\n  Manual_Estimate = c(\n    round(beta0_hat, 4),\n    round(beta1_hat, 4),\n    round(s_squared, 4),\n    round(R_squared, 4)\n  ),\n  Interpretation = c(\n    paste(\"Strength when hardness = 0:\", round(beta0_hat, 2)),\n    paste(\"Strength increases by\", round(beta1_hat, 2), \"per unit hardness\"),\n    paste(\"Error variance:\", round(s_squared, 2)),\n    paste(round(R_squared * 100, 1), \"% of variation explained\")\n  )\n)\n\nprint(\"Manual Simple Linear Regression Results:\")\nprint(manual_results)\n\n# Verification with R's lm function\nlm_model &lt;- lm(strength ~ hardness, data = steel_data)\nlm_summary &lt;- summary(lm_model)\n\nprint(\"R's lm() verification:\")\nprint(lm_summary)\n\n# Compare manual vs R calculations\ncomparison &lt;- data.table(\n  Parameter = c(\"Intercept\", \"Slope\", \"R-squared\", \"Residual SE\"),\n  Manual = c(beta0_hat, beta1_hat, R_squared, sqrt(MSE)),\n  R_Function = c(\n    coef(lm_model)[1], coef(lm_model)[2],\n    summary(lm_model)$r.squared, summary(lm_model)$sigma\n  ),\n  Difference = c(\n    abs(beta0_hat - coef(lm_model)[1]),\n    abs(beta1_hat - coef(lm_model)[2]),\n    abs(R_squared - summary(lm_model)$r.squared),\n    abs(sqrt(MSE) - summary(lm_model)$sigma)\n  )\n)\n\nprint(\"Manual vs R Function Comparison:\")\nprint(comparison)\n\n\n\n\n\nIntervalsR Code\n\n\n\n\n[1] \"Hypothesis Test Results:\"\n\n\n                   Test Null_Hypothesis Test_Statistic P_Value Critical_Value\n                 &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;          &lt;num&gt;\n1:         Slope β₁ = 0          β₁ = 0        16.0739  0.0000          2.101\n2:     Intercept β₀ = 0          β₀ = 0         3.4605  0.0028          2.101\n3: Overall Model F-test          β₁ = 0       258.3704  0.0000          4.414\n    Decision                               Conclusion\n      &lt;char&gt;                                   &lt;char&gt;\n1: Reject H₀          Significant linear relationship\n2: Reject H₀ Intercept significantly different from 0\n3: Reject H₀                     Model is significant\n\n\n[1] \"ANOVA Table:\"\n\n\n       Source       SS    df       MS F_Statistic P_Value\n       &lt;char&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 14948.90     1 14948.90    258.3704       0\n2:      Error  1041.45    18    57.86          NA      NA\n3:      Total 15990.35    19       NA          NA      NA\n\n\n\n\n\n# Hypothesis testing for regression parameters\nalpha &lt;- 0.05\ndf_error &lt;- n - 2\nt_critical &lt;- qt(1 - alpha / 2, df_error)\n\n# Standard errors\nSE_beta1 &lt;- sqrt(MSE / Sxx)\nSE_beta0 &lt;- sqrt(MSE * (1 / n + x_bar^2 / Sxx))\n\n# Test for slope (β₁ = 0)\nt_stat_beta1 &lt;- beta1_hat / SE_beta1\np_value_beta1 &lt;- 2 * (1 - pt(abs(t_stat_beta1), df_error))\n\n# Test for intercept (β₀ = 0)\nt_stat_beta0 &lt;- beta0_hat / SE_beta0\np_value_beta0 &lt;- 2 * (1 - pt(abs(t_stat_beta0), df_error))\n\n# F-test for overall significance\nF_stat &lt;- MSR / MSE\np_value_F &lt;- 1 - pf(F_stat, 1, df_error)\n\n# Hypothesis test results\nhypothesis_tests &lt;- data.table(\n  Test = c(\"Slope β₁ = 0\", \"Intercept β₀ = 0\", \"Overall Model F-test\"),\n  Null_Hypothesis = c(\"β₁ = 0\", \"β₀ = 0\", \"β₁ = 0\"),\n  Test_Statistic = c(round(t_stat_beta1, 4), round(t_stat_beta0, 4), round(F_stat, 4)),\n  P_Value = c(round(p_value_beta1, 4), round(p_value_beta0, 4), round(p_value_F, 4)),\n  Critical_Value = c(\n    round(t_critical, 3), round(t_critical, 3),\n    round(qf(1 - alpha, 1, df_error), 3)\n  ),\n  Decision = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_F &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Conclusion = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Significant linear relationship\", \"No significant relationship\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Intercept significantly different from 0\", \"Intercept not significant\"),\n    ifelse(p_value_F &lt; alpha, \"Model is significant\", \"Model is not significant\")\n  )\n)\n\nprint(\"Hypothesis Test Results:\")\nprint(hypothesis_tests)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR, 2), round(SSE, 2), round(SST, 2)),\n  df = c(1, df_error, n - 1),\n  MS = c(round(MSR, 2), round(MSE, 2), NA),\n  F_Statistic = c(round(F_stat, 4), NA, NA),\n  P_Value = c(round(p_value_F, 4), NA, NA)\n)\n\nprint(\"ANOVA Table:\")\nprint(anova_table)"
  },
  {
    "objectID": "book/Ch061.html#model-adequacy",
    "href": "book/Ch061.html#model-adequacy",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteDiagnostic Procedures\n\n\n\nKey Plots: - Residuals vs Fitted Values - Normal Q-Q Plot\n- Scale-Location Plot - Residuals vs Order\nTests: - Shapiro-Wilk (normality) - Durbin-Watson (independence)\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.3: Residual Analysis\n\n\n\n\nDiagnosticsR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Mean Response:\"\n\n\n   Hardness Predicted CI_Lower CI_Upper CI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   116.75   130.63    13.88\n2: 50.83012    144.16   139.31   149.00     9.69\n3: 59.97786    164.62   161.00   168.24     7.25\n4: 69.12560    185.08   180.95   189.22     8.26\n5: 78.27333    205.55   199.61   211.49    11.89\n\n\n[1] \"95% Prediction Intervals for Individual Observations:\"\n\n\n   Hardness Predicted PI_Lower PI_Upper PI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   106.27   141.12    34.85\n2: 50.83012    144.16   127.46   160.86    33.40\n3: 59.97786    164.62   148.23   181.01    32.77\n4: 69.12560    185.08   168.58   201.59    33.01\n5: 78.27333    205.55   188.50   222.60    34.10\n\n\n[1] \"Interval Width Comparison:\"\n\n\n   Hardness CI_Width PI_Width PI_vs_CI_Ratio\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1: 41.68238    13.88    34.85           2.51\n2: 50.83012     9.69    33.40           3.45\n3: 59.97786     7.25    32.77           4.52\n4: 69.12560     8.26    33.01           4.00\n5: 78.27333    11.89    34.10           2.87\n\n\n\n\n\n# Confidence and prediction intervals\n# Choose some specific x values for demonstration\nx_new &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 5)\n\n# Confidence intervals for mean response\nCI_results &lt;- data.table()\nPI_results &lt;- data.table()\n\nfor (x0 in x_new) {\n  # Predicted value\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval for mean response\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_CI &lt;- t_critical * SE_mean\n  CI_lower &lt;- y_hat - margin_CI\n  CI_upper &lt;- y_hat + margin_CI\n\n  # Prediction interval for individual observation\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_PI &lt;- t_critical * SE_pred\n  PI_lower &lt;- y_hat - margin_PI\n  PI_upper &lt;- y_hat + margin_PI\n\n  CI_results &lt;- rbind(CI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    CI_Lower = round(CI_lower, 2),\n    CI_Upper = round(CI_upper, 2),\n    CI_Width = round(CI_upper - CI_lower, 2)\n  ))\n\n  PI_results &lt;- rbind(PI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    PI_Lower = round(PI_lower, 2),\n    PI_Upper = round(PI_upper, 2),\n    PI_Width = round(PI_upper - PI_lower, 2)\n  ))\n}\n\nprint(\"95% Confidence Intervals for Mean Response:\")\nprint(CI_results)\n\nprint(\"95% Prediction Intervals for Individual Observations:\")\nprint(PI_results)\n\n# Compare CI and PI widths\ninterval_comparison &lt;- data.table(\n  Hardness = x_new,\n  CI_Width = CI_results$CI_Width,\n  PI_Width = PI_results$PI_Width,\n  PI_vs_CI_Ratio = round(PI_results$PI_Width / CI_results$CI_Width, 2)\n)\n\nprint(\"Interval Width Comparison:\")\nprint(interval_comparison)\n\n\n\n\n\nPlotsR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Model diagnostic plots\n\n\n\n\n\n\n\n\n# Confidence and prediction intervals visualization\n# Create fine grid for smooth curves\nx_grid &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 100)\ninterval_data &lt;- data.table()\n\nfor (x0 in x_grid) {\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  CI_lower &lt;- y_hat - t_critical * SE_mean\n  CI_upper &lt;- y_hat + t_critical * SE_mean\n\n  # Prediction interval\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  PI_lower &lt;- y_hat - t_critical * SE_pred\n  PI_upper &lt;- y_hat + t_critical * SE_pred\n\n  interval_data &lt;- rbind(interval_data, data.table(\n    hardness = x0,\n    fitted = y_hat,\n    CI_lower = CI_lower,\n    CI_upper = CI_upper,\n    PI_lower = PI_lower,\n    PI_upper = PI_upper\n  ))\n}\n\nggplot() +\n  # Prediction intervals (wider, lighter)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = PI_lower, ymax = PI_upper),\n    alpha = 0.2, fill = \"red\"\n  ) +\n  # Confidence intervals (narrower, darker)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = CI_lower, ymax = CI_upper),\n    alpha = 0.3, fill = \"blue\"\n  ) +\n  # Regression line\n  geom_line(data = interval_data, aes(x = hardness, y = fitted), color = \"black\", linewidth = 1) +\n  # Data points\n  geom_point(data = steel_data, aes(x = hardness, y = strength), size = 3, alpha = 0.7) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Confidence and Prediction Intervals\",\n    subtitle = \"Blue = 95% CI for mean response, Red = 95% PI for individual observations\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )"
  },
  {
    "objectID": "book/Ch061.html#correlation-analysis",
    "href": "book/Ch061.html#correlation-analysis",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteSample Correlation\n\n\n\nr = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i - \\bar{x})^2 \\sum_{i=1}^n (y_i - \\bar{y})^2}}\nProperties: - -1 \\leq r \\leq 1 - r^2 = R^2 (simple regression) - Scale invariant\nTest for Significance: t = \\frac{r}{\\sqrt{\\frac{1-r^2}{n-2}}} \\sim t_{n-2}\n\n\n\n\n\n\n\n\n\n\n\nNoteExample 6.4: Steel Data Correlation\n\n\n\n\nCorrelationR Code\n\n\n\n\nError: object 'fitted_values' not found\n\n\nError: object 'residuals_data' not found\n\n\n[1] \"Residual Analysis Summary:\"\n\n\nError: object 'residual_summary' not found\n\n\nError: object 'residuals_data' not found\n\n\nError: object 'outliers' not found\n\n\nError in shapiro.test(residuals): is.numeric(x) is not TRUE\n\n\n[1] \"Shapiro-Wilk normality test for residuals:\"\n\n\nError: object 'shapiro_test' not found\n\n\nError in x[0L]: object of type 'closure' is not subsettable\n\n\nError: object 'dw_stat' not found\n\n\n[1] \"(Values near 2 indicate no autocorrelation)\"\n\n\n\n\n\n# Model adequacy checking - residual analysis\n# Calculate residuals and standardized residuals\nresiduals_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  standardized_residuals = residuals / sqrt(MSE),\n  observation = 1:n,\n  hardness = steel_data$hardness\n)\n\n# Summary statistics for residuals\nresidual_summary &lt;- residuals_data %&gt;%\n  fsummarise(\n    Mean_Residuals = fmean(residuals),\n    SD_Residuals = fsd(residuals),\n    Min_Residuals = fmin(residuals),\n    Max_Residuals = fmax(residuals),\n    Mean_Std_Residuals = fmean(standardized_residuals),\n    SD_Std_Residuals = fsd(standardized_residuals)\n  )\n\nprint(\"Residual Analysis Summary:\")\nprint(residual_summary)\n\n# Check for outliers (|standardized residual| &gt; 2)\noutliers &lt;- residuals_data[abs(standardized_residuals) &gt; 2]\nif (nrow(outliers) &gt; 0) {\n  print(\"Potential outliers (|std residual| &gt; 2):\")\n  print(outliers)\n} else {\n  print(\"No obvious outliers detected (|std residual| &gt; 2)\")\n}\n\n# Normality test for residuals\nshapiro_test &lt;- shapiro.test(residuals)\nprint(\"Shapiro-Wilk normality test for residuals:\")\nprint(shapiro_test)\n\n# Durbin-Watson test for independence (if data has natural order)\ndw_stat &lt;- sum(diff(residuals)^2) / sum(residuals^2)\nprint(paste(\"Durbin-Watson statistic:\", round(dw_stat, 4)))\nprint(\"(Values near 2 indicate no autocorrelation)\")"
  },
  {
    "objectID": "book/Ch061.html#summary",
    "href": "book/Ch061.html#summary",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteKey Engineering Applications\n\n\n\nSimple Linear Regression: - Process optimization - Quality control - Predictive modeling\nCorrelation Analysis:\n- Variable screening - Relationship assessment - Data exploration\nModel Validation: - Assumption checking - Outlier detection - Prediction accuracy"
  },
  {
    "objectID": "book/Ch01.html",
    "href": "book/Ch01.html",
    "title": "1 The Role of Statistics in Engineering",
    "section": "",
    "text": "TipMajor Themes of Chapter 1\n\n\n\n\nEngineering Method: Understanding how statistics supports engineering problem-solving through systematic data analysis\nData Collection: Learning different methods for gathering engineering data, from historical records to controlled experiments\nVariability: Recognizing and quantifying uncertainty in engineering systems as a fundamental aspect of real-world processes\nStatistical Thinking: Developing a framework for data-driven decision making that considers uncertainty and variation\nProcess Monitoring: Observing and controlling engineering processes over time using statistical methods\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nIdentify the role that statistics can play in the engineering problem-solving process.\nDiscuss how variability affects data collected and used in making decisions.\nDiscuss the methods that engineers use to collect data.\nExplain the importance of random samples.\nIdentify the advantages of designed experiments in data collection.\nDistinguish between mechanistic and empirical models.\nApply statistical thinking to process monitoring over time.\n\n\n\n\n\n\n\n\n\n\n\nNoteThe engineering problem-solving method\n\n\n\nEngineering Foundation: Engineers solve problems of interest to society by the efficient application of scientific principles. The engineering or scientific method is the approach to formulating and solving these problems.\nLet’s review the steps in this process.\n\nDevelop a clear and concise description of the problem.\nIdentify, at least tentatively, the factors that affect this problem or may play a role in its solution.\nPropose a model for the problem, using scientific or engineering knowledge of the phenomenon being studied. State any limitations or assumptions of the model.\nConduct appropriate experiments and collect data to test or validate the tentative model or conclusions in steps 2 and 3.\nRefine the model based on the observed data (repeating steps 2 through 5 as needed).\nManipulate the model to assist in developing a solution to the problem.\nConduct an appropriate experiment to confirm the effectiveness and efficiency of the proposed solution.\nDraw conclusions or make recommendations based on the problem solution.\n\n\n\n\n\n\n\nFigure 1: The engineering problem-solving method.\n\n\n\nStatistical Integration: In today’s data-driven engineering environment, statistical methods are essential tools that complement traditional engineering analysis, helping engineers make sense of uncertain, variable data and draw reliable conclusions.\n\n\n\n\n\n\n\n\n\n\nNoteStatistics in Engineering Problem-Solving\n\n\n\nStatistics plays a crucial role in each phase of the engineering method:\n1. Problem Recognition and Definition\n\nHelps identify patterns in data that indicate problems\nQuantifies the magnitude and frequency of issues\nProvides tools for problem prioritization\n\n2. Hypothesis Formation\n\nUses data analysis to suggest potential causes\nApplies statistical models to test theories\nEmploys correlation analysis to identify relationships\n\n3. Data Collection and Analysis\n\nDesigns efficient experiments and sampling plans\nProvides methods for data quality assessment\nOffers tools for exploratory data analysis\n\n4. Conclusion and Decision Making\n\nQuantifies uncertainty in results\nProvides confidence intervals and hypothesis tests\nEnables risk-based decision making\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Manufacturing Process Investigation\n\n\n\nEngineering Context: A manufacturing engineer notices increased variability in product dimensions, which could lead to quality issues and customer complaints. This demonstrates systematic application of statistical methods to engineering problem-solving.\n\n\n[1] \"Manufacturing Problem Example - Step by Step Analysis\"\n\n\n\nVariability Comparison Between Time Periods\n\n\nperiod\ncount\nmean_dimension\nstd_dev\nmin_value\nmax_value\n\n\n\n\nFirst_Half\n15\n50.322\n0.148\n50.172\n50.587\n\n\nSecond_Half\n15\n49.731\n0.138\n49.528\n49.976\n\n\n\n\n\n\nAnalysis by Operator\n\n\noperator\ncount\nmean_dimension\nstd_dev\n\n\n\n\nA\n10\n49.940\n0.290\n\n\nB\n10\n50.206\n0.327\n\n\nC\n10\n49.934\n0.332\n\n\n\n\n\n[1] \"Temperature-Dimension Correlation: 0.249\"\n\n\nStatistical Analysis Steps:\n\nProblem Recognition: Control charts show process instability\nHypothesis Formation: Possible causes include temperature, operator, or material variation\nData Collection: Design experiment to test factors systematically\nAnalysis and Conclusion: Statistical tests identify significant factors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability\n\n\n\n\nUsed to quantify likelihood or chance\nUsed to represent risk or uncertainty in engineering applications\nCan be interpreted as our degree of belief or relative frequency\n\nKey Probability Concepts:\n\nSample Space (S): Set of all possible outcomes\nEvent (A): Subset of the sample space\nProbability P(A): Measure of likelihood, where 0 ≤ P(A) ≤ 1\n\nEngineering Applications:\n\nReliability analysis: P(component failure)\nQuality control: P(defective product)\nRisk assessment: P(system malfunction)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistics\n\n\n\nDeals with the collection, presentation, analysis, and use of data to:\n\nMake decisions\nSolve problems\nDesign products and processes\n\nStatistical techniques are useful for describing and understanding variability. By variability, we mean successive observations of a system or phenomenon do not produce exactly the same result.\nStatistics gives us a framework for describing this variability and for learning about potential sources of variability.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Reasoning\n\n\n\nStatistics is the science of uncertainty & variability\nTurning Data into Information:\nData → Information → Knowledge → Wisdom\n\n\n\n\n\n\nFigure 2: Big Data in Engineering\n\n\n\nStatistics is the Art and Science of learning from Data.\nThe Statistical Thinking Process:\n\nRecognize the need for data-based decisions\nUnderstand the importance of data quality\nAppreciate the role of variability\nUse appropriate statistical methods\nCommunicate results effectively\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEssential Definitions\n\n\n\nPopulation\n\nSet of measurements of interest. Characteristics of the population (parameters) are typically of interest.\n\nSample\n\nSubset of measurements of interest. A characteristic of the sample (statistic) is used to infer population characteristics (parameters).\n\nParameter\n\nA characteristic of the population (usually unknown and estimated from sample data).\n\nStatistic\n\nA characteristic of the sample (computed from observed data).\n\nDescriptive Statistics\n\nDescribing the important characteristics of a set of data.\n\nInferential Statistics\n\nUsing sample data to make inferences (or generalizations) about a population.\n\nStatistical Inference\n\nMaking a statement about the population (parameter) based on the sample (statistic).\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: O-Ring Development for Semiconductor Equipment\n\n\n\nEngineering Context: An engineer is developing a rubber compound for use in O-rings. The O-rings are to be employed as seals in plasma etching tools used in the semiconductor industry, so their resistance to acids and other corrosive substances is an important characteristic.\nThe engineer uses the standard rubber compound to produce eight O-rings in a development laboratory and measures the tensile strength of each specimen after immersion in a nitric acid solution at 30°C for 25 minutes. The tensile strengths (in psi) of the eight O-rings are 1030, 1035, 1020, 1049, 1028, 1026, 1019, and 1010.\n\n\n\nO-Ring Sample Statistics & Confidence Interval\n\n\nsample_size\nsample_mean\nsample_median\nsample_std_dev\nsample_min\nsample_max\nstd_error\nt_value\nmargin_error\nci_lower\nci_upper\n\n\n\n\n8\n1027.1\n1027\n11.7\n1010\n1049\n4.14\n2.36\n9.8\n1017.3\n1036.9\n\n\n\n\n\nAnalysis:\n\nPopulation: All possible O-rings made with this rubber compound\nSample: The eight O-rings tested (n = 8)\nParameter: True mean tensile strength (μ) of all O-rings\nStatistic: Sample mean tensile strength (x̄ = 1027.1 psi)\n\nAs we should have anticipated, not all the O-ring specimens exhibit the same measurement of tensile strength.\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Variable\n\n\n\nSince tensile strength varies or exhibits variability, it is a random variable.\nA random variable X can be modeled by:\nX = \\mu + \\epsilon\nwhere μ is a constant and ε is a random disturbance, or “noise” term.\nSources of Variability:\n\nCommon Causes: Natural variation inherent in the process\nSpecial Causes: Unusual events that create additional variation\nMeasurement Error: Variation due to measurement system\nEnvironmental Factors: Temperature, humidity, vibration effects\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCollecting Engineering Data\n\n\n\nThree basic methods for collecting data:\n\nA retrospective study using historical data\nAn observational study\nA designed experiment\n\nChoosing the Right Method:\nThe choice depends on:\n\nAvailable resources and time\nLevel of control over variables\nObjective of the study\nEthical and practical constraints\n\n\n\n\n\n\n\n\n\n\n\nNoteRetrospective Study\n\n\n\nA retrospective study uses either all or a sample of the historical process data from some period of time. The objective of this study might be to determine the relationships among variables like temperature and concentration in a chemical process.\nAdvantages:\n\nData already exists (cost-effective)\nLarge datasets often available\nNo disruption to current operations\n\nDisadvantages:\n\nData quality may be poor\nImportant variables may not have been recorded\nConfounding variables difficult to control\nCausation difficult to establish\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Chemical Plant Process Analysis\n\n\n\nA chemical plant wants to improve acetone concentration in their output. They have 6 months of historical data including temperature, pressure, and acetone concentration.\n\n\n\nHistorical Data Summary (6 months)\n\n\ntotal_days\navg_temperature\navg_pressure\navg_acetone\n\n\n\n\n181\n84.8\n2.5\n153.6\n\n\n\n\n\n\nWeekend vs Weekday Comparison\n\n\nis_weekend\nnumber_of_days\navg_acetone\n\n\n\n\nFALSE\n130\n153.8\n\n\nTRUE\n51\n153.0\n\n\n\n\n\nAnalysis Approach:\n\nData Cleaning: Check for missing values and outliers\nExploratory Analysis: Examine relationships between variables\nStatistical Modeling: Develop predictive models\nValidation: Test model performance on recent data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservational Study\n\n\n\nAn observational study simply observes the process or population during a period of routine operation without making deliberate changes to the system.\nTypes of Observational Studies:\n\nCross-sectional: Data collected at a single point in time\nLongitudinal: Data collected over extended time periods\nCase-control: Comparing groups with and without certain characteristics\n\nAdvantages:\n\nRealistic operating conditions\nLess expensive than experiments\nCan study variables that cannot be manipulated\n\nDisadvantages:\n\nCannot establish causation\nConfounding variables present\nLimited control over data quality\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Power Consumption Analysis\n\n\n\nEngineers want to study the relationship between ambient temperature and power consumption in a manufacturing facility.\n\n\n\nSeasonal Summary\n\n\nseason\navg_temp\navg_power\nmax_power\n\n\n\n\nFall\n52.5\n550\n587\n\n\nSpring\n79.9\n544\n583\n\n\nSummer\n66.1\n508\n542\n\n\nWinter\n66.0\n507\n542\n\n\n\n\n\nKey Observations:\n\nStrong relationship between temperature and power consumption\nSeasonal patterns evident\nWeekend vs. weekday differences\nPotential energy savings opportunities identified\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDesigned Experiments\n\n\n\nThe third way that engineering data are collected is with a designed experiment. In a designed experiment, the engineer makes deliberate or purposeful changes in controllable variables (called factors) of the system, observes the resulting system output, and then makes a decision or an inference about which variables are responsible for the changes observed in the output performance.\nKey Components:\n\nFactors: Variables that can be controlled and changed\nLevels: Different values of factors to be tested\nResponse: Output variable(s) to be measured\nExperimental Units: Objects to which treatments are applied\n\nAdvantages:\n\nCan establish cause-and-effect relationships\nControl confounding variables\nEfficient use of resources\nStatistical validity\n\nTypes of Designs:\n\nCompletely Randomized Design\nRandomized Block Design\nFactorial Design\nResponse Surface Design\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Semiconductor Etching Process Optimization\n\n\n\nA semiconductor manufacturer wants to optimize the etching process. They identify three factors: RF Power, Pressure, and Gas Flow Rate.\n\n\n\nExperimental Results (2³ Factorial Design)\n\n\nrun_order\nRF_Power\nPressure\nGas_Flow\netch_rate\n\n\n\n\n1\nLow\nLow\nHigh\n71.3\n\n\n2\nHigh\nHigh\nHigh\n127.3\n\n\n3\nHigh\nLow\nHigh\n101.3\n\n\n4\nHigh\nHigh\nLow\n111.3\n\n\n5\nLow\nHigh\nLow\n81.3\n\n\n6\nLow\nLow\nLow\n75.3\n\n\n7\nHigh\nLow\nLow\n105.3\n\n\n8\nLow\nHigh\nHigh\n97.3\n\n\n\n\n\n\nMain Effects Analysis\n\n\nFactor\nEffect\n\n\n\n\nRF Power\n30\n\n\nPressure\n16\n\n\nGas Flow\n6\n\n\n\n\n\nExperimental Design:\n\n2³ Factorial Design (8 experimental runs)\nFactors: RF Power (Low/High), Pressure (Low/High), Gas Flow (Low/High)\nResponse: Etch Rate (nm/min)\nRandomized run order to minimize bias\n\nResults Show:\n\nRF Power has the strongest effect\nPressure and Gas Flow interaction is significant\nOptimal settings identified\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Samples\n\n\n\nAlmost all statistical analysis is based on the idea of using a sample of data that has been selected from some population.\nThe objective is to use the sample data to make decisions or learn something about the population.\nOnly random samples are likely to be useful in statistics, as they give us the best chance of obtaining a sample that is representative of the population.\nTypes of Random Sampling:\n\nSimple Random Sampling\nSystematic Sampling\nStratified Sampling\nCluster Sampling\n\n\n\n\n\n\n\n\n\nNoteSimple Random Sample\n\n\n\nA simple random sample of size n is a sample that has been selected from a population in such a way that each possible sample of size n has an equally likely chance of being selected.\nRequirements for Simple Random Sampling:\n\nRandom Selection: Each unit has equal probability of selection\nIndependence: Selection of one unit doesn’t affect others\nKnown Population: Complete list (sampling frame) available\n\nImplementation Methods:\n\nRandom number tables\nComputer random number generators\nPhysical randomization methods\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Quality Control Sampling\n\n\n\nA quality engineer needs to sample 50 products from a batch of 1000 items for testing.\n\n\n\nPopulation vs Sample Comparison\n\n\nParameter\nValue\n\n\n\n\nPopulation Mean\n84.83\n\n\nPopulation Std\n9.29\n\n\nSample Mean\n83.32\n\n\nSample Std\n9.74\n\n\nStandard Error\n1.31\n\n\n\n\n\nSampling Process:\n\nNumber all items from 1 to 1000\nGenerate 50 random numbers between 1 and 1000\nSelect items corresponding to random numbers\nTest selected items and analyze results\n\nThis ensures each item has an equal chance (50/1000 = 0.05) of being selected.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTime Series Analysis\n\n\n\nWhenever data are collected over time it is important to plot the data over time. Phenomena that might affect the system or process often become more visible in a time-oriented plot and the concept of stability can be better judged.\nWhy Time Plots Are Important:\n\nReveal trends and patterns\nIdentify special causes of variation\nShow process stability\nDetect autocorrelation\nHelp in forecasting\n\n\n\n\n\n\n\nFigure 3: A dot diagram illustrates variation but does not identify the problem.\n\n\n\n\n\n\n\n\n\nFigure 4: A time series plot of acetone concentration provides more information than the dot diagram.\n\n\n\n\n\n\n\n\n\nFigure 5: A control chart for the chemical process concentration data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Process Control\n\n\n\nControl Charts are statistical tools used to monitor process performance over time:\nKey Components:\n\nCenter Line: Process average\nControl Limits: Boundaries for common cause variation\nUpper Control Limit (UCL): μ + 3σ\nLower Control Limit (LCL): μ - 3σ\n\nTypes of Control Charts:\n\nX̄-Chart: For sample means\nR-Chart: For sample ranges\np-Chart: For proportion defective\nc-Chart: For count of defects\n\nInterpretation Rules:\n\nPoints outside control limits indicate special causes\nNon-random patterns suggest assignable causes\nProcess is “in control” when only common cause variation present\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Chemical Process Control\n\n\n\nMonitor the output of a chemical process where samples are taken every hour for 24 hours:\n\n\n\nProcess Summary (24 hours)\n\n\nsamples\nmean_conc\nstd_conc\nooc_points\n\n\n\n\n24\n89.27\n4.47\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Process monitoring: dot diagram vs. time series plot\n\n\n\n\n\nKey Insights from Time Series Plot:\n\nProcess shows upward trend over time\nIncreased variability in later hours\nPossible shift at hour 15\nNeed to investigate special causes\n\nControl Chart Analysis:\n\nSeveral points outside control limits\nProcess not in statistical control\nCorrective action required\n\n\n\n\n\n\n\n\n\n\n\n\nNotePattern Recognition\n\n\n\nCommon Patterns to Look For:\n1. Trends\n\nGradual increase or decrease over time\nMay indicate tool wear, drift, or systematic changes\n\n2. Shifts\n\nSudden change in process level\nOften due to change in materials, operators, or settings\n\n3. Cycles\n\nRegular patterns that repeat\nMay be related to temperature, shift changes, or other periodic factors\n\n4. Unusual Points\n\nIndividual measurements far from typical values\nMay indicate special causes or measurement errors\n\nStatistical Tests for Patterns:\n\nRun Test: Tests for randomness\nTrend Test: Detects systematic trends\nShift Test: Identifies level changes\n\n\n\n\n\n\n\n\n\nPattern Analysis Summary\n\n\npattern\nmean_value\nstd_value\ntrend_corr\n\n\n\n\nCycles\n50.10\n3.24\n-0.06\n\n\nRandom\n49.73\n2.61\n-0.25\n\n\nShift\n52.00\n4.48\n0.72\n\n\nTrend\n52.89\n4.65\n0.90\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Different patterns in time series data\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChapter 1 Summary\n\n\n\nThe Role of Statistics in Engineering:\n\nProblem-Solving Framework: Statistics provides tools for each phase of the engineering method\nData Collection Methods: Retrospective studies, observational studies, and designed experiments each have their place\nUnderstanding Variability: All engineering systems exhibit variability that must be quantified and controlled\nProcess Monitoring: Time series analysis and control charts enable continuous improvement\n\nKey Principles:\n\nStatistical Thinking: Focus on variation, data quality, and continuous improvement\nRandom Sampling: Essential for valid statistical inference\nAppropriate Methods: Choose data collection method based on objectives and constraints\nTime Perspective: Always consider how processes behave over time\n\nPractical Applications:\n\nQuality control and improvement\nProcess optimization\nRisk assessment\nDesign verification and validation\nReliability analysis\n\n\n\nThis chapter establishes the fundamental role of statistics in engineering practice, emphasizing the importance of understanding variability, choosing appropriate data collection methods, and thinking statistically about engineering problems.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "book/Ch01.html#the-engineering-method-and-statistical-thinking",
    "href": "book/Ch01.html#the-engineering-method-and-statistical-thinking",
    "title": "1 The Role of Statistics in Engineering",
    "section": "",
    "text": "NoteThe engineering problem-solving method\n\n\n\nEngineering Foundation: Engineers solve problems of interest to society by the efficient application of scientific principles. The engineering or scientific method is the approach to formulating and solving these problems.\nLet’s review the steps in this process.\n\nDevelop a clear and concise description of the problem.\nIdentify, at least tentatively, the factors that affect this problem or may play a role in its solution.\nPropose a model for the problem, using scientific or engineering knowledge of the phenomenon being studied. State any limitations or assumptions of the model.\nConduct appropriate experiments and collect data to test or validate the tentative model or conclusions in steps 2 and 3.\nRefine the model based on the observed data (repeating steps 2 through 5 as needed).\nManipulate the model to assist in developing a solution to the problem.\nConduct an appropriate experiment to confirm the effectiveness and efficiency of the proposed solution.\nDraw conclusions or make recommendations based on the problem solution.\n\n\n\n\n\n\n\nFigure 1: The engineering problem-solving method.\n\n\n\nStatistical Integration: In today’s data-driven engineering environment, statistical methods are essential tools that complement traditional engineering analysis, helping engineers make sense of uncertain, variable data and draw reliable conclusions.\n\n\n\n\n\n\n\n\n\n\nNoteStatistics in Engineering Problem-Solving\n\n\n\nStatistics plays a crucial role in each phase of the engineering method:\n1. Problem Recognition and Definition\n\nHelps identify patterns in data that indicate problems\nQuantifies the magnitude and frequency of issues\nProvides tools for problem prioritization\n\n2. Hypothesis Formation\n\nUses data analysis to suggest potential causes\nApplies statistical models to test theories\nEmploys correlation analysis to identify relationships\n\n3. Data Collection and Analysis\n\nDesigns efficient experiments and sampling plans\nProvides methods for data quality assessment\nOffers tools for exploratory data analysis\n\n4. Conclusion and Decision Making\n\nQuantifies uncertainty in results\nProvides confidence intervals and hypothesis tests\nEnables risk-based decision making\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Manufacturing Process Investigation\n\n\n\nEngineering Context: A manufacturing engineer notices increased variability in product dimensions, which could lead to quality issues and customer complaints. This demonstrates systematic application of statistical methods to engineering problem-solving.\n\n\n[1] \"Manufacturing Problem Example - Step by Step Analysis\"\n\n\n\nVariability Comparison Between Time Periods\n\n\nperiod\ncount\nmean_dimension\nstd_dev\nmin_value\nmax_value\n\n\n\n\nFirst_Half\n15\n50.322\n0.148\n50.172\n50.587\n\n\nSecond_Half\n15\n49.731\n0.138\n49.528\n49.976\n\n\n\n\n\n\nAnalysis by Operator\n\n\noperator\ncount\nmean_dimension\nstd_dev\n\n\n\n\nA\n10\n49.940\n0.290\n\n\nB\n10\n50.206\n0.327\n\n\nC\n10\n49.934\n0.332\n\n\n\n\n\n[1] \"Temperature-Dimension Correlation: 0.249\"\n\n\nStatistical Analysis Steps:\n\nProblem Recognition: Control charts show process instability\nHypothesis Formation: Possible causes include temperature, operator, or material variation\nData Collection: Design experiment to test factors systematically\nAnalysis and Conclusion: Statistical tests identify significant factors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability\n\n\n\n\nUsed to quantify likelihood or chance\nUsed to represent risk or uncertainty in engineering applications\nCan be interpreted as our degree of belief or relative frequency\n\nKey Probability Concepts:\n\nSample Space (S): Set of all possible outcomes\nEvent (A): Subset of the sample space\nProbability P(A): Measure of likelihood, where 0 ≤ P(A) ≤ 1\n\nEngineering Applications:\n\nReliability analysis: P(component failure)\nQuality control: P(defective product)\nRisk assessment: P(system malfunction)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistics\n\n\n\nDeals with the collection, presentation, analysis, and use of data to:\n\nMake decisions\nSolve problems\nDesign products and processes\n\nStatistical techniques are useful for describing and understanding variability. By variability, we mean successive observations of a system or phenomenon do not produce exactly the same result.\nStatistics gives us a framework for describing this variability and for learning about potential sources of variability.\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Reasoning\n\n\n\nStatistics is the science of uncertainty & variability\nTurning Data into Information:\nData → Information → Knowledge → Wisdom\n\n\n\n\n\n\nFigure 2: Big Data in Engineering\n\n\n\nStatistics is the Art and Science of learning from Data.\nThe Statistical Thinking Process:\n\nRecognize the need for data-based decisions\nUnderstand the importance of data quality\nAppreciate the role of variability\nUse appropriate statistical methods\nCommunicate results effectively\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEssential Definitions\n\n\n\nPopulation\n\nSet of measurements of interest. Characteristics of the population (parameters) are typically of interest.\n\nSample\n\nSubset of measurements of interest. A characteristic of the sample (statistic) is used to infer population characteristics (parameters).\n\nParameter\n\nA characteristic of the population (usually unknown and estimated from sample data).\n\nStatistic\n\nA characteristic of the sample (computed from observed data).\n\nDescriptive Statistics\n\nDescribing the important characteristics of a set of data.\n\nInferential Statistics\n\nUsing sample data to make inferences (or generalizations) about a population.\n\nStatistical Inference\n\nMaking a statement about the population (parameter) based on the sample (statistic).\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: O-Ring Development for Semiconductor Equipment\n\n\n\nEngineering Context: An engineer is developing a rubber compound for use in O-rings. The O-rings are to be employed as seals in plasma etching tools used in the semiconductor industry, so their resistance to acids and other corrosive substances is an important characteristic.\nThe engineer uses the standard rubber compound to produce eight O-rings in a development laboratory and measures the tensile strength of each specimen after immersion in a nitric acid solution at 30°C for 25 minutes. The tensile strengths (in psi) of the eight O-rings are 1030, 1035, 1020, 1049, 1028, 1026, 1019, and 1010.\n\n\n\nO-Ring Sample Statistics & Confidence Interval\n\n\nsample_size\nsample_mean\nsample_median\nsample_std_dev\nsample_min\nsample_max\nstd_error\nt_value\nmargin_error\nci_lower\nci_upper\n\n\n\n\n8\n1027.1\n1027\n11.7\n1010\n1049\n4.14\n2.36\n9.8\n1017.3\n1036.9\n\n\n\n\n\nAnalysis:\n\nPopulation: All possible O-rings made with this rubber compound\nSample: The eight O-rings tested (n = 8)\nParameter: True mean tensile strength (μ) of all O-rings\nStatistic: Sample mean tensile strength (x̄ = 1027.1 psi)\n\nAs we should have anticipated, not all the O-ring specimens exhibit the same measurement of tensile strength.\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Variable\n\n\n\nSince tensile strength varies or exhibits variability, it is a random variable.\nA random variable X can be modeled by:\nX = \\mu + \\epsilon\nwhere μ is a constant and ε is a random disturbance, or “noise” term.\nSources of Variability:\n\nCommon Causes: Natural variation inherent in the process\nSpecial Causes: Unusual events that create additional variation\nMeasurement Error: Variation due to measurement system\nEnvironmental Factors: Temperature, humidity, vibration effects",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "book/Ch01.html#collecting-engineering-data",
    "href": "book/Ch01.html#collecting-engineering-data",
    "title": "1 The Role of Statistics in Engineering",
    "section": "",
    "text": "NoteCollecting Engineering Data\n\n\n\nThree basic methods for collecting data:\n\nA retrospective study using historical data\nAn observational study\nA designed experiment\n\nChoosing the Right Method:\nThe choice depends on:\n\nAvailable resources and time\nLevel of control over variables\nObjective of the study\nEthical and practical constraints\n\n\n\n\n\n\n\n\n\n\n\nNoteRetrospective Study\n\n\n\nA retrospective study uses either all or a sample of the historical process data from some period of time. The objective of this study might be to determine the relationships among variables like temperature and concentration in a chemical process.\nAdvantages:\n\nData already exists (cost-effective)\nLarge datasets often available\nNo disruption to current operations\n\nDisadvantages:\n\nData quality may be poor\nImportant variables may not have been recorded\nConfounding variables difficult to control\nCausation difficult to establish\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Chemical Plant Process Analysis\n\n\n\nA chemical plant wants to improve acetone concentration in their output. They have 6 months of historical data including temperature, pressure, and acetone concentration.\n\n\n\nHistorical Data Summary (6 months)\n\n\ntotal_days\navg_temperature\navg_pressure\navg_acetone\n\n\n\n\n181\n84.8\n2.5\n153.6\n\n\n\n\n\n\nWeekend vs Weekday Comparison\n\n\nis_weekend\nnumber_of_days\navg_acetone\n\n\n\n\nFALSE\n130\n153.8\n\n\nTRUE\n51\n153.0\n\n\n\n\n\nAnalysis Approach:\n\nData Cleaning: Check for missing values and outliers\nExploratory Analysis: Examine relationships between variables\nStatistical Modeling: Develop predictive models\nValidation: Test model performance on recent data\n\n\n\n\n\n\n\n\n\n\n\n\nNoteObservational Study\n\n\n\nAn observational study simply observes the process or population during a period of routine operation without making deliberate changes to the system.\nTypes of Observational Studies:\n\nCross-sectional: Data collected at a single point in time\nLongitudinal: Data collected over extended time periods\nCase-control: Comparing groups with and without certain characteristics\n\nAdvantages:\n\nRealistic operating conditions\nLess expensive than experiments\nCan study variables that cannot be manipulated\n\nDisadvantages:\n\nCannot establish causation\nConfounding variables present\nLimited control over data quality\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Power Consumption Analysis\n\n\n\nEngineers want to study the relationship between ambient temperature and power consumption in a manufacturing facility.\n\n\n\nSeasonal Summary\n\n\nseason\navg_temp\navg_power\nmax_power\n\n\n\n\nFall\n52.5\n550\n587\n\n\nSpring\n79.9\n544\n583\n\n\nSummer\n66.1\n508\n542\n\n\nWinter\n66.0\n507\n542\n\n\n\n\n\nKey Observations:\n\nStrong relationship between temperature and power consumption\nSeasonal patterns evident\nWeekend vs. weekday differences\nPotential energy savings opportunities identified\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDesigned Experiments\n\n\n\nThe third way that engineering data are collected is with a designed experiment. In a designed experiment, the engineer makes deliberate or purposeful changes in controllable variables (called factors) of the system, observes the resulting system output, and then makes a decision or an inference about which variables are responsible for the changes observed in the output performance.\nKey Components:\n\nFactors: Variables that can be controlled and changed\nLevels: Different values of factors to be tested\nResponse: Output variable(s) to be measured\nExperimental Units: Objects to which treatments are applied\n\nAdvantages:\n\nCan establish cause-and-effect relationships\nControl confounding variables\nEfficient use of resources\nStatistical validity\n\nTypes of Designs:\n\nCompletely Randomized Design\nRandomized Block Design\nFactorial Design\nResponse Surface Design\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Semiconductor Etching Process Optimization\n\n\n\nA semiconductor manufacturer wants to optimize the etching process. They identify three factors: RF Power, Pressure, and Gas Flow Rate.\n\n\n\nExperimental Results (2³ Factorial Design)\n\n\nrun_order\nRF_Power\nPressure\nGas_Flow\netch_rate\n\n\n\n\n1\nLow\nLow\nHigh\n71.3\n\n\n2\nHigh\nHigh\nHigh\n127.3\n\n\n3\nHigh\nLow\nHigh\n101.3\n\n\n4\nHigh\nHigh\nLow\n111.3\n\n\n5\nLow\nHigh\nLow\n81.3\n\n\n6\nLow\nLow\nLow\n75.3\n\n\n7\nHigh\nLow\nLow\n105.3\n\n\n8\nLow\nHigh\nHigh\n97.3\n\n\n\n\n\n\nMain Effects Analysis\n\n\nFactor\nEffect\n\n\n\n\nRF Power\n30\n\n\nPressure\n16\n\n\nGas Flow\n6\n\n\n\n\n\nExperimental Design:\n\n2³ Factorial Design (8 experimental runs)\nFactors: RF Power (Low/High), Pressure (Low/High), Gas Flow (Low/High)\nResponse: Etch Rate (nm/min)\nRandomized run order to minimize bias\n\nResults Show:\n\nRF Power has the strongest effect\nPressure and Gas Flow interaction is significant\nOptimal settings identified\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Samples\n\n\n\nAlmost all statistical analysis is based on the idea of using a sample of data that has been selected from some population.\nThe objective is to use the sample data to make decisions or learn something about the population.\nOnly random samples are likely to be useful in statistics, as they give us the best chance of obtaining a sample that is representative of the population.\nTypes of Random Sampling:\n\nSimple Random Sampling\nSystematic Sampling\nStratified Sampling\nCluster Sampling\n\n\n\n\n\n\n\n\n\nNoteSimple Random Sample\n\n\n\nA simple random sample of size n is a sample that has been selected from a population in such a way that each possible sample of size n has an equally likely chance of being selected.\nRequirements for Simple Random Sampling:\n\nRandom Selection: Each unit has equal probability of selection\nIndependence: Selection of one unit doesn’t affect others\nKnown Population: Complete list (sampling frame) available\n\nImplementation Methods:\n\nRandom number tables\nComputer random number generators\nPhysical randomization methods\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Quality Control Sampling\n\n\n\nA quality engineer needs to sample 50 products from a batch of 1000 items for testing.\n\n\n\nPopulation vs Sample Comparison\n\n\nParameter\nValue\n\n\n\n\nPopulation Mean\n84.83\n\n\nPopulation Std\n9.29\n\n\nSample Mean\n83.32\n\n\nSample Std\n9.74\n\n\nStandard Error\n1.31\n\n\n\n\n\nSampling Process:\n\nNumber all items from 1 to 1000\nGenerate 50 random numbers between 1 and 1000\nSelect items corresponding to random numbers\nTest selected items and analyze results\n\nThis ensures each item has an equal chance (50/1000 = 0.05) of being selected.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "book/Ch01.html#observing-processes-over-time",
    "href": "book/Ch01.html#observing-processes-over-time",
    "title": "1 The Role of Statistics in Engineering",
    "section": "",
    "text": "NoteTime Series Analysis\n\n\n\nWhenever data are collected over time it is important to plot the data over time. Phenomena that might affect the system or process often become more visible in a time-oriented plot and the concept of stability can be better judged.\nWhy Time Plots Are Important:\n\nReveal trends and patterns\nIdentify special causes of variation\nShow process stability\nDetect autocorrelation\nHelp in forecasting\n\n\n\n\n\n\n\nFigure 3: A dot diagram illustrates variation but does not identify the problem.\n\n\n\n\n\n\n\n\n\nFigure 4: A time series plot of acetone concentration provides more information than the dot diagram.\n\n\n\n\n\n\n\n\n\nFigure 5: A control chart for the chemical process concentration data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Process Control\n\n\n\nControl Charts are statistical tools used to monitor process performance over time:\nKey Components:\n\nCenter Line: Process average\nControl Limits: Boundaries for common cause variation\nUpper Control Limit (UCL): μ + 3σ\nLower Control Limit (LCL): μ - 3σ\n\nTypes of Control Charts:\n\nX̄-Chart: For sample means\nR-Chart: For sample ranges\np-Chart: For proportion defective\nc-Chart: For count of defects\n\nInterpretation Rules:\n\nPoints outside control limits indicate special causes\nNon-random patterns suggest assignable causes\nProcess is “in control” when only common cause variation present\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample: Chemical Process Control\n\n\n\nMonitor the output of a chemical process where samples are taken every hour for 24 hours:\n\n\n\nProcess Summary (24 hours)\n\n\nsamples\nmean_conc\nstd_conc\nooc_points\n\n\n\n\n24\n89.27\n4.47\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Process monitoring: dot diagram vs. time series plot\n\n\n\n\n\nKey Insights from Time Series Plot:\n\nProcess shows upward trend over time\nIncreased variability in later hours\nPossible shift at hour 15\nNeed to investigate special causes\n\nControl Chart Analysis:\n\nSeveral points outside control limits\nProcess not in statistical control\nCorrective action required\n\n\n\n\n\n\n\n\n\n\n\n\nNotePattern Recognition\n\n\n\nCommon Patterns to Look For:\n1. Trends\n\nGradual increase or decrease over time\nMay indicate tool wear, drift, or systematic changes\n\n2. Shifts\n\nSudden change in process level\nOften due to change in materials, operators, or settings\n\n3. Cycles\n\nRegular patterns that repeat\nMay be related to temperature, shift changes, or other periodic factors\n\n4. Unusual Points\n\nIndividual measurements far from typical values\nMay indicate special causes or measurement errors\n\nStatistical Tests for Patterns:\n\nRun Test: Tests for randomness\nTrend Test: Detects systematic trends\nShift Test: Identifies level changes\n\n\n\n\n\n\n\n\n\nPattern Analysis Summary\n\n\npattern\nmean_value\nstd_value\ntrend_corr\n\n\n\n\nCycles\n50.10\n3.24\n-0.06\n\n\nRandom\n49.73\n2.61\n-0.25\n\n\nShift\n52.00\n4.48\n0.72\n\n\nTrend\n52.89\n4.65\n0.90\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Different patterns in time series data",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "book/Ch01.html#summary-and-key-takeaways",
    "href": "book/Ch01.html#summary-and-key-takeaways",
    "title": "1 The Role of Statistics in Engineering",
    "section": "",
    "text": "NoteChapter 1 Summary\n\n\n\nThe Role of Statistics in Engineering:\n\nProblem-Solving Framework: Statistics provides tools for each phase of the engineering method\nData Collection Methods: Retrospective studies, observational studies, and designed experiments each have their place\nUnderstanding Variability: All engineering systems exhibit variability that must be quantified and controlled\nProcess Monitoring: Time series analysis and control charts enable continuous improvement\n\nKey Principles:\n\nStatistical Thinking: Focus on variation, data quality, and continuous improvement\nRandom Sampling: Essential for valid statistical inference\nAppropriate Methods: Choose data collection method based on objectives and constraints\nTime Perspective: Always consider how processes behave over time\n\nPractical Applications:\n\nQuality control and improvement\nProcess optimization\nRisk assessment\nDesign verification and validation\nReliability analysis\n\n\n\nThis chapter establishes the fundamental role of statistics in engineering practice, emphasizing the importance of understanding variability, choosing appropriate data collection methods, and thinking statistically about engineering problems.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "The Role of Statistics in Engineering"
    ]
  },
  {
    "objectID": "book/Ch06.html",
    "href": "book/Ch06.html",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "TipMajor Themes of Chapter 6\n\n\n\n\nEmpirical Models: Building mathematical relationships from data\nSimple Linear Regression: Modeling relationships between two variables\nMultiple Regression: Extending to multiple predictor variables\nModel Adequacy: Checking assumptions and validating models\nAdvanced Topics: Polynomial models, categorical variables, and variable selection\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nUnderstand the fundamental concepts of empirical modeling and regression.\nApply least squares estimation to fit simple linear regression models.\nConduct hypothesis tests and construct confidence intervals for regression parameters.\nMake predictions with appropriate uncertainty quantification.\nCheck model adequacy using residual analysis and diagnostic plots.\nUnderstand the relationship between correlation and regression.\nFit and interpret multiple regression models.\nWork with polynomial models and categorical variables.\nApply variable selection techniques for model building.\n\n\n\n\n\n\n\n\n\n\n\nNoteWhat are Empirical Models?\n\n\n\nEmpirical models are mathematical relationships derived from observed data rather than theoretical principles. They are essential in engineering for:\n\nProcess Optimization: Understanding how input variables affect outputs\nQuality Control: Relating product characteristics to process conditions\nDesign: Predicting performance based on design parameters\nTroubleshooting: Identifying factors that influence system behavior\n\nTypes of Empirical Models:\n\nLinear Models: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\epsilon\nPolynomial Models: y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\epsilon\nInteraction Models: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 + \\epsilon\n\nKey Assumptions:\n\nLinear relationship between predictors and response\nIndependent observations\nConstant variance (homoscedasticity)\nNormally distributed errors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSimple Linear Regression Model\n\n\n\nThe simple linear regression model relates a response variable y to a single predictor variable x:\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\nwhere:\n\ny_i = response variable for observation i\nx_i = predictor variable for observation i\n\\beta_0 = intercept parameter\n\\beta_1 = slope parameter\n\\epsilon_i = random error term, \\epsilon_i \\sim N(0, \\sigma^2)\n\nLeast Squares Estimators:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nwhere:\n\nS_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\nS_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2\nS_{yy} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA materials engineer wants to relate the tensile strength of steel specimens to their hardness. Data from 20 specimens:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Steel Strength vs Hardness Data Summary:\"\n\n\n       n Hardness_Mean Hardness_SD Hardness_Min Hardness_Max Strength_Mean\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:    20      62.03234    12.53884     41.68238     78.27333      169.2171\n   Strength_SD Strength_Min Strength_Max\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:     29.0103     120.2922     220.4809\n\n\n[1] \"First 10 observations:\"\n\n\n    specimen hardness strength\n       &lt;int&gt;    &lt;num&gt;    &lt;num&gt;\n 1:        1 51.50310 153.5504\n 2:        2 71.53221 196.7090\n 3:        3 56.35908 159.1039\n 4:        4 75.32070 204.1872\n 5:        5 77.61869 204.6000\n 6:        6 41.82226 133.8510\n 7:        7 61.12422 171.7934\n 8:        8 75.69676 188.5090\n 9:        9 62.05740 175.7543\n10:       10 58.26459 156.8791\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Steel strength vs hardness data (simple linear regression)\nset.seed(123)\nsteel_data &lt;- data.table(\n  specimen = 1:20,\n  hardness = runif(20, min = 40, max = 80),\n  strength = NULL\n)\n\n# Create realistic relationship with some noise\nsteel_data[, strength := 15 + 2.5 * hardness + rnorm(20, mean = 0, sd = 8)]\n\n# Summary statistics\nsteel_summary &lt;- steel_data %&gt;%\n  fsummarise(\n    n = fnobs(hardness),\n    Hardness_Mean = fmean(hardness),\n    Hardness_SD = fsd(hardness),\n    Hardness_Min = fmin(hardness),\n    Hardness_Max = fmax(hardness),\n    Strength_Mean = fmean(strength),\n    Strength_SD = fsd(strength),\n    Strength_Min = fmin(strength),\n    Strength_Max = fmax(strength)\n  )\n\nprint(\"Steel Strength vs Hardness Data Summary:\")\nprint(steel_summary)\n\n# Detailed data view\nprint(\"First 10 observations:\")\nprint(head(steel_data, 10))\n\n# Create data directory if needed\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\n# Write data to CSV\nfwrite(steel_data, \"data/steel_strength.csv\")\n\n\n\n\n\n\n\nManual Calculation:\nGiven: n = 20, \\bar{x} = hardness mean, \\bar{y} = strength mean\n\nCalculate Sums:\n\nS_{xx} = \\sum(x_i - \\bar{x})^2\nS_{xy} = \\sum(x_i - \\bar{x})(y_i - \\bar{y})\nS_{yy} = \\sum(y_i - \\bar{y})^2\n\nEstimate Parameters: \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nFitted Model: \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\n\nR OutputR Code\n\n\n\n\n[1] \"Manual Simple Linear Regression Results:\"\n\n\n        Parameter Manual_Estimate                               Interpretation\n           &lt;char&gt;           &lt;num&gt;                                       &lt;char&gt;\n1: β₀ (Intercept)         30.4493            Strength when hardness = 0: 30.45\n2:     β₁ (Slope)          2.2370 Strength increases by 2.24 per unit hardness\n3:             σ²         57.8584                        Error variance: 57.86\n4:             R²          0.9349                93.5 % of variation explained\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = strength ~ hardness, data = steel_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2758  -5.0561  -0.6948   5.4929  15.1408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.4493     8.7991   3.461  0.00279 ** \nhardness      2.2370     0.1392  16.074 4.03e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.606 on 18 degrees of freedom\nMultiple R-squared:  0.9349,    Adjusted R-squared:  0.9313 \nF-statistic: 258.4 on 1 and 18 DF,  p-value: 4.031e-12\n\n\n[1] \"Manual vs R Function Comparison:\"\n\n\n     Parameter    Manual R_Function   Difference\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   Intercept 30.449343  30.449343 7.815970e-14\n2:       Slope  2.237023   2.237023 8.881784e-16\n3:   R-squared  0.934870   0.934870 0.000000e+00\n4: Residual SE  7.606471   7.606471 2.664535e-15\n\n\n\n\n\n# Simple linear regression - manual calculations\nn &lt;- fnobs(steel_data$hardness)\nx_bar &lt;- fmean(steel_data$hardness)\ny_bar &lt;- fmean(steel_data$strength)\n\n# Calculate sums of squares and cross-products\nx &lt;- steel_data$hardness\ny &lt;- steel_data$strength\n\n# Manual calculation of sums\nSxx &lt;- sum((x - x_bar)^2)\nSyy &lt;- sum((y - y_bar)^2)\nSxy &lt;- sum((x - x_bar) * (y - y_bar))\n\n# Parameter estimates\nbeta1_hat &lt;- Sxy / Sxx\nbeta0_hat &lt;- y_bar - beta1_hat * x_bar\n\n# Sum of squares\nSSR &lt;- beta1_hat * Sxy # Regression sum of squares\nSSE &lt;- Syy - beta1_hat * Sxy # Error sum of squares\nSST &lt;- Syy # Total sum of squares\n\n# Mean squares\nMSR &lt;- SSR / 1 # df = 1 for simple regression\nMSE &lt;- SSE / (n - 2) # df = n - 2\ns_squared &lt;- MSE\n\n# R-squared\nR_squared &lt;- SSR / SST\n\n# Manual calculations summary\nmanual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Slope)\", \"σ²\", \"R²\"),\n  Manual_Estimate = c(\n    round(beta0_hat, 4),\n    round(beta1_hat, 4),\n    round(s_squared, 4),\n    round(R_squared, 4)\n  ),\n  Interpretation = c(\n    paste(\"Strength when hardness = 0:\", round(beta0_hat, 2)),\n    paste(\"Strength increases by\", round(beta1_hat, 2), \"per unit hardness\"),\n    paste(\"Error variance:\", round(s_squared, 2)),\n    paste(round(R_squared * 100, 1), \"% of variation explained\")\n  )\n)\n\nprint(\"Manual Simple Linear Regression Results:\")\nprint(manual_results)\n\n# Verification with R's lm function\nlm_model &lt;- lm(strength ~ hardness, data = steel_data)\nlm_summary &lt;- summary(lm_model)\n\nprint(\"R's lm() verification:\")\nprint(lm_summary)\n\n# Compare manual vs R calculations\ncomparison &lt;- data.table(\n  Parameter = c(\"Intercept\", \"Slope\", \"R-squared\", \"Residual SE\"),\n  Manual = c(beta0_hat, beta1_hat, R_squared, sqrt(MSE)),\n  R_Function = c(\n    coef(lm_model)[1], coef(lm_model)[2],\n    summary(lm_model)$r.squared, summary(lm_model)$sigma\n  ),\n  Difference = c(\n    abs(beta0_hat - coef(lm_model)[1]),\n    abs(beta1_hat - coef(lm_model)[2]),\n    abs(R_squared - summary(lm_model)$r.squared),\n    abs(sqrt(MSE) - summary(lm_model)$sigma)\n  )\n)\n\nprint(\"Manual vs R Function Comparison:\")\nprint(comparison)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Simple linear regression: Tensile strength vs. hardness\n\n\n\n\n\n\n\n\n# Simple linear regression visualization\np1 &lt;- ggplot(data = steel_data, mapping = aes(x = hardness, y = strength)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Simple Linear Regression: Tensile Strength vs Hardness\",\n    subtitle = paste(\"ŷ =\", round(beta0_hat, 2), \"+\", round(beta1_hat, 2), \"x, R² =\", round(R_squared, 3))\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n# Residuals vs fitted\nfitted_values &lt;- beta0_hat + beta1_hat * steel_data$hardness\nresiduals &lt;- steel_data$strength - fitted_values\n\nresidual_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  hardness = steel_data$hardness\n)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Tests for Regression Parameters\n\n\n\nTest for Slope (Significance of Regression):\nH₀: β₁ = 0 vs H₁: β₁ ≠ 0\nTest Statistic:\nt_0 = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{\\hat{\\beta}_1}{\\sqrt{MS_E/S_{xx}}}\nwhere MS_E = \\frac{SS_E}{n-2} and SS_E = S_{yy} - \\hat{\\beta}_1 S_{xy}\nTest for Intercept:\nH₀: β₀ = 0 vs H₁: β₀ ≠ 0\nTest Statistic:\nt_0 = \\frac{\\hat{\\beta}_0}{SE(\\hat{\\beta}_0)} = \\frac{\\hat{\\beta}_0}{\\sqrt{MS_E(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}})}}\nANOVA Table for Regression:\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nRegression\nSS_R = \\hat{\\beta}_1 S_{xy}\n1\nMS_R\nMS_R/MS_E\n\n\nError\nSS_E = S_{yy} - \\hat{\\beta}_1 S_{xy}\nn-2\nMS_E\n\n\n\nTotal\nSS_T = S_{yy}\nn-1\n\n\n\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Hypothesis Test Results:\"\n\n\n                   Test Null_Hypothesis Test_Statistic P_Value Critical_Value\n                 &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;          &lt;num&gt;\n1:         Slope β₁ = 0          β₁ = 0        16.0739  0.0000          2.101\n2:     Intercept β₀ = 0          β₀ = 0         3.4605  0.0028          2.101\n3: Overall Model F-test          β₁ = 0       258.3704  0.0000          4.414\n    Decision                               Conclusion\n      &lt;char&gt;                                   &lt;char&gt;\n1: Reject H₀          Significant linear relationship\n2: Reject H₀ Intercept significantly different from 0\n3: Reject H₀                     Model is significant\n\n\n[1] \"ANOVA Table:\"\n\n\n       Source       SS    df       MS F_Statistic P_Value\n       &lt;char&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 14948.90     1 14948.90    258.3704       0\n2:      Error  1041.45    18    57.86          NA      NA\n3:      Total 15990.35    19       NA          NA      NA\n\n\n\n\n\n# Hypothesis testing for regression parameters\nalpha &lt;- 0.05\ndf_error &lt;- n - 2\nt_critical &lt;- qt(1 - alpha / 2, df_error)\n\n# Standard errors\nSE_beta1 &lt;- sqrt(MSE / Sxx)\nSE_beta0 &lt;- sqrt(MSE * (1 / n + x_bar^2 / Sxx))\n\n# Test for slope (β₁ = 0)\nt_stat_beta1 &lt;- beta1_hat / SE_beta1\np_value_beta1 &lt;- 2 * (1 - pt(abs(t_stat_beta1), df_error))\n\n# Test for intercept (β₀ = 0)\nt_stat_beta0 &lt;- beta0_hat / SE_beta0\np_value_beta0 &lt;- 2 * (1 - pt(abs(t_stat_beta0), df_error))\n\n# F-test for overall significance\nF_stat &lt;- MSR / MSE\np_value_F &lt;- 1 - pf(F_stat, 1, df_error)\n\n# Hypothesis test results\nhypothesis_tests &lt;- data.table(\n  Test = c(\"Slope β₁ = 0\", \"Intercept β₀ = 0\", \"Overall Model F-test\"),\n  Null_Hypothesis = c(\"β₁ = 0\", \"β₀ = 0\", \"β₁ = 0\"),\n  Test_Statistic = c(round(t_stat_beta1, 4), round(t_stat_beta0, 4), round(F_stat, 4)),\n  P_Value = c(round(p_value_beta1, 4), round(p_value_beta0, 4), round(p_value_F, 4)),\n  Critical_Value = c(\n    round(t_critical, 3), round(t_critical, 3),\n    round(qf(1 - alpha, 1, df_error), 3)\n  ),\n  Decision = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_F &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Conclusion = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Significant linear relationship\", \"No significant relationship\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Intercept significantly different from 0\", \"Intercept not significant\"),\n    ifelse(p_value_F &lt; alpha, \"Model is significant\", \"Model is not significant\")\n  )\n)\n\nprint(\"Hypothesis Test Results:\")\nprint(hypothesis_tests)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR, 2), round(SSE, 2), round(SST, 2)),\n  df = c(1, df_error, n - 1),\n  MS = c(round(MSR, 2), round(MSE, 2), NA),\n  F_Statistic = c(round(F_stat, 4), NA, NA),\n  P_Value = c(round(p_value_F, 4), NA, NA)\n)\n\nprint(\"ANOVA Table:\")\nprint(anova_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Intervals\n\n\n\nFor Slope β₁:\n\\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\sqrt{\\frac{MS_E}{S_{xx}}}\nFor Intercept β₀:\n\\hat{\\beta}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)}\nFor Mean Response at x₀:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nCoefficient of Determination:\nR^2 = \\frac{SS_R}{SS_T} = \\frac{\\hat{\\beta}_1 S_{xy}}{S_{yy}} = 1 - \\frac{SS_E}{SS_T}\nR^2 represents the proportion of total variation explained by the regression.\n\n\n\n\n\n\n\n\n\n\n\nNotePrediction Intervals\n\n\n\nFor predicting a single future observation at x₀:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nKey Differences:\n\nConfidence Interval: For the mean response (narrower)\nPrediction Interval: For individual observation (wider due to additional uncertainty)\n\nThe extra “1” in the prediction interval accounts for the variability of individual observations around the mean response.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Mean Response:\"\n\n\n   Hardness Predicted CI_Lower CI_Upper CI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   116.75   130.63    13.88\n2: 50.83012    144.16   139.31   149.00     9.69\n3: 59.97786    164.62   161.00   168.24     7.25\n4: 69.12560    185.08   180.95   189.22     8.26\n5: 78.27333    205.55   199.61   211.49    11.89\n\n\n[1] \"95% Prediction Intervals for Individual Observations:\"\n\n\n   Hardness Predicted PI_Lower PI_Upper PI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   106.27   141.12    34.85\n2: 50.83012    144.16   127.46   160.86    33.40\n3: 59.97786    164.62   148.23   181.01    32.77\n4: 69.12560    185.08   168.58   201.59    33.01\n5: 78.27333    205.55   188.50   222.60    34.10\n\n\n[1] \"Interval Width Comparison:\"\n\n\n   Hardness CI_Width PI_Width PI_vs_CI_Ratio\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1: 41.68238    13.88    34.85           2.51\n2: 50.83012     9.69    33.40           3.45\n3: 59.97786     7.25    32.77           4.52\n4: 69.12560     8.26    33.01           4.00\n5: 78.27333    11.89    34.10           2.87\n\n\n\n\n\n# Confidence and prediction intervals\n# Choose some specific x values for demonstration\nx_new &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 5)\n\n# Confidence intervals for mean response\nCI_results &lt;- data.table()\nPI_results &lt;- data.table()\n\nfor (x0 in x_new) {\n  # Predicted value\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval for mean response\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_CI &lt;- t_critical * SE_mean\n  CI_lower &lt;- y_hat - margin_CI\n  CI_upper &lt;- y_hat + margin_CI\n\n  # Prediction interval for individual observation\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_PI &lt;- t_critical * SE_pred\n  PI_lower &lt;- y_hat - margin_PI\n  PI_upper &lt;- y_hat + margin_PI\n\n  CI_results &lt;- rbind(CI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    CI_Lower = round(CI_lower, 2),\n    CI_Upper = round(CI_upper, 2),\n    CI_Width = round(CI_upper - CI_lower, 2)\n  ))\n\n  PI_results &lt;- rbind(PI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    PI_Lower = round(PI_lower, 2),\n    PI_Upper = round(PI_upper, 2),\n    PI_Width = round(PI_upper - PI_lower, 2)\n  ))\n}\n\nprint(\"95% Confidence Intervals for Mean Response:\")\nprint(CI_results)\n\nprint(\"95% Prediction Intervals for Individual Observations:\")\nprint(PI_results)\n\n# Compare CI and PI widths\ninterval_comparison &lt;- data.table(\n  Hardness = x_new,\n  CI_Width = CI_results$CI_Width,\n  PI_Width = PI_results$PI_Width,\n  PI_vs_CI_Ratio = round(PI_results$PI_Width / CI_results$CI_Width, 2)\n)\n\nprint(\"Interval Width Comparison:\")\nprint(interval_comparison)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Confidence and prediction intervals\n\n\n\n\n\n\n\n\n# Confidence and prediction intervals visualization\n# Create fine grid for smooth curves\nx_grid &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 100)\ninterval_data &lt;- data.table()\n\nfor (x0 in x_grid) {\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  CI_lower &lt;- y_hat - t_critical * SE_mean\n  CI_upper &lt;- y_hat + t_critical * SE_mean\n\n  # Prediction interval\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  PI_lower &lt;- y_hat - t_critical * SE_pred\n  PI_upper &lt;- y_hat + t_critical * SE_pred\n\n  interval_data &lt;- rbind(interval_data, data.table(\n    hardness = x0,\n    fitted = y_hat,\n    CI_lower = CI_lower,\n    CI_upper = CI_upper,\n    PI_lower = PI_lower,\n    PI_upper = PI_upper\n  ))\n}\n\nggplot() +\n  # Prediction intervals (wider, lighter)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = PI_lower, ymax = PI_upper),\n    alpha = 0.2, fill = \"red\"\n  ) +\n  # Confidence intervals (narrower, darker)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = CI_lower, ymax = CI_upper),\n    alpha = 0.3, fill = \"blue\"\n  ) +\n  # Regression line\n  geom_line(data = interval_data, aes(x = hardness, y = fitted), color = \"black\", linewidth = 1) +\n  # Data points\n  geom_point(data = steel_data, aes(x = hardness, y = strength), size = 3, alpha = 0.7) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Confidence and Prediction Intervals\",\n    subtitle = \"Blue = 95% CI for mean response, Red = 95% PI for individual observations\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteResidual Analysis\n\n\n\nResiduals: e_i = y_i - \\hat{y}_i\nStandardized Residuals: d_i = \\frac{e_i}{\\sqrt{MS_E}}\nStudentized Residuals: More accurate for outlier detection\nKey Diagnostic Plots:\n\nResiduals vs. Fitted Values: Check for constant variance\nNormal Q-Q Plot: Check normality assumption\nResiduals vs. Order: Check for independence\nResiduals vs. Predictor: Check linearity assumption\n\nWhat to Look For:\n\nRandom scatter (no patterns)\nConstant spread across fitted values\nPoints following straight line in Q-Q plot\nNo obvious trends or cycles\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Residual Analysis Summary:\"\n\n\n   Mean_Residuals SD_Residuals Min_Residuals Max_Residuals Mean_Std_Residuals\n            &lt;num&gt;        &lt;num&gt;         &lt;num&gt;         &lt;num&gt;              &lt;num&gt;\n1:   7.105427e-15     7.403595     -11.27576      15.14077       9.325873e-16\n   SD_Std_Residuals\n              &lt;num&gt;\n1:        0.9733285\n\n\n[1] \"No obvious outliers detected (|std residual| &gt; 2)\"\n\n\n[1] \"Shapiro-Wilk normality test for residuals:\"\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.96347, p-value = 0.6152\n\n\n[1] \"Durbin-Watson statistic: 1.7519\"\n\n\n[1] \"(Values near 2 indicate no autocorrelation)\"\n\n\n\n\n\n# Model adequacy checking - residual analysis\n# Calculate residuals and standardized residuals\nresiduals_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  standardized_residuals = residuals / sqrt(MSE),\n  observation = 1:n,\n  hardness = steel_data$hardness\n)\n\n# Summary statistics for residuals\nresidual_summary &lt;- residuals_data %&gt;%\n  fsummarise(\n    Mean_Residuals = fmean(residuals),\n    SD_Residuals = fsd(residuals),\n    Min_Residuals = fmin(residuals),\n    Max_Residuals = fmax(residuals),\n    Mean_Std_Residuals = fmean(standardized_residuals),\n    SD_Std_Residuals = fsd(standardized_residuals)\n  )\n\nprint(\"Residual Analysis Summary:\")\nprint(residual_summary)\n\n# Check for outliers (|standardized residual| &gt; 2)\noutliers &lt;- residuals_data[abs(standardized_residuals) &gt; 2]\nif (nrow(outliers) &gt; 0) {\n  print(\"Potential outliers (|std residual| &gt; 2):\")\n  print(outliers)\n} else {\n  print(\"No obvious outliers detected (|std residual| &gt; 2)\")\n}\n\n# Normality test for residuals\nshapiro_test &lt;- shapiro.test(residuals)\nprint(\"Shapiro-Wilk normality test for residuals:\")\nprint(shapiro_test)\n\n# Durbin-Watson test for independence (if data has natural order)\ndw_stat &lt;- sum(diff(residuals)^2) / sum(residuals^2)\nprint(paste(\"Durbin-Watson statistic:\", round(dw_stat, 4)))\nprint(\"(Values near 2 indicate no autocorrelation)\")\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: Residual analysis plots\n\n\n\n\n\n\n\n\n# Residual analysis plots\n# 1. Residuals vs Fitted\np1 &lt;- ggplot(data = residuals_data, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Normal Q-Q plot\np2 &lt;- ggplot(data = residuals_data, aes(sample = standardized_residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    x = \"Theoretical Quantiles\", y = \"Sample Quantiles\",\n    title = \"Normal Q-Q Plot of Residuals\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Scale-location plot\np3 &lt;- ggplot(data = residuals_data, aes(x = fitted, y = sqrt(abs(standardized_residuals)))) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    x = \"Fitted Values\", y = \"√|Standardized Residuals|\",\n    title = \"Scale-Location Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Residuals vs Order\np4 &lt;- ggplot(data = residuals_data, aes(x = observation, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_line(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Observation Order\", y = \"Residuals\", title = \"Residuals vs Order\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine all diagnostic plots\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCorrelation vs. Regression\n\n\n\nCorrelation Coefficient:\nr = \\frac{S_{xy}}{\\sqrt{S_{xx} S_{yy}}}\nRelationship to Regression:\n\nR^2 = r^2 in simple linear regression\nCorrelation measures strength of linear relationship\nRegression provides prediction equation\n\nImportant Notes:\n\nCorrelation ≠ Causation\nBoth assume linear relationship\nSensitive to outliers\n-1 \\leq r \\leq 1\n\nInterpretation of |r|:\n\n0.0-0.3: Weak relationship\n0.3-0.7: Moderate relationship\n0.7-1.0: Strong relationship\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Correlation vs Regression Analysis:\"\n\n\n                       Measure  Value                          Interpretation\n                        &lt;char&gt;  &lt;num&gt;                                  &lt;char&gt;\n1: Correlation coefficient (r) 0.9669              Strong linear relationship\n2:                          r² 0.9349                  93.5 % shared variance\n3:          R² from regression 0.9349 93.5 % variance explained by regression\n4:        Difference |r² - R²| 0.0000                       Perfect agreement\n\n\n[1] \"Correlation Significance Test:\"\n\n\n                       Test                             H0 t_statistic p_value\n                     &lt;char&gt;                         &lt;char&gt;       &lt;num&gt;   &lt;num&gt;\n1: Correlation significance ρ = 0 (no linear relationship)     16.0739       0\n    decision              conclusion\n      &lt;char&gt;                  &lt;char&gt;\n1: Reject H₀ Significant correlation\n\n\n\n\n\n# Correlation analysis\ncorrelation_coef &lt;- cor(steel_data$hardness, steel_data$strength)\ncorrelation_squared &lt;- correlation_coef^2\n\n# Verify relationship between correlation and R-squared\ncorrelation_results &lt;- data.table(\n  Measure = c(\"Correlation coefficient (r)\", \"r²\", \"R² from regression\", \"Difference |r² - R²|\"),\n  Value = c(\n    round(correlation_coef, 4),\n    round(correlation_squared, 4),\n    round(R_squared, 4),\n    round(abs(correlation_squared - R_squared), 6)\n  ),\n  Interpretation = c(\n    ifelse(abs(correlation_coef) &gt; 0.7, \"Strong linear relationship\",\n      ifelse(abs(correlation_coef) &gt; 0.3, \"Moderate linear relationship\", \"Weak linear relationship\")\n    ),\n    paste(round(correlation_squared * 100, 1), \"% shared variance\"),\n    paste(round(R_squared * 100, 1), \"% variance explained by regression\"),\n    ifelse(abs(correlation_squared - R_squared) &lt; 0.001, \"Perfect agreement\", \"Some difference\")\n  )\n)\n\nprint(\"Correlation vs Regression Analysis:\")\nprint(correlation_results)\n\n# Test significance of correlation\nt_stat_cor &lt;- correlation_coef * sqrt((n - 2) / (1 - correlation_coef^2))\np_value_cor &lt;- 2 * (1 - pt(abs(t_stat_cor), n - 2))\n\ncorrelation_test &lt;- data.table(\n  Test = \"Correlation significance\",\n  H0 = \"ρ = 0 (no linear relationship)\",\n  t_statistic = round(t_stat_cor, 4),\n  p_value = round(p_value_cor, 4),\n  decision = ifelse(p_value_cor &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n  conclusion = ifelse(p_value_cor &lt; alpha, \"Significant correlation\", \"No significant correlation\")\n)\n\nprint(\"Correlation Significance Test:\")\nprint(correlation_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMultiple Linear Regression Model\n\n\n\nThe multiple regression model with k predictors:\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik} + \\epsilon_i\nMatrix Form:\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nwhere:\n\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\nLeast Squares Solution:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nProperties:\n\nUnbiased: E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\nMinimum variance among linear unbiased estimators\nNormally distributed under normality assumption\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA chemical engineer studies how reaction temperature and catalyst concentration affect yield:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Chemical Process Data Summary:\"\n\n\n       n Temp_Mean  Temp_SD Conc_Mean  Conc_SD Yield_Mean Yield_SD\n   &lt;int&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;      &lt;num&gt;    &lt;num&gt;\n1:    25  175.6184 13.62157  21.08764 6.724034   192.3572 14.55564\n\n\n[1] \"Correlation Matrix:\"\n\n\n              temperature concentration  yield\ntemperature        1.0000       -0.0674 0.7262\nconcentration     -0.0674        1.0000 0.5652\nyield              0.7262        0.5652 1.0000\n\n\n\n\n\n# Multiple regression example - Chemical process yield\nset.seed(456)\nchemical_data &lt;- data.table(\n  run = 1:25,\n  temperature = runif(25, min = 150, max = 200),\n  concentration = runif(25, min = 10, max = 30),\n  yield = NULL\n)\n\n# Create realistic multiple regression relationship\nchemical_data[, yield := 20 + 0.8 * temperature + 1.5 * concentration + rnorm(25, mean = 0, sd = 5)]\n\n# Summary statistics\nchemical_summary &lt;- chemical_data %&gt;%\n  fsummarise(\n    n = fnobs(run),\n    Temp_Mean = fmean(temperature),\n    Temp_SD = fsd(temperature),\n    Conc_Mean = fmean(concentration),\n    Conc_SD = fsd(concentration),\n    Yield_Mean = fmean(yield),\n    Yield_SD = fsd(yield)\n  )\n\nprint(\"Chemical Process Data Summary:\")\nprint(chemical_summary)\n\n# Correlation matrix\ncor_matrix &lt;- cor(chemical_data[, .(temperature, concentration, yield)])\nprint(\"Correlation Matrix:\")\nprint(round(cor_matrix, 4))\n\n# Write data to CSV\nfwrite(chemical_data, \"data/chemical_yield.csv\")\n\n\n\n\n\n\n\nMultiple Regression Model:\nYield = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Concentration + \\epsilon\nManual Matrix Calculation:\n\nDesign Matrix X: \\mathbf{X} = \\begin{bmatrix}\n1 & x_{1,temp} & x_{1,conc} \\\\\n1 & x_{2,temp} & x_{2,conc} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{n,temp} & x_{n,conc}\n\\end{bmatrix}\nParameter Estimates: \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Manual Multiple Regression Results:\"\n\n\n            Parameter Estimate Std_Error t_value p_value\n               &lt;char&gt;    &lt;num&gt;     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n1:     β₀ (Intercept)  20.1051   12.8589  1.5635  0.1322\n2:   β₁ (Temperature)   0.8205    0.0699 11.7400  0.0000\n3: β₂ (Concentration)   1.3355    0.1416  9.4333  0.0000\n\n\n[1] \"Model Summary Statistics:\"\n\n\n     Statistic    Value\n        &lt;char&gt;    &lt;num&gt;\n1:          R²   0.9063\n2: Adjusted R²   0.8978\n3: Residual SE   4.6530\n4: F-statistic 106.4279\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = yield ~ temperature + concentration, data = chemical_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6632 -1.8041 -0.8522  1.3582 10.2857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.10507   12.85893   1.564    0.132    \ntemperature    0.82047    0.06989  11.740 6.05e-11 ***\nconcentration  1.33553    0.14158   9.433 3.45e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.653 on 22 degrees of freedom\nMultiple R-squared:  0.9063,    Adjusted R-squared:  0.8978 \nF-statistic: 106.4 on 2 and 22 DF,  p-value: 4.873e-12\n\n\n\n\n\n# Multiple regression - manual matrix calculations\n# Design matrix X\nn_multi &lt;- nrow(chemical_data)\nX &lt;- cbind(1, chemical_data$temperature, chemical_data$concentration)\ny &lt;- chemical_data$yield\n\n# Manual matrix calculations\nXtX &lt;- t(X) %*% X\nXty &lt;- t(X) %*% y\nbeta_hat &lt;- solve(XtX) %*% Xty\n\n# Extract coefficients\nbeta0_multi &lt;- beta_hat[1, 1]\nbeta1_multi &lt;- beta_hat[2, 1]\nbeta2_multi &lt;- beta_hat[3, 1]\n\n# Calculate fitted values and residuals\ny_fitted &lt;- X %*% beta_hat\nresiduals_multi &lt;- y - y_fitted\n\n# Sum of squares\nSST_multi &lt;- sum((y - mean(y))^2)\nSSE_multi &lt;- sum(residuals_multi^2)\nSSR_multi &lt;- SST_multi - SSE_multi\n\n# Degrees of freedom\nk &lt;- 2 # number of predictors\ndf_regression &lt;- k\ndf_error_multi &lt;- n_multi - k - 1\ndf_total &lt;- n_multi - 1\n\n# Mean squares\nMSR_multi &lt;- SSR_multi / df_regression\nMSE_multi &lt;- SSE_multi / df_error_multi\n\n# R-squared and adjusted R-squared\nR_squared_multi &lt;- SSR_multi / SST_multi\nR_squared_adj &lt;- 1 - (SSE_multi / df_error_multi) / (SST_multi / df_total)\n\n# Standard errors of coefficients\nvar_covar_matrix &lt;- MSE_multi * solve(XtX)\nSE_beta0_multi &lt;- sqrt(var_covar_matrix[1, 1])\nSE_beta1_multi &lt;- sqrt(var_covar_matrix[2, 2])\nSE_beta2_multi &lt;- sqrt(var_covar_matrix[3, 3])\n\n# Manual multiple regression results\nmultiple_manual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Estimate = c(round(beta0_multi, 4), round(beta1_multi, 4), round(beta2_multi, 4)),\n  Std_Error = c(round(SE_beta0_multi, 4), round(SE_beta1_multi, 4), round(SE_beta2_multi, 4)),\n  t_value = c(\n    round(beta0_multi / SE_beta0_multi, 4),\n    round(beta1_multi / SE_beta1_multi, 4),\n    round(beta2_multi / SE_beta2_multi, 4)\n  ),\n  p_value = c(\n    round(2 * (1 - pt(abs(beta0_multi / SE_beta0_multi), df_error_multi)), 4),\n    round(2 * (1 - pt(abs(beta1_multi / SE_beta1_multi), df_error_multi)), 4),\n    round(2 * (1 - pt(abs(beta2_multi / SE_beta2_multi), df_error_multi)), 4)\n  )\n)\n\nprint(\"Manual Multiple Regression Results:\")\nprint(multiple_manual_results)\n\n# Model summary statistics\nmodel_stats &lt;- data.table(\n  Statistic = c(\"R²\", \"Adjusted R²\", \"Residual SE\", \"F-statistic\"),\n  Value = c(\n    round(R_squared_multi, 4),\n    round(R_squared_adj, 4),\n    round(sqrt(MSE_multi), 4),\n    round(MSR_multi / MSE_multi, 4)\n  )\n)\n\nprint(\"Model Summary Statistics:\")\nprint(model_stats)\n\n# Verify with R's lm function\nlm_multi &lt;- lm(yield ~ temperature + concentration, data = chemical_data)\nprint(\"R's lm() verification:\")\nprint(summary(lm_multi))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 4: Multiple regression visualization\n\n\n\n\n\n\n\n\n# Multiple regression visualization\n# Create 3D-like visualization using different perspectives\n\n# Partial regression plots\np1 &lt;- ggplot(data = chemical_data, aes(x = temperature, y = yield)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(x = \"Temperature\", y = \"Yield\", title = \"Yield vs Temperature\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2 &lt;- ggplot(data = chemical_data, aes(x = concentration, y = yield)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(x = \"Concentration\", y = \"Yield\", title = \"Yield vs Concentration\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Fitted vs actual\nfitted_multi &lt;- as.vector(y_fitted)\np3 &lt;- ggplot(\n  data = data.table(fitted = fitted_multi, actual = y),\n  aes(x = fitted, y = actual)\n) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Actual Values\", title = \"Fitted vs Actual\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Residuals vs fitted\nresiduals_multi_vec &lt;- as.vector(residuals_multi)\np4 &lt;- ggplot(\n  data = data.table(fitted = fitted_multi, residuals = residuals_multi_vec),\n  aes(x = fitted, y = residuals)\n) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Testing in Multiple Regression\n\n\n\nOverall Significance Test:\nH₀: β₁ = β₂ = … = βₖ = 0 vs H₁: At least one βⱼ ≠ 0\nF-Test:\nF_0 = \\frac{MS_R}{MS_E} = \\frac{SS_R/k}{SS_E/(n-k-1)}\nIndividual Parameter Tests:\nH₀: βⱼ = 0 vs H₁: βⱼ ≠ 0\nt-Test:\nt_0 = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\nANOVA Table:\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nRegression\nSS_R\nk\nMS_R\nMS_R/MS_E\n\n\nError\nSS_E\nn-k-1\nMS_E\n\n\n\nTotal\nSS_T\nn-1\n\n\n\n\n\nAdjusted R²:\nR_{adj}^2 = 1 - \\frac{SS_E/(n-k-1)}{SS_T/(n-1)} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Multiple Regression Hypothesis Test Results:\"\n\n\n            Parameter Test_Type Null_Hypothesis Test_Statistic P_Value\n               &lt;char&gt;    &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;\n1:      Overall Model    F-test     β₁ = β₂ = 0       106.4279  0.0000\n2:     β₀ (Intercept)    t-test          β₀ = 0         1.5635  0.1322\n3:   β₁ (Temperature)    t-test          β₁ = 0        11.7400  0.0000\n4: β₂ (Concentration)    t-test          β₂ = 0         9.4333  0.0000\n            Decision                   Interpretation\n              &lt;char&gt;                           &lt;char&gt;\n1:         Reject H₀             Model is significant\n2: Fail to reject H₀        Intercept not significant\n3:         Reject H₀   Temperature effect significant\n4:         Reject H₀ Concentration effect significant\n\n\n[1] \"Multiple Regression ANOVA Table:\"\n\n\n       Source      SS    df      MS F_Statistic P_Value\n       &lt;char&gt;   &lt;num&gt; &lt;num&gt;   &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 4608.48     2 2304.24    106.4279       0\n2:      Error  476.32    22   21.65          NA      NA\n3:      Total 5084.80    24      NA          NA      NA\n\n\n[1] \"95% Confidence Intervals for Parameters:\"\n\n\nError in Math.data.frame(structure(list(Parameter = c(\"β₀ (Intercept)\", : non-numeric-alike variable(s) in data frame: Parameter\n\n\n\n\n\n# Multiple regression inference\nalpha_multi &lt;- 0.05\nt_critical_multi &lt;- qt(1 - alpha_multi / 2, df_error_multi)\nF_critical_multi &lt;- qf(1 - alpha_multi, df_regression, df_error_multi)\n\n# Overall F-test\nF_stat_multi &lt;- MSR_multi / MSE_multi\np_value_F_multi &lt;- 1 - pf(F_stat_multi, df_regression, df_error_multi)\n\n# Individual t-tests\nt_stats &lt;- c(\n  beta0_multi / SE_beta0_multi,\n  beta1_multi / SE_beta1_multi,\n  beta2_multi / SE_beta2_multi\n)\n\np_values_t &lt;- 2 * (1 - pt(abs(t_stats), df_error_multi))\n\n# Hypothesis test results\nhypothesis_tests_multi &lt;- data.table(\n  Parameter = c(\"Overall Model\", \"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Test_Type = c(\"F-test\", \"t-test\", \"t-test\", \"t-test\"),\n  Null_Hypothesis = c(\"β₁ = β₂ = 0\", \"β₀ = 0\", \"β₁ = 0\", \"β₂ = 0\"),\n  Test_Statistic = c(round(F_stat_multi, 4), round(t_stats, 4)),\n  P_Value = c(round(p_value_F_multi, 4), round(p_values_t, 4)),\n  Decision = c(\n    ifelse(p_value_F_multi &lt; alpha_multi, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_values_t &lt; alpha_multi, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Interpretation = c(\n    ifelse(p_value_F_multi &lt; alpha_multi, \"Model is significant\", \"Model not significant\"),\n    ifelse(p_values_t[1] &lt; alpha_multi, \"Intercept significant\", \"Intercept not significant\"),\n    ifelse(p_values_t[2] &lt; alpha_multi, \"Temperature effect significant\", \"Temperature not significant\"),\n    ifelse(p_values_t[3] &lt; alpha_multi, \"Concentration effect significant\", \"Concentration not significant\")\n  )\n)\n\nprint(\"Multiple Regression Hypothesis Test Results:\")\nprint(hypothesis_tests_multi)\n\n# ANOVA table for multiple regression\nanova_table_multi &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR_multi, 2), round(SSE_multi, 2), round(SST_multi, 2)),\n  df = c(df_regression, df_error_multi, df_total),\n  MS = c(round(MSR_multi, 2), round(MSE_multi, 2), NA),\n  F_Statistic = c(round(F_stat_multi, 4), NA, NA),\n  P_Value = c(round(p_value_F_multi, 4), NA, NA)\n)\n\nprint(\"Multiple Regression ANOVA Table:\")\nprint(anova_table_multi)\n\n# Confidence intervals for parameters\nCI_multi &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Estimate = c(beta0_multi, beta1_multi, beta2_multi),\n  Lower_95CI = c(\n    beta0_multi - t_critical_multi * SE_beta0_multi,\n    beta1_multi - t_critical_multi * SE_beta1_multi,\n    beta2_multi - t_critical_multi * SE_beta2_multi\n  ),\n  Upper_95CI = c(\n    beta0_multi + t_critical_multi * SE_beta0_multi,\n    beta1_multi + t_critical_multi * SE_beta1_multi,\n    beta2_multi + t_critical_multi * SE_beta2_multi\n  )\n)\n\nprint(\"95% Confidence Intervals for Parameters:\")\nprint(round(CI_multi, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMultiple Regression Diagnostics\n\n\n\nKey Diagnostic Plots:\n\nResiduals vs. Fitted: Check linearity and constant variance\nNormal Q-Q Plot: Check normality of residuals\nScale-Location: Check homoscedasticity\nResiduals vs. Leverage: Identify influential points\n\nMulticollinearity Detection:\n\nVariance Inflation Factor (VIF):\n\nVIF_j = \\frac{1}{1-R_j^2}\nwhere R_j^2 is the R² from regressing x_j on other predictors\n\nRule of thumb: VIF &gt; 10 indicates serious multicollinearity\n\nInfluential Observations:\n\nLeverage: h_{ii} (diagonal elements of hat matrix)\nCook’s Distance: Measures influence on fitted values\nStudentized Residuals: Better for outlier detection\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Multiple Regression Diagnostic Summary:\"\n\n\n                    Diagnostic  Value Threshold            Concern\n                        &lt;char&gt;  &lt;num&gt;    &lt;char&gt;             &lt;char&gt;\n1: Max |Standardized Residual| 2.2105       2-3 Potential outliers\n2:  Max |Studentized Residual| 2.3367       2-3 Potential outliers\n3:                Max Leverage 0.1960    &gt; 0.24                 OK\n4:         Max Cook's Distance 0.2256         1                 OK\n5:             VIF Temperature 1.0046        10                 OK\n6:           VIF Concentration 1.0046        10                 OK\n\n\n[1] \"Flagged observations:\"\n   observation standardized_resid studentized_resid leverage cooks_distance\n         &lt;int&gt;              &lt;num&gt;             &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1:           8             -1.862            -1.989    0.124          0.163\n2:          11              2.211             2.337    0.105          0.191\n3:          20              1.895             2.066    0.159          0.226\n   outlier high_leverage influential\n    &lt;lgcl&gt;        &lt;lgcl&gt;      &lt;lgcl&gt;\n1:   FALSE         FALSE        TRUE\n2:    TRUE         FALSE        TRUE\n3:   FALSE         FALSE        TRUE\n\n\n\n\n\n# Multiple regression diagnostics\n# Calculate leverages (diagonal elements of hat matrix)\nH &lt;- X %*% solve(XtX) %*% t(X)\nleverages &lt;- diag(H)\n\n# Standardized and studentized residuals\nstandardized_resid_multi &lt;- residuals_multi_vec / sqrt(MSE_multi)\nstudentized_resid_multi &lt;- residuals_multi_vec / sqrt(MSE_multi * (1 - leverages))\n\n# Cook's distance\ncooks_distance &lt;- (standardized_resid_multi^2 / (k + 1)) * (leverages / (1 - leverages))\n\n# VIF calculations (for multicollinearity)\n# VIF for temperature (regress temp on concentration)\ntemp_on_conc &lt;- lm(temperature ~ concentration, data = chemical_data)\nR2_temp &lt;- summary(temp_on_conc)$r.squared\nVIF_temp &lt;- 1 / (1 - R2_temp)\n\n# VIF for concentration (regress conc on temperature)\nconc_on_temp &lt;- lm(concentration ~ temperature, data = chemical_data)\nR2_conc &lt;- summary(conc_on_temp)$r.squared\nVIF_conc &lt;- 1 / (1 - R2_conc)\n\n# Diagnostic summary\ndiagnostic_summary &lt;- data.table(\n  Diagnostic = c(\n    \"Max |Standardized Residual|\", \"Max |Studentized Residual|\",\n    \"Max Leverage\", \"Max Cook's Distance\", \"VIF Temperature\", \"VIF Concentration\"\n  ),\n  Value = c(\n    round(max(abs(standardized_resid_multi)), 4),\n    round(max(abs(studentized_resid_multi)), 4),\n    round(max(leverages), 4),\n    round(max(cooks_distance), 4),\n    round(VIF_temp, 4),\n    round(VIF_conc, 4)\n  ),\n  Threshold = c(\"2-3\", \"2-3\", paste(\"&gt;\", round(2 * (k + 1) / n_multi, 3)), \"1\", \"10\", \"10\"),\n  Concern = c(\n    ifelse(max(abs(standardized_resid_multi)) &gt; 2, \"Potential outliers\", \"OK\"),\n    ifelse(max(abs(studentized_resid_multi)) &gt; 2, \"Potential outliers\", \"OK\"),\n    ifelse(max(leverages) &gt; 2 * (k + 1) / n_multi, \"High leverage points\", \"OK\"),\n    ifelse(max(cooks_distance) &gt; 1, \"Influential observations\", \"OK\"),\n    ifelse(VIF_temp &gt; 10, \"Serious multicollinearity\", \"OK\"),\n    ifelse(VIF_conc &gt; 10, \"Serious multicollinearity\", \"OK\")\n  )\n)\n\nprint(\"Multiple Regression Diagnostic Summary:\")\nprint(diagnostic_summary)\n\n# Identify specific problematic observations\nproblem_obs &lt;- data.table(\n  observation = 1:n_multi,\n  standardized_resid = round(standardized_resid_multi, 3),\n  studentized_resid = round(studentized_resid_multi, 3),\n  leverage = round(leverages, 3),\n  cooks_distance = round(cooks_distance, 3)\n)\n\n# Flag potential problems\nproblem_obs[, outlier := abs(standardized_resid) &gt; 2]\nproblem_obs[, high_leverage := leverage &gt; 2 * (k + 1) / n_multi]\nproblem_obs[, influential := cooks_distance &gt; 4 / n_multi] # Conservative threshold\n\nflagged_obs &lt;- problem_obs[outlier == TRUE | high_leverage == TRUE | influential == TRUE]\n\nif (nrow(flagged_obs) &gt; 0) {\n  print(\"Flagged observations:\")\n  print(flagged_obs)\n} else {\n  print(\"No observations flagged for potential problems\")\n}\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 5: Multiple regression diagnostic plots\n\n\n\n\n\n\n\n\n# Multiple regression diagnostic plots\nmulti_diagnostic_data &lt;- data.table(\n  fitted = fitted_multi,\n  residuals = residuals_multi_vec,\n  standardized_resid = standardized_resid_multi,\n  studentized_resid = studentized_resid_multi,\n  leverage = leverages,\n  cooks_distance = cooks_distance,\n  observation = 1:n_multi\n)\n\n# 1. Residuals vs Fitted\np1_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Normal Q-Q plot\np2_multi &lt;- ggplot(data = multi_diagnostic_data, aes(sample = standardized_resid)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    x = \"Theoretical Quantiles\", y = \"Standardized Residuals\",\n    title = \"Normal Q-Q Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Scale-Location\np3_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = fitted, y = sqrt(abs(standardized_resid)))) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    x = \"Fitted Values\", y = \"√|Standardized Residuals|\",\n    title = \"Scale-Location\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Residuals vs Leverage\np4_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = leverage, y = studentized_resid)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 2 * (k + 1) / n_multi, linetype = \"dashed\", color = \"blue\") +\n  labs(\n    x = \"Leverage\", y = \"Studentized Residuals\",\n    title = \"Residuals vs Leverage\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1_multi, p2_multi, p3_multi, p4_multi, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePolynomial Regression\n\n\n\nWhen the relationship is curved, use polynomial models:\nSecond-Order (Quadratic):\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\nThird-Order (Cubic):\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon\nInterpretation:\n\nβ₁: Linear effect\nβ₂: Quadratic effect (curvature)\nβ₃: Cubic effect (inflection)\n\nModel Selection:\n\nStart with linear model\nAdd higher-order terms if needed\nUse hypothesis tests to determine necessary degree\nBe cautious of overfitting\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA process engineer studies how temperature affects reaction rate and suspects a nonlinear relationship:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Reaction Rate vs Temperature Data:\"\n\n\n    temperature      rate\n          &lt;num&gt;     &lt;num&gt;\n 1:         100  69.57229\n 2:         110  66.01070\n 3:         120  77.32496\n 4:         130  82.34042\n 5:         140  84.94795\n 6:         150  88.67155\n 7:         160  92.08906\n 8:         170  97.41559\n 9:         180  98.66312\n10:         190 107.59609\n\n\n\n\n\n# Polynomial regression example\nset.seed(789)\nreaction_data &lt;- data.table(\n  temperature = seq(100, 300, by = 10),\n  rate = NULL\n)\n\n# Create nonlinear relationship\nreaction_data[, rate := 5 + 0.8 * temperature - 0.002 * temperature^2 +\n  0.000003 * temperature^3 + rnorm(21, mean = 0, sd = 3)]\n\nprint(\"Reaction Rate vs Temperature Data:\")\nprint(head(reaction_data, 10))\n\n# Write data to CSV\nfwrite(reaction_data, \"data/reaction_rate.csv\")\n\n\n\n\n\n\n\nModel Comparison:\n\nLinear: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\epsilon\nQuadratic: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Temperature^2 + \\epsilon\nCubic: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Temperature^2 + \\beta_3 \\times Temperature^3 + \\epsilon\n\n\nR OutputR Code\n\n\n\n\n[1] \"Polynomial Model Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Linear\", \"Quadratic\", : non-numeric-alike variable(s) in data frame: Model, Formula\n\n\n[1] \"F-test: Linear vs Quadratic\"\n\n\nAnalysis of Variance Table\n\nModel 1: rate ~ temperature\nModel 2: rate ~ temperature + I(temperature^2)\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     19 103.809                              \n2     18  86.945  1    16.864 3.4914 0.07805 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[1] \"F-test: Quadratic vs Cubic\"\n\n\nAnalysis of Variance Table\n\nModel 1: rate ~ temperature + I(temperature^2)\nModel 2: rate ~ temperature + I(temperature^2) + I(temperature^3)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 86.945                           \n2     17 86.571  1   0.37395 0.0734 0.7897\n\n\n[1] \"Best model based on Adjusted R²: Quadratic\"\n\n\n[1] \"Quadratic Model Summary:\"\n\nCall:\nlm(formula = rate ~ temperature + I(temperature^2), data = reaction_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6671 -1.2600  0.1417  1.5729  3.1584 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      20.9024617  5.5825087   3.744  0.00148 ** \ntemperature       0.4917544  0.0592276   8.303 1.44e-07 ***\nI(temperature^2) -0.0002742  0.0001467  -1.869  0.07805 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.198 on 18 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9915 \nF-statistic:  1165 on 2 and 18 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Fit polynomial models of different degrees\n# Linear model\nlm_linear &lt;- lm(rate ~ temperature, data = reaction_data)\n\n# Quadratic model\nlm_quad &lt;- lm(rate ~ temperature + I(temperature^2), data = reaction_data)\n\n# Cubic model\nlm_cubic &lt;- lm(rate ~ temperature + I(temperature^2) + I(temperature^3), data = reaction_data)\n\n# Model comparison\nmodel_comparison &lt;- data.table(\n  Model = c(\"Linear\", \"Quadratic\", \"Cubic\"),\n  Formula = c(\n    \"rate ~ temperature\",\n    \"rate ~ temperature + temperature²\",\n    \"rate ~ temperature + temperature² + temperature³\"\n  ),\n  R_squared = c(\n    summary(lm_linear)$r.squared,\n    summary(lm_quad)$r.squared,\n    summary(lm_cubic)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_linear)$adj.r.squared,\n    summary(lm_quad)$adj.r.squared,\n    summary(lm_cubic)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_linear), AIC(lm_quad), AIC(lm_cubic)),\n  BIC = c(BIC(lm_linear), BIC(lm_quad), BIC(lm_cubic)),\n  F_statistic = c(\n    summary(lm_linear)$fstatistic[1],\n    summary(lm_quad)$fstatistic[1],\n    summary(lm_cubic)$fstatistic[1]\n  ),\n  p_value = c(\n    pf(summary(lm_linear)$fstatistic[1],\n      summary(lm_linear)$fstatistic[2],\n      summary(lm_linear)$fstatistic[3],\n      lower.tail = FALSE\n    ),\n    pf(summary(lm_quad)$fstatistic[1],\n      summary(lm_quad)$fstatistic[2],\n      summary(lm_quad)$fstatistic[3],\n      lower.tail = FALSE\n    ),\n    pf(summary(lm_cubic)$fstatistic[1],\n      summary(lm_cubic)$fstatistic[2],\n      summary(lm_cubic)$fstatistic[3],\n      lower.tail = FALSE\n    )\n  )\n)\n\nprint(\"Polynomial Model Comparison:\")\nprint(round(model_comparison, 4))\n\n# Sequential F-tests for additional terms\n# Test if quadratic term improves linear model\nanova_linear_quad &lt;- anova(lm_linear, lm_quad)\nprint(\"F-test: Linear vs Quadratic\")\nprint(anova_linear_quad)\n\n# Test if cubic term improves quadratic model\nanova_quad_cubic &lt;- anova(lm_quad, lm_cubic)\nprint(\"F-test: Quadratic vs Cubic\")\nprint(anova_quad_cubic)\n\n# Detailed summary of best model (based on adj R²)\nbest_model_idx &lt;- which.max(model_comparison$Adj_R_squared)\nbest_model_name &lt;- model_comparison$Model[best_model_idx]\nprint(paste(\"Best model based on Adjusted R²:\", best_model_name))\n\nif (best_model_name == \"Cubic\") {\n  print(\"Cubic Model Summary:\")\n  print(summary(lm_cubic))\n} else if (best_model_name == \"Quadratic\") {\n  print(\"Quadratic Model Summary:\")\n  print(summary(lm_quad))\n} else {\n  print(\"Linear Model Summary:\")\n  print(summary(lm_linear))\n}\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 6: Polynomial regression models comparison\n\n\n\n\n\n\n\n\n# Polynomial regression visualization\n# Create prediction data for smooth curves\ntemp_pred &lt;- seq(min(reaction_data$temperature), max(reaction_data$temperature), length.out = 100)\npred_data &lt;- data.table(temperature = temp_pred)\n\n# Predictions from each model\npred_linear &lt;- predict(lm_linear, newdata = pred_data)\npred_quad &lt;- predict(lm_quad, newdata = pred_data)\npred_cubic &lt;- predict(lm_cubic, newdata = pred_data)\n\n# Combine predictions\nplot_data &lt;- data.table(\n  temperature = rep(temp_pred, 3),\n  predicted_rate = c(pred_linear, pred_quad, pred_cubic),\n  model = rep(c(\"Linear\", \"Quadratic\", \"Cubic\"), each = length(temp_pred))\n)\n\n# Plot all models\nggplot() +\n  geom_point(\n    data = reaction_data, aes(x = temperature, y = rate),\n    size = 3, alpha = 0.7, color = \"black\"\n  ) +\n  geom_line(\n    data = plot_data, aes(x = temperature, y = predicted_rate, color = model),\n    linewidth = 1.2\n  ) +\n  scale_color_manual(values = c(\"Linear\" = \"red\", \"Quadratic\" = \"blue\", \"Cubic\" = \"green\")) +\n  labs(\n    x = \"Temperature\",\n    y = \"Reaction Rate\",\n    title = \"Polynomial Regression Models Comparison\",\n    subtitle = \"Comparing linear, quadratic, and cubic fits\",\n    color = \"Model\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDummy Variables for Categorical Predictors\n\n\n\nWhen predictors are categorical, use dummy variables:\nBinary Categorical Variable (2 levels):\nFor variable with levels A and B:\n\nx = 0 for level A\nx = 1 for level B\n\nMulti-level Categorical Variable (k levels):\nUse k-1 dummy variables:\nx_1 = \\begin{cases} 1 & \\text{if level 2} \\\\ 0 & \\text{otherwise} \\end{cases}\nx_2 = \\begin{cases} 1 & \\text{if level 3} \\\\ 0 & \\text{otherwise} \\end{cases}\n\\vdots\nInterpretation:\n\nReference level: All dummy variables = 0\nβ₀: Mean response for reference level\nβⱼ: Difference in mean response between level j+1 and reference level\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA quality engineer studies how supplier and heat treatment affect strength:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Categorical Variables Example - First 15 observations:\"\n\n\n    observation supplier heat_treatment  strength\n          &lt;int&gt;   &lt;char&gt;         &lt;char&gt;     &lt;num&gt;\n 1:           1        A             No  96.20567\n 2:           2        A            Yes 121.76799\n 3:           3        A             No  98.99465\n 4:           4        A            Yes 112.79896\n 5:           5        A             No 106.48668\n 6:           6        A            Yes 106.10137\n 7:           7        A             No  95.80303\n 8:           8        A            Yes 119.79145\n 9:           9        A             No 102.14922\n10:          10        A            Yes 117.88237\n11:          11        A             No 100.10912\n12:          12        A            Yes 115.25777\n13:          13        A             No  92.78922\n14:          14        A            Yes 130.48557\n15:          15        A             No 103.01518\n\n\n[1] \"Summary by Supplier and Heat Treatment:\"\n\n\n   supplier heat_treatment     n mean_strength sd_strength min_strength\n     &lt;char&gt;         &lt;char&gt; &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:        A             No    10      99.89853    4.317355     92.78922\n2:        A            Yes    10     117.80654    6.941354    106.10137\n3:        B             No    10     111.01700    5.039307    100.85786\n4:        B            Yes    10     125.45412    4.000820    117.65973\n5:        C             No    10     102.42515    4.899730     94.05402\n6:        C            Yes    10     131.78177    6.143025    119.69643\n   max_strength\n          &lt;num&gt;\n1:     106.4867\n2:     130.4856\n3:     117.6057\n4:     129.8654\n5:     110.6014\n6:     141.1987\n\n\n\n\n\n# Categorical variables example\nset.seed(101112)\nstrength_cat_data &lt;- data.table(\n  observation = 1:60,\n  supplier = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  heat_treatment = rep(c(\"No\", \"Yes\"), 30),\n  strength = NULL\n)\n\n# Create realistic effects\n# Base strength: 100\n# Supplier B: +10, Supplier C: +5 (vs A)\n# Heat treatment: +15\n# Some interaction: Supplier C with heat treatment gets extra +8\nstrength_cat_data[, strength := 100 +\n  ifelse(supplier == \"B\", 10, 0) +\n  ifelse(supplier == \"C\", 5, 0) +\n  ifelse(heat_treatment == \"Yes\", 15, 0) +\n  ifelse(supplier == \"C\" & heat_treatment == \"Yes\", 8, 0) +\n  rnorm(60, mean = 0, sd = 5)]\n\nprint(\"Categorical Variables Example - First 15 observations:\")\nprint(head(strength_cat_data, 15))\n\n# Summary by groups\ngroup_summary &lt;- strength_cat_data %&gt;%\n  fgroup_by(supplier, heat_treatment) %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    mean_strength = fmean(strength),\n    sd_strength = fsd(strength),\n    min_strength = fmin(strength),\n    max_strength = fmax(strength)\n  )\n\nprint(\"Summary by Supplier and Heat Treatment:\")\nprint(group_summary)\n\n# Write data to CSV\nfwrite(strength_cat_data, \"data/strength_categorical.csv\")\n\n\n\n\n\n\n\nModel with Categorical Variables:\nStrength = \\beta_0 + \\beta_1 \\times Supplier_B + \\beta_2 \\times Supplier_C + \\beta_3 \\times HeatTreat_{Yes} + \\epsilon\nInterpretation:\n\nβ₀: Mean strength for Supplier A, no heat treatment\nβ₁: Effect of Supplier B vs. Supplier A\nβ₂: Effect of Supplier C vs. Supplier A\nβ₃: Effect of heat treatment\n\n\nR OutputR Code\n\n\n\n\n[1] \"Main Effects Model:\"\n\n\n\nCall:\nlm(formula = strength ~ supplier + heat_treatment, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.0348  -4.0101   0.5405   3.7302  13.8116 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         98.569      1.596  61.766  &lt; 2e-16 ***\nsupplierB            9.383      1.954   4.801 1.22e-05 ***\nsupplierC            8.251      1.954   4.222 8.99e-05 ***\nheat_treatmentYes   20.567      1.596  12.888  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.181 on 56 degrees of freedom\nMultiple R-squared:  0.7756,    Adjusted R-squared:  0.7636 \nF-statistic: 64.52 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Model with Interaction:\"\n\n\n\nCall:\nlm(formula = strength ~ supplier * heat_treatment, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0853  -2.9002   0.1333   3.1805  12.6790 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   99.899      1.683  59.354  &lt; 2e-16 ***\nsupplierB                     11.118      2.380   4.671 2.03e-05 ***\nsupplierC                      2.527      2.380   1.061  0.29319    \nheat_treatmentYes             17.908      2.380   7.524 5.82e-10 ***\nsupplierB:heat_treatmentYes   -3.471      3.366  -1.031  0.30709    \nsupplierC:heat_treatmentYes   11.449      3.366   3.401  0.00127 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.322 on 54 degrees of freedom\nMultiple R-squared:  0.8395,    Adjusted R-squared:  0.8247 \nF-statistic: 56.51 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Manual Dummy Variable Model (should match main effects):\"\n\n\n\nCall:\nlm(formula = strength ~ supplier_B + supplier_C + heat_yes, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.0348  -4.0101   0.5405   3.7302  13.8116 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   98.569      1.596  61.766  &lt; 2e-16 ***\nsupplier_B     9.383      1.954   4.801 1.22e-05 ***\nsupplier_C     8.251      1.954   4.222 8.99e-05 ***\nheat_yes      20.567      1.596  12.888  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.181 on 56 degrees of freedom\nMultiple R-squared:  0.7756,    Adjusted R-squared:  0.7636 \nF-statistic: 64.52 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Coefficient Interpretation:\"\n\n\n      Coefficient Value                                  Interpretation\n           &lt;char&gt; &lt;num&gt;                                          &lt;char&gt;\n1:      Intercept 98.57 Mean strength for Supplier A, no heat treatment\n2:     Supplier B  9.38         Additional strength for Supplier B vs A\n3:     Supplier C  8.25         Additional strength for Supplier C vs A\n4: Heat Treatment 20.57         Additional strength from heat treatment\n\n\n[1] \"F-test for overall supplier effect:\"\n\n\nAnalysis of Variance Table\n\nModel 1: strength ~ heat_treatment\nModel 2: strength ~ supplier + heat_treatment\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     58 3188.6                                  \n2     56 2139.2  2    1049.3 13.735 1.401e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[1] \"F-test for interaction effect:\"\n\n\nAnalysis of Variance Table\n\nModel 1: strength ~ supplier + heat_treatment\nModel 2: strength ~ supplier * heat_treatment\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     56 2139.2                                  \n2     54 1529.7  2    609.52 10.758 0.0001168 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# Regression with categorical variables\n# R automatically creates dummy variables with factor variables\nstrength_cat_data[, supplier := factor(supplier)]\nstrength_cat_data[, heat_treatment := factor(heat_treatment)]\n\n# Main effects model\nlm_cat_main &lt;- lm(strength ~ supplier + heat_treatment, data = strength_cat_data)\n\n# Model with interaction\nlm_cat_interact &lt;- lm(strength ~ supplier * heat_treatment, data = strength_cat_data)\n\nprint(\"Main Effects Model:\")\nprint(summary(lm_cat_main))\n\nprint(\"Model with Interaction:\")\nprint(summary(lm_cat_interact))\n\n# Manual dummy variable creation for illustration\nstrength_cat_data[, supplier_B := ifelse(supplier == \"B\", 1, 0)]\nstrength_cat_data[, supplier_C := ifelse(supplier == \"C\", 1, 0)]\nstrength_cat_data[, heat_yes := ifelse(heat_treatment == \"Yes\", 1, 0)]\n\n# Manual model with dummy variables\nlm_manual_dummy &lt;- lm(strength ~ supplier_B + supplier_C + heat_yes, data = strength_cat_data)\n\nprint(\"Manual Dummy Variable Model (should match main effects):\")\nprint(summary(lm_manual_dummy))\n\n# Interpret coefficients\ncoef_interpretation &lt;- data.table(\n  Coefficient = c(\"Intercept\", \"Supplier B\", \"Supplier C\", \"Heat Treatment\"),\n  Value = round(coef(lm_cat_main), 2),\n  Interpretation = c(\n    \"Mean strength for Supplier A, no heat treatment\",\n    \"Additional strength for Supplier B vs A\",\n    \"Additional strength for Supplier C vs A\",\n    \"Additional strength from heat treatment\"\n  )\n)\n\nprint(\"Coefficient Interpretation:\")\nprint(coef_interpretation)\n\n# Test significance of categorical variables\n# Test overall supplier effect\nanova_supplier &lt;- anova(lm(strength ~ heat_treatment, data = strength_cat_data), lm_cat_main)\nprint(\"F-test for overall supplier effect:\")\nprint(anova_supplier)\n\n# Test interaction effect\nanova_interaction &lt;- anova(lm_cat_main, lm_cat_interact)\nprint(\"F-test for interaction effect:\")\nprint(anova_interaction)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Model Statistics Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Main Effects\", \"With Interaction\": non-numeric-alike variable(s) in data frame: Model\n\n\n\n\n\n\n\n\nFigure 7: Categorical variables in regression\n\n\n\n\n\n\n\n\n# Visualization of categorical variables\n# Box plots by groups\np1_cat &lt;- ggplot(data = strength_cat_data, aes(x = supplier, y = strength, fill = heat_treatment)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    x = \"Supplier\", y = \"Strength\", fill = \"Heat Treatment\",\n    title = \"Strength by Supplier and Heat Treatment\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Interaction plot\ninteraction_means &lt;- strength_cat_data %&gt;%\n  fgroup_by(supplier, heat_treatment) %&gt;%\n  fsummarise(mean_strength = fmean(strength))\n\np2_cat &lt;- ggplot(data = interaction_means, aes(\n  x = supplier, y = mean_strength,\n  color = heat_treatment, group = heat_treatment\n)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 4) +\n  labs(\n    x = \"Supplier\", y = \"Mean Strength\", color = \"Heat Treatment\",\n    title = \"Interaction Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Residuals from interaction model\nresid_interact &lt;- residuals(lm_cat_interact)\nfitted_interact &lt;- fitted(lm_cat_interact)\n\np3_cat &lt;- ggplot(\n  data = data.table(fitted = fitted_interact, residuals = resid_interact),\n  aes(x = fitted, y = residuals)\n) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Model comparison table\nmodel_stats_cat &lt;- data.table(\n  Model = c(\"Main Effects\", \"With Interaction\"),\n  R_squared = c(summary(lm_cat_main)$r.squared, summary(lm_cat_interact)$r.squared),\n  Adj_R_squared = c(summary(lm_cat_main)$adj.r.squared, summary(lm_cat_interact)$adj.r.squared),\n  AIC = c(AIC(lm_cat_main), AIC(lm_cat_interact)),\n  F_statistic = c(summary(lm_cat_main)$fstatistic[1], summary(lm_cat_interact)$fstatistic[1])\n)\n\nprint(\"Model Statistics Comparison:\")\nprint(round(model_stats_cat, 3))\n\ngrid.arrange(p1_cat, p2_cat, p3_cat, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteModel Selection Methods\n\n\n\nForward Selection:\n\nStart with no variables\nAdd variables one by one\nStop when no improvement\n\nBackward Elimination:\n\nStart with all variables\nRemove variables one by one\nStop when all remaining are significant\n\nStepwise Selection:\n\nCombine forward and backward\nCan add or remove at each step\nMore flexible approach\n\nSelection Criteria:\n\nAIC (Akaike Information Criterion): AIC = n \\ln(SS_E/n) + 2(k+1)\nBIC (Bayesian Information Criterion): BIC = n \\ln(SS_E/n) + (k+1)\\ln(n)\nAdjusted R²: Penalizes for additional variables\nMallows’ Cp: C_p = \\frac{SS_E}{MS_E} + 2(k+1) - n\n\nBest Practices:\n\nUse multiple criteria\nCross-validate final model\nConsider subject matter knowledge\nAvoid overfitting\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nAn engineer has multiple potential predictors for product quality and wants to identify the most important ones:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Quality Prediction Data Summary:\"\n\n\n[1] \"Variable Summary:\"\n\n\n[1] \"Correlation Matrix:\"\n\n\n              temperature pressure flow_rate     pH catalyst_conc   time\ntemperature         1.000   -0.143     0.075  0.022         0.142  0.023\npressure           -0.143    1.000    -0.035  0.369        -0.114 -0.172\nflow_rate           0.075   -0.035     1.000  0.370         0.084 -0.061\npH                  0.022    0.369     0.370  1.000         0.121 -0.140\ncatalyst_conc       0.142   -0.114     0.084  0.121         1.000  0.071\ntime                0.023   -0.172    -0.061 -0.140         0.071  1.000\nquality             0.771    0.284     0.157  0.381         0.282 -0.017\n              quality\ntemperature     0.771\npressure        0.284\nflow_rate       0.157\npH              0.381\ncatalyst_conc   0.282\ntime           -0.017\nquality         1.000\n\n\n\n\n\n# Variable selection example\nset.seed(131415)\nquality_data &lt;- data.table(\n  observation = 1:50,\n  temperature = rnorm(50, mean = 200, sd = 20),\n  pressure = rnorm(50, mean = 10, sd = 2),\n  flow_rate = rnorm(50, mean = 5, sd = 1),\n  pH = rnorm(50, mean = 7, sd = 0.5),\n  catalyst_conc = rnorm(50, mean = 0.1, sd = 0.02),\n  time = rnorm(50, mean = 60, sd = 10),\n  quality = NULL\n)\n\n# Create realistic relationship where only some variables matter\n# Important: temperature, pressure, catalyst_conc\n# Less important: pH\n# Not important: flow_rate, time\nquality_data[, quality := 50 +\n  0.3 * temperature +\n  2.0 * pressure +\n  100 * catalyst_conc +\n  3.0 * pH +\n  0.1 * flow_rate + # weak effect\n  0.05 * time + # very weak effect\n  rnorm(50, mean = 0, sd = 3)]\n\nprint(\"Quality Prediction Data Summary:\")\npredictors &lt;- c(\"temperature\", \"pressure\", \"flow_rate\", \"pH\", \"catalyst_conc\", \"time\")\nsummary_stats &lt;- quality_data[, lapply(.SD, function(x) list(mean = mean(x), sd = sd(x))),\n  .SDcols = c(predictors, \"quality\")\n]\n\nprint(\"Variable Summary:\")\nfor (var in names(summary_stats)) {\n  cat(sprintf(\n    \"%s: Mean = %.3f, SD = %.3f\\n\",\n    var, summary_stats[[var]]$mean, summary_stats[[var]]$sd\n  ))\n}\n\n# Correlation matrix\ncor_matrix_quality &lt;- cor(quality_data[, .SD, .SDcols = c(predictors, \"quality\")])\nprint(\"Correlation Matrix:\")\nprint(round(cor_matrix_quality, 3))\n\n# Write data to CSV\nfwrite(quality_data, \"data/quality_predictors.csv\")\n\n\n\n\n\n\n\nAvailable Predictors:\n\nTemperature, Pressure, Flow_Rate, pH, Catalyst_Conc, Time\n\nSelection Process:\n\nAll Possible Models: Evaluate all 2^k possible models\nStepwise Selection: Use automated procedures\nCriterion-Based: Compare AIC, BIC, Cp values\n\n\nR OutputR Code\n\n\n\n\n[1] \"Model Selection Summary:\"\n\n\nError in Math.data.frame(structure(list(Method = c(\"Full Model\", \"Stepwise AIC\", : non-numeric-alike variable(s) in data frame: Method\n\n\n[1] \"Final Selected Model Summary:\"\n\n\n\nCall:\nlm(formula = quality ~ temperature + pressure + pH + catalyst_conc, \n    data = quality_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5183 -2.0431 -0.2991  2.2480  6.2915 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    64.4306     7.9554   8.099 2.47e-10 ***\ntemperature     0.2677     0.0208  12.872  &lt; 2e-16 ***\npressure        1.3271     0.2602   5.100 6.58e-06 ***\npH              3.5314     1.0675   3.308  0.00185 ** \ncatalyst_conc  78.7116    26.5539   2.964  0.00484 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.074 on 45 degrees of freedom\nMultiple R-squared:  0.838, Adjusted R-squared:  0.8236 \nF-statistic: 58.19 on 4 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Selected Variables:\"\n\n\n[1] \"temperature\"   \"pressure\"      \"pH\"            \"catalyst_conc\"\n\n\n\n\n\n# Variable selection procedures\n\n# Full model with all predictors\nlm_full &lt;- lm(quality ~ temperature + pressure + flow_rate + pH + catalyst_conc + time,\n  data = quality_data\n)\n\n# Stepwise selection using AIC\nstep_aic &lt;- step(lm_full, direction = \"both\", trace = FALSE)\n\n# Forward selection starting from intercept only\nlm_intercept &lt;- lm(quality ~ 1, data = quality_data)\nstep_forward &lt;- step(lm_intercept,\n  scope = list(lower = lm_intercept, upper = lm_full),\n  direction = \"forward\", trace = FALSE\n)\n\n# Backward elimination starting from full model\nstep_backward &lt;- step(lm_full, direction = \"backward\", trace = FALSE)\n\n# Compare selected models\nmodel_selection_summary &lt;- data.table(\n  Method = c(\"Full Model\", \"Stepwise AIC\", \"Forward AIC\", \"Backward AIC\"),\n  R_squared = c(\n    summary(lm_full)$r.squared,\n    summary(step_aic)$r.squared,\n    summary(step_forward)$r.squared,\n    summary(step_backward)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_full)$adj.r.squared,\n    summary(step_aic)$adj.r.squared,\n    summary(step_forward)$adj.r.squared,\n    summary(step_backward)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_full), AIC(step_aic), AIC(step_forward), AIC(step_backward)),\n  BIC = c(BIC(lm_full), BIC(step_aic), BIC(step_forward), BIC(step_backward)),\n  n_parameters = c(\n    length(coef(lm_full)),\n    length(coef(step_aic)),\n    length(coef(step_forward)),\n    length(coef(step_backward))\n  )\n)\n\nprint(\"Model Selection Summary:\")\nprint(round(model_selection_summary[, .(Method, R_squared, Adj_R_squared, AIC, BIC, n_parameters)], 4))\n\n# Final selected model (based on AIC stepwise)\nprint(\"Final Selected Model Summary:\")\nprint(summary(step_aic))\n\n# Extract selected variables\nselected_vars &lt;- names(coef(step_aic))[-1] # Remove intercept\nprint(\"Selected Variables:\")\nprint(selected_vars)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 8: Variable selection results\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Variable selection results\n\n\n\n\n\n\n\n\n# Variable selection visualization\n\n# Model comparison plot\nmodel_comp_data &lt;- model_selection_summary[, .(Method, AIC, BIC, Adj_R_squared)]\nmodel_comp_long &lt;- melt(model_comp_data,\n  id.vars = \"Method\",\n  measure.vars = c(\"AIC\", \"BIC\"),\n  variable.name = \"Criterion\", value.name = \"Value\"\n)\n\np1_var &lt;- ggplot(data = model_comp_long, aes(x = Method, y = Value, fill = Criterion)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  labs(\n    x = \"Model Selection Method\", y = \"Criterion Value\",\n    title = \"Model Selection Criteria\", fill = \"Criterion\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Adjusted R² comparison\np2_var &lt;- ggplot(data = model_selection_summary, aes(x = reorder(Method, -Adj_R_squared), y = Adj_R_squared)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_text(aes(label = round(Adj_R_squared, 3)), vjust = -0.5) +\n  labs(\n    x = \"Model Selection Method\", y = \"Adjusted R²\",\n    title = \"Model Performance Comparison\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Final model coefficients\nfinal_coefs &lt;- coef(step_aic)[-1] # Remove intercept\ncoef_data &lt;- data.table(\n  variable = names(final_coefs),\n  coefficient = as.numeric(final_coefs)\n)\n\np3_var &lt;- ggplot(data = coef_data, aes(x = reorder(variable, abs(coefficient)), y = coefficient)) +\n  geom_col(fill = \"orange\", alpha = 0.7) +\n  coord_flip() +\n  labs(\n    x = \"Variable\", y = \"Coefficient\",\n    title = \"Final Model Coefficients\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Correlation plot of selected variables\nselected_cor &lt;- cor_matrix_quality[c(selected_vars, \"quality\"), c(selected_vars, \"quality\")]\n\np4_var &lt;- corrplot(selected_cor,\n  method = \"color\", type = \"upper\",\n  order = \"hclust\", tl.cex = 0.8, tl.col = \"black\"\n)\n\ngrid.arrange(p1_var, p2_var, p3_var, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteComprehensive Example\n\n\n\nA manufacturing engineer conducts a complete regression analysis to optimize a production process:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Production Optimization Data Summary:\"\n\n\n       n temp_mean  temp_sd pressure_mean pressure_sd time_mean  time_sd\n   &lt;int&gt;     &lt;num&gt;    &lt;num&gt;         &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:    40  182.9235 13.28839      7.768272    1.368849  44.61594 6.506526\n   yield_mean yield_sd\n        &lt;num&gt;    &lt;num&gt;\n1:   153.0184 7.917538\n\n\n[1] \"Yield by Operator:\"\n\n\n   operator     n mean_yield sd_yield\n     &lt;char&gt; &lt;int&gt;      &lt;num&gt;    &lt;num&gt;\n1:        A    12   153.8781 6.764170\n2:        B    13   153.7998 8.222506\n3:        C    15   151.6535 8.805235\n\n\n\n\n\n# Comprehensive regression analysis example\nset.seed(161718)\nproduction_data &lt;- data.table(\n  batch = 1:40,\n  temp = rnorm(40, mean = 180, sd = 15),\n  pressure = rnorm(40, mean = 8, sd = 1.5),\n  time = rnorm(40, mean = 45, sd = 8),\n  operator = sample(c(\"A\", \"B\", \"C\"), 40, replace = TRUE),\n  yield = NULL\n)\n\n# Complex relationship with interactions and quadratic terms\nproduction_data[, yield := 60 +\n  0.5 * temp +\n  3.0 * pressure +\n  0.3 * time +\n  0.01 * temp * pressure + # interaction\n  -0.0015 * temp^2 + # quadratic term\n  ifelse(operator == \"B\", 5, 0) + # operator effect\n  ifelse(operator == \"C\", -2, 0) +\n  rnorm(40, mean = 0, sd = 4)]\n\nprint(\"Production Optimization Data Summary:\")\nprod_summary &lt;- production_data %&gt;%\n  fsummarise(\n    n = fnobs(batch),\n    temp_mean = fmean(temp),\n    temp_sd = fsd(temp),\n    pressure_mean = fmean(pressure),\n    pressure_sd = fsd(pressure),\n    time_mean = fmean(time),\n    time_sd = fsd(time),\n    yield_mean = fmean(yield),\n    yield_sd = fsd(yield)\n  )\nprint(prod_summary)\n\n# Operator summary\noperator_summary &lt;- production_data %&gt;%\n  fgroup_by(operator) %&gt;%\n  fsummarise(\n    n = fnobs(yield),\n    mean_yield = fmean(yield),\n    sd_yield = fsd(yield)\n  )\nprint(\"Yield by Operator:\")\nprint(operator_summary)\n\n# Write data to CSV\nfwrite(production_data, \"data/production_optimization.csv\")\n\n\n\n\n\n\n\nComplete Analysis Steps:\n\nExploratory Data Analysis\nSimple Linear Regression\nMultiple Regression\nPolynomial Terms\nModel Diagnostics\nVariable Selection\nFinal Model Validation\n\n\nR OutputR Code\n\n\n\n\n[1] \"=== COMPREHENSIVE REGRESSION ANALYSIS ===\"\n\n\n[1] \"\\n--- STEP 1: EXPLORATORY DATA ANALYSIS ---\"\n\n\n[1] \"Correlation Matrix (numeric variables):\"\n\n\n          temp pressure  time yield\ntemp     1.000    0.202 0.326 0.256\npressure 0.202    1.000 0.081 0.865\ntime     0.326    0.081 1.000 0.193\nyield    0.256    0.865 0.193 1.000\n\n\n[1] \"\\n--- STEP 2: SIMPLE LINEAR REGRESSIONS ---\"\n\n\n[1] \"Simple Linear Regression Results:\"\n\n\nError in Math.data.frame(structure(list(Predictor = c(\"Temperature\", \"Pressure\", : non-numeric-alike variable(s) in data frame: Predictor\n\n\n[1] \"\\n--- STEP 3: MULTIPLE REGRESSION - MAIN EFFECTS ---\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator, data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9510 -1.9880 -0.6332  1.6444  7.3494 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 100.92275    8.32329  12.125 6.75e-14 ***\ntemp          0.02179    0.04351   0.501   0.6198    \npressure      5.16863    0.40476  12.770 1.58e-14 ***\ntime          0.18107    0.09638   1.879   0.0689 .  \noperatorB     2.70940    1.50947   1.795   0.0816 .  \noperatorC    -2.66870    1.39023  -1.920   0.0633 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.339 on 34 degrees of freedom\nMultiple R-squared:  0.845, Adjusted R-squared:  0.8222 \nF-statistic: 37.06 on 5 and 34 DF,  p-value: 7.99e-13\n\n\n[1] \"\\n--- STEP 4: POLYNOMIAL TERMS ---\"\n\n\n[1] \"Model with quadratic terms:\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator + I(temp^2) + \n    I(pressure^2), data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8800 -2.3948 -0.3778  1.8195  6.9391 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    1.016e+02  9.148e+01   1.111   0.2750  \ntemp           1.551e-01  9.902e-01   0.157   0.8765  \npressure       1.953e+00  4.266e+00   0.458   0.6501  \ntime           1.820e-01  9.871e-02   1.844   0.0744 .\noperatorB      2.769e+00  1.608e+00   1.722   0.0947 .\noperatorC     -2.831e+00  1.454e+00  -1.948   0.0603 .\nI(temp^2)     -3.837e-04  2.745e-03  -0.140   0.8897  \nI(pressure^2)  2.063e-01  2.720e-01   0.758   0.4539  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.41 on 32 degrees of freedom\nMultiple R-squared:  0.8478,    Adjusted R-squared:  0.8145 \nF-statistic: 25.46 on 7 and 32 DF,  p-value: 2.249e-11\n\n\n[1] \"F-test for polynomial terms:\"\n\n\nAnalysis of Variance Table\n\nModel 1: yield ~ temp + pressure + time + operator\nModel 2: yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     34 379.04                           \n2     32 372.12  2    6.9196 0.2975 0.7447\n\n\n[1] \"\\n--- STEP 5: INTERACTION TERMS ---\"\n\n\n[1] \"Model with interaction:\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator + I(temp^2) + \n    temp:pressure, data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0145 -1.9143 -0.6736  1.6542  7.3233 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   88.1805758 90.6278053   0.973   0.3379  \ntemp           0.1572700  1.0375790   0.152   0.8805  \npressure       5.3005491  7.3022345   0.726   0.4732  \ntime           0.1820613  0.0996318   1.827   0.0770 .\noperatorB      2.7710622  1.6318944   1.698   0.0992 .\noperatorC     -2.6475067  1.4768318  -1.793   0.0825 .\nI(temp^2)     -0.0003614  0.0032065  -0.113   0.9110  \ntemp:pressure -0.0006904  0.0393592  -0.018   0.9861  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.441 on 32 degrees of freedom\nMultiple R-squared:  0.8451,    Adjusted R-squared:  0.8112 \nF-statistic: 24.93 on 7 and 32 DF,  p-value: 2.967e-11\n\n\n[1] \"F-test for interaction term:\"\n\n\nAnalysis of Variance Table\n\nModel 1: yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2)\nModel 2: yield ~ temp + pressure + time + operator + I(temp^2) + temp:pressure\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1     32 372.12                      \n2     32 378.80  0   -6.6817         \n\n\n[1] \"\\n--- STEP 6: MODEL SELECTION ---\"\n\n\n[1] \"Model Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Main Effects\", \"Polynomial\", : non-numeric-alike variable(s) in data frame: Model\n\n\n[1] \"Best model by AIC: Main Effects\"\n\n\n[1] \"\\n--- STEP 7: FINAL MODEL DIAGNOSTICS ---\"\n\n\n[1] \"Final Model Diagnostic Summary:\"\n\n\n[1] \"Residual standard error:\"\n\n\nError in print.default(\"Multiple R-squared:\", round(summary(final_model)$r.squared, : invalid printing digits 0\n\n\nError in print.default(\"Adjusted R-squared:\", round(summary(final_model)$adj.r.squared, : invalid printing digits 0\n\n\n\n\n\n# Comprehensive regression analysis steps\n\nprint(\"=== COMPREHENSIVE REGRESSION ANALYSIS ===\")\n\n# Step 1: Exploratory Data Analysis\nprint(\"\\n--- STEP 1: EXPLORATORY DATA ANALYSIS ---\")\ncor_matrix_prod &lt;- cor(production_data[, .(temp, pressure, time, yield)])\nprint(\"Correlation Matrix (numeric variables):\")\nprint(round(cor_matrix_prod, 3))\n\n# Step 2: Simple linear regressions\nprint(\"\\n--- STEP 2: SIMPLE LINEAR REGRESSIONS ---\")\nlm_temp &lt;- lm(yield ~ temp, data = production_data)\nlm_pressure &lt;- lm(yield ~ pressure, data = production_data)\nlm_time &lt;- lm(yield ~ time, data = production_data)\n\nsimple_models &lt;- data.table(\n  Predictor = c(\"Temperature\", \"Pressure\", \"Time\"),\n  R_squared = c(\n    summary(lm_temp)$r.squared,\n    summary(lm_pressure)$r.squared,\n    summary(lm_time)$r.squared\n  ),\n  F_statistic = c(\n    summary(lm_temp)$fstatistic[1],\n    summary(lm_pressure)$fstatistic[1],\n    summary(lm_time)$fstatistic[1]\n  ),\n  p_value = c(\n    pf(summary(lm_temp)$fstatistic[1], 1, 38, lower.tail = FALSE),\n    pf(summary(lm_pressure)$fstatistic[1], 1, 38, lower.tail = FALSE),\n    pf(summary(lm_time)$fstatistic[1], 1, 38, lower.tail = FALSE)\n  )\n)\nprint(\"Simple Linear Regression Results:\")\nprint(round(simple_models, 4))\n\n# Step 3: Multiple regression with main effects\nprint(\"\\n--- STEP 3: MULTIPLE REGRESSION - MAIN EFFECTS ---\")\nproduction_data[, operator := factor(operator)]\nlm_main &lt;- lm(yield ~ temp + pressure + time + operator, data = production_data)\nprint(summary(lm_main))\n\n# Step 4: Add polynomial terms\nprint(\"\\n--- STEP 4: POLYNOMIAL TERMS ---\")\nlm_poly &lt;- lm(yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2),\n  data = production_data\n)\nprint(\"Model with quadratic terms:\")\nprint(summary(lm_poly))\n\n# Test if polynomial terms are needed\nanova_poly &lt;- anova(lm_main, lm_poly)\nprint(\"F-test for polynomial terms:\")\nprint(anova_poly)\n\n# Step 5: Add interaction terms\nprint(\"\\n--- STEP 5: INTERACTION TERMS ---\")\nlm_interact &lt;- lm(yield ~ temp + pressure + time + operator + I(temp^2) + temp:pressure,\n  data = production_data\n)\nprint(\"Model with interaction:\")\nprint(summary(lm_interact))\n\n# Test if interaction is needed\nanova_interact &lt;- anova(lm_poly, lm_interact)\nprint(\"F-test for interaction term:\")\nprint(anova_interact)\n\n# Step 6: Model selection\nprint(\"\\n--- STEP 6: MODEL SELECTION ---\")\ncomprehensive_models &lt;- data.table(\n  Model = c(\"Main Effects\", \"Polynomial\", \"With Interaction\"),\n  R_squared = c(\n    summary(lm_main)$r.squared,\n    summary(lm_poly)$r.squared,\n    summary(lm_interact)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_main)$adj.r.squared,\n    summary(lm_poly)$adj.r.squared,\n    summary(lm_interact)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_main), AIC(lm_poly), AIC(lm_interact)),\n  BIC = c(BIC(lm_main), BIC(lm_poly), BIC(lm_interact)),\n  RMSE = c(\n    sqrt(mean(residuals(lm_main)^2)),\n    sqrt(mean(residuals(lm_poly)^2)),\n    sqrt(mean(residuals(lm_interact)^2))\n  )\n)\n\nprint(\"Model Comparison:\")\nprint(round(comprehensive_models, 4))\n\n# Select best model (lowest AIC)\nbest_model_comp &lt;- which.min(comprehensive_models$AIC)\nprint(paste(\"Best model by AIC:\", comprehensive_models$Model[best_model_comp]))\n\n# Step 7: Final model diagnostics\nprint(\"\\n--- STEP 7: FINAL MODEL DIAGNOSTICS ---\")\nfinal_model &lt;- lm_interact # Assuming this is the best\n\n# Basic diagnostics\nresiduals_final &lt;- residuals(final_model)\nfitted_final &lt;- fitted(final_model)\nn_final &lt;- nrow(production_data)\nk_final &lt;- length(coef(final_model)) - 1\n\ndiagnostic_final &lt;- data.table(\n  observation = 1:n_final,\n  residuals = residuals_final,\n  fitted = fitted_final,\n  standardized_resid = residuals_final / sd(residuals_final)\n)\n\nprint(\"Final Model Diagnostic Summary:\")\nprint(\"Residual standard error:\", round(summary(final_model)$sigma, 3))\nprint(\"Multiple R-squared:\", round(summary(final_model)$r.squared, 3))\nprint(\"Adjusted R-squared:\", round(summary(final_model)$adj.r.squared, 3))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 10: Comprehensive regression analysis dashboard\n\n\n\n\n\n\n\n\n# Comprehensive regression analysis dashboard\n\n# 1. Pairwise scatter plots\np1_comp &lt;- ggplot(data = production_data, aes(x = temp, y = yield, color = operator)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Temperature\", y = \"Yield\", title = \"Yield vs Temperature\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2_comp &lt;- ggplot(data = production_data, aes(x = pressure, y = yield, color = operator)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Pressure\", y = \"Yield\", title = \"Yield vs Pressure\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Model comparison plot\nmodel_comp_plot &lt;- ggplot(\n  data = comprehensive_models,\n  aes(x = reorder(Model, -Adj_R_squared), y = Adj_R_squared)\n) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_text(aes(label = round(Adj_R_squared, 3)), vjust = -0.5) +\n  labs(x = \"Model\", y = \"Adjusted R²\", title = \"Model Comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Final model diagnostics\np3_comp &lt;- ggplot(data = diagnostic_final, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Final Model Diagnostics\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Actual vs Predicted\np4_comp &lt;- ggplot(\n  data = data.table(actual = production_data$yield, predicted = fitted_final),\n  aes(x = predicted, y = actual)\n) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted Yield\", y = \"Actual Yield\", title = \"Actual vs Predicted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1_comp, p2_comp, model_comp_plot, p3_comp, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEssential Formulas Summary\n\n\n\nSimple Linear Regression:\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nMultiple Regression:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nCoefficient of Determination:\nR^2 = \\frac{SS_R}{SS_T}, \\quad R_{adj}^2 = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\nPrediction Interval:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-k-1} \\sqrt{MS_E\\left(1 + \\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0\\right)}\nF-Test for Overall Significance:\nF_0 = \\frac{MS_R}{MS_E} = \\frac{SS_R/k}{SS_E/(n-k-1)}\nt-Test for Individual Parameters:\nt_0 = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Key Formulas and Functions Summary:\"\n\n\n                Purpose                Key_Formula    R_Function\n                 &lt;char&gt;                     &lt;char&gt;        &lt;char&gt;\n1: Parameter Estimation             β̂ = (X'X)⁻¹X'y          lm()\n2:   Hypothesis Testing              t = β̂ⱼ/SE(β̂ⱼ)     summary()\n3:      Model Selection AIC = n ln(SSE/n) + 2(k+1) step(), AIC()\n4:           Prediction     ŷ ± t_{α/2} × SE(pred)     predict()\n5:          Diagnostics            VIF = 1/(1-R²ⱼ) vif(), plot()\n              Interpretation\n                      &lt;char&gt;\n1:   Least squares estimates\n2:   Test significance of βⱼ\n3: Balance fit vs complexity\n4:     Individual prediction\n5:   Check multicollinearity\n\n\n\n\nTable 1: Model selection criteria comparison\n\n\n\n\nRegression Model Types and Applications\n\n\nModel_Type\nExample\nKey_Feature\nR_squared_Range\nMain_Diagnostic\nWhen_to_Use\n\n\n\n\nSimple Linear\nSteel Strength\nSingle predictor\n0.60-0.80\nLinearity\nExploring relationships\n\n\nMultiple Linear\nChemical Yield\nMultiple predictors\n0.75-0.90\nMulticollinearity\nMultiple continuous predictors\n\n\nPolynomial\nReaction Rate\nNonlinear relationship\n0.85-0.95\nCurvature\nCurved relationships\n\n\nCategorical\nStrength by Supplier\nFactor variables\n0.70-0.85\nGroup effects\nComparing groups/categories\n\n\nVariable Selection\nQuality Prediction\nAutomated selection\n0.80-0.92\nOverfitting\nMany potential predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Model selection criteria comparison table\nall_models_summary &lt;- data.table(\n  Model_Type = c(\"Simple Linear\", \"Multiple Linear\", \"Polynomial\", \"Categorical\", \"Variable Selection\"),\n  Example = c(\"Steel Strength\", \"Chemical Yield\", \"Reaction Rate\", \"Strength by Supplier\", \"Quality Prediction\"),\n  Key_Feature = c(\"Single predictor\", \"Multiple predictors\", \"Nonlinear relationship\", \"Factor variables\", \"Automated selection\"),\n  R_squared_Range = c(\"0.60-0.80\", \"0.75-0.90\", \"0.85-0.95\", \"0.70-0.85\", \"0.80-0.92\"),\n  Main_Diagnostic = c(\"Linearity\", \"Multicollinearity\", \"Curvature\", \"Group effects\", \"Overfitting\"),\n  When_to_Use = c(\n    \"Exploring relationships\",\n    \"Multiple continuous predictors\",\n    \"Curved relationships\",\n    \"Comparing groups/categories\",\n    \"Many potential predictors\"\n  )\n)\n\nkbl(all_models_summary,\n  caption = \"Regression Model Types and Applications\"\n) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(1, width = \"2.5cm\") %&gt;%\n  column_spec(2, width = \"2.5cm\") %&gt;%\n  column_spec(3, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"2cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\") %&gt;%\n  column_spec(6, width = \"3.5cm\")\n\n# Summary of all key formulas and criteria\nformula_summary &lt;- data.table(\n  Purpose = c(\"Parameter Estimation\", \"Hypothesis Testing\", \"Model Selection\", \"Prediction\", \"Diagnostics\"),\n  Key_Formula = c(\n    \"β̂ = (X'X)⁻¹X'y\",\n    \"t = β̂ⱼ/SE(β̂ⱼ)\",\n    \"AIC = n ln(SSE/n) + 2(k+1)\",\n    \"ŷ ± t_{α/2} × SE(pred)\",\n    \"VIF = 1/(1-R²ⱼ)\"\n  ),\n  R_Function = c(\"lm()\", \"summary()\", \"step(), AIC()\", \"predict()\", \"vif(), plot()\"),\n  Interpretation = c(\n    \"Least squares estimates\",\n    \"Test significance of βⱼ\",\n    \"Balance fit vs complexity\",\n    \"Individual prediction\",\n    \"Check multicollinearity\"\n  )\n)\n\nprint(\"Key Formulas and Functions Summary:\")\nprint(formula_summary)\n\n\n\n\n\n\n\n\n\nThis chapter covered the fundamental concepts of building empirical models:\n\nSimple Linear Regression - Modeling relationships between two variables\nParameter Estimation - Using least squares to fit models\nHypothesis Testing - Testing significance of model parameters\nModel Validation - Checking assumptions through residual analysis\nMultiple Regression - Extending to multiple predictors\nAdvanced Topics - Polynomial models, categorical variables, variable selection\n\nKey Principles:\n\nAlways validate model assumptions\nUse appropriate diagnostic tools\nConsider both statistical and practical significance\nApply subject matter knowledge in model building\nValidate models on independent data when possible",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#introduction-to-empirical-models",
    "href": "book/Ch06.html#introduction-to-empirical-models",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteWhat are Empirical Models?\n\n\n\nEmpirical models are mathematical relationships derived from observed data rather than theoretical principles. They are essential in engineering for:\n\nProcess Optimization: Understanding how input variables affect outputs\nQuality Control: Relating product characteristics to process conditions\nDesign: Predicting performance based on design parameters\nTroubleshooting: Identifying factors that influence system behavior\n\nTypes of Empirical Models:\n\nLinear Models: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\epsilon\nPolynomial Models: y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\ldots + \\epsilon\nInteraction Models: y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_{12} x_1 x_2 + \\epsilon\n\nKey Assumptions:\n\nLinear relationship between predictors and response\nIndependent observations\nConstant variance (homoscedasticity)\nNormally distributed errors",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#simple-linear-regression",
    "href": "book/Ch06.html#simple-linear-regression",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteSimple Linear Regression Model\n\n\n\nThe simple linear regression model relates a response variable y to a single predictor variable x:\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\nwhere:\n\ny_i = response variable for observation i\nx_i = predictor variable for observation i\n\\beta_0 = intercept parameter\n\\beta_1 = slope parameter\n\\epsilon_i = random error term, \\epsilon_i \\sim N(0, \\sigma^2)\n\nLeast Squares Estimators:\n\\hat{\\beta}_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} = \\frac{S_{xy}}{S_{xx}}\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nwhere:\n\nS_{xy} = \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})\nS_{xx} = \\sum_{i=1}^n (x_i - \\bar{x})^2\nS_{yy} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA materials engineer wants to relate the tensile strength of steel specimens to their hardness. Data from 20 specimens:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Steel Strength vs Hardness Data Summary:\"\n\n\n       n Hardness_Mean Hardness_SD Hardness_Min Hardness_Max Strength_Mean\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:    20      62.03234    12.53884     41.68238     78.27333      169.2171\n   Strength_SD Strength_Min Strength_Max\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;\n1:     29.0103     120.2922     220.4809\n\n\n[1] \"First 10 observations:\"\n\n\n    specimen hardness strength\n       &lt;int&gt;    &lt;num&gt;    &lt;num&gt;\n 1:        1 51.50310 153.5504\n 2:        2 71.53221 196.7090\n 3:        3 56.35908 159.1039\n 4:        4 75.32070 204.1872\n 5:        5 77.61869 204.6000\n 6:        6 41.82226 133.8510\n 7:        7 61.12422 171.7934\n 8:        8 75.69676 188.5090\n 9:        9 62.05740 175.7543\n10:       10 58.26459 156.8791\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\nlibrary(tidyr)\nlibrary(corrplot)\n\n# Steel strength vs hardness data (simple linear regression)\nset.seed(123)\nsteel_data &lt;- data.table(\n  specimen = 1:20,\n  hardness = runif(20, min = 40, max = 80),\n  strength = NULL\n)\n\n# Create realistic relationship with some noise\nsteel_data[, strength := 15 + 2.5 * hardness + rnorm(20, mean = 0, sd = 8)]\n\n# Summary statistics\nsteel_summary &lt;- steel_data %&gt;%\n  fsummarise(\n    n = fnobs(hardness),\n    Hardness_Mean = fmean(hardness),\n    Hardness_SD = fsd(hardness),\n    Hardness_Min = fmin(hardness),\n    Hardness_Max = fmax(hardness),\n    Strength_Mean = fmean(strength),\n    Strength_SD = fsd(strength),\n    Strength_Min = fmin(strength),\n    Strength_Max = fmax(strength)\n  )\n\nprint(\"Steel Strength vs Hardness Data Summary:\")\nprint(steel_summary)\n\n# Detailed data view\nprint(\"First 10 observations:\")\nprint(head(steel_data, 10))\n\n# Create data directory if needed\nif (!dir.exists(\"data\")) dir.create(\"data\")\n\n# Write data to CSV\nfwrite(steel_data, \"data/steel_strength.csv\")\n\n\n\n\n\n\n\nManual Calculation:\nGiven: n = 20, \\bar{x} = hardness mean, \\bar{y} = strength mean\n\nCalculate Sums:\n\nS_{xx} = \\sum(x_i - \\bar{x})^2\nS_{xy} = \\sum(x_i - \\bar{x})(y_i - \\bar{y})\nS_{yy} = \\sum(y_i - \\bar{y})^2\n\nEstimate Parameters: \\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}} \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nFitted Model: \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\n\n\nR OutputR Code\n\n\n\n\n[1] \"Manual Simple Linear Regression Results:\"\n\n\n        Parameter Manual_Estimate                               Interpretation\n           &lt;char&gt;           &lt;num&gt;                                       &lt;char&gt;\n1: β₀ (Intercept)         30.4493            Strength when hardness = 0: 30.45\n2:     β₁ (Slope)          2.2370 Strength increases by 2.24 per unit hardness\n3:             σ²         57.8584                        Error variance: 57.86\n4:             R²          0.9349                93.5 % of variation explained\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = strength ~ hardness, data = steel_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.2758  -5.0561  -0.6948   5.4929  15.1408 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  30.4493     8.7991   3.461  0.00279 ** \nhardness      2.2370     0.1392  16.074 4.03e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 7.606 on 18 degrees of freedom\nMultiple R-squared:  0.9349,    Adjusted R-squared:  0.9313 \nF-statistic: 258.4 on 1 and 18 DF,  p-value: 4.031e-12\n\n\n[1] \"Manual vs R Function Comparison:\"\n\n\n     Parameter    Manual R_Function   Difference\n        &lt;char&gt;     &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:   Intercept 30.449343  30.449343 7.815970e-14\n2:       Slope  2.237023   2.237023 8.881784e-16\n3:   R-squared  0.934870   0.934870 0.000000e+00\n4: Residual SE  7.606471   7.606471 2.664535e-15\n\n\n\n\n\n# Simple linear regression - manual calculations\nn &lt;- fnobs(steel_data$hardness)\nx_bar &lt;- fmean(steel_data$hardness)\ny_bar &lt;- fmean(steel_data$strength)\n\n# Calculate sums of squares and cross-products\nx &lt;- steel_data$hardness\ny &lt;- steel_data$strength\n\n# Manual calculation of sums\nSxx &lt;- sum((x - x_bar)^2)\nSyy &lt;- sum((y - y_bar)^2)\nSxy &lt;- sum((x - x_bar) * (y - y_bar))\n\n# Parameter estimates\nbeta1_hat &lt;- Sxy / Sxx\nbeta0_hat &lt;- y_bar - beta1_hat * x_bar\n\n# Sum of squares\nSSR &lt;- beta1_hat * Sxy # Regression sum of squares\nSSE &lt;- Syy - beta1_hat * Sxy # Error sum of squares\nSST &lt;- Syy # Total sum of squares\n\n# Mean squares\nMSR &lt;- SSR / 1 # df = 1 for simple regression\nMSE &lt;- SSE / (n - 2) # df = n - 2\ns_squared &lt;- MSE\n\n# R-squared\nR_squared &lt;- SSR / SST\n\n# Manual calculations summary\nmanual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Slope)\", \"σ²\", \"R²\"),\n  Manual_Estimate = c(\n    round(beta0_hat, 4),\n    round(beta1_hat, 4),\n    round(s_squared, 4),\n    round(R_squared, 4)\n  ),\n  Interpretation = c(\n    paste(\"Strength when hardness = 0:\", round(beta0_hat, 2)),\n    paste(\"Strength increases by\", round(beta1_hat, 2), \"per unit hardness\"),\n    paste(\"Error variance:\", round(s_squared, 2)),\n    paste(round(R_squared * 100, 1), \"% of variation explained\")\n  )\n)\n\nprint(\"Manual Simple Linear Regression Results:\")\nprint(manual_results)\n\n# Verification with R's lm function\nlm_model &lt;- lm(strength ~ hardness, data = steel_data)\nlm_summary &lt;- summary(lm_model)\n\nprint(\"R's lm() verification:\")\nprint(lm_summary)\n\n# Compare manual vs R calculations\ncomparison &lt;- data.table(\n  Parameter = c(\"Intercept\", \"Slope\", \"R-squared\", \"Residual SE\"),\n  Manual = c(beta0_hat, beta1_hat, R_squared, sqrt(MSE)),\n  R_Function = c(\n    coef(lm_model)[1], coef(lm_model)[2],\n    summary(lm_model)$r.squared, summary(lm_model)$sigma\n  ),\n  Difference = c(\n    abs(beta0_hat - coef(lm_model)[1]),\n    abs(beta1_hat - coef(lm_model)[2]),\n    abs(R_squared - summary(lm_model)$r.squared),\n    abs(sqrt(MSE) - summary(lm_model)$sigma)\n  )\n)\n\nprint(\"Manual vs R Function Comparison:\")\nprint(comparison)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Simple linear regression: Tensile strength vs. hardness\n\n\n\n\n\n\n\n\n# Simple linear regression visualization\np1 &lt;- ggplot(data = steel_data, mapping = aes(x = hardness, y = strength)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linewidth = 1) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Simple Linear Regression: Tensile Strength vs Hardness\",\n    subtitle = paste(\"ŷ =\", round(beta0_hat, 2), \"+\", round(beta1_hat, 2), \"x, R² =\", round(R_squared, 3))\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n# Residuals vs fitted\nfitted_values &lt;- beta0_hat + beta1_hat * steel_data$hardness\nresiduals &lt;- steel_data$strength - fitted_values\n\nresidual_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  hardness = steel_data$hardness\n)\n\np2 &lt;- ggplot(data = residual_data, mapping = aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(\n    x = \"Fitted Values\",\n    y = \"Residuals\",\n    title = \"Residuals vs Fitted Values\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine plots\ngrid.arrange(p1, p2, nrow = 1, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Tests for Regression Parameters\n\n\n\nTest for Slope (Significance of Regression):\nH₀: β₁ = 0 vs H₁: β₁ ≠ 0\nTest Statistic:\nt_0 = \\frac{\\hat{\\beta}_1}{SE(\\hat{\\beta}_1)} = \\frac{\\hat{\\beta}_1}{\\sqrt{MS_E/S_{xx}}}\nwhere MS_E = \\frac{SS_E}{n-2} and SS_E = S_{yy} - \\hat{\\beta}_1 S_{xy}\nTest for Intercept:\nH₀: β₀ = 0 vs H₁: β₀ ≠ 0\nTest Statistic:\nt_0 = \\frac{\\hat{\\beta}_0}{SE(\\hat{\\beta}_0)} = \\frac{\\hat{\\beta}_0}{\\sqrt{MS_E(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}})}}\nANOVA Table for Regression:\n\n\n\n\n\n\n\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nRegression\nSS_R = \\hat{\\beta}_1 S_{xy}\n1\nMS_R\nMS_R/MS_E\n\n\nError\nSS_E = S_{yy} - \\hat{\\beta}_1 S_{xy}\nn-2\nMS_E\n\n\n\nTotal\nSS_T = S_{yy}\nn-1\n\n\n\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Hypothesis Test Results:\"\n\n\n                   Test Null_Hypothesis Test_Statistic P_Value Critical_Value\n                 &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;          &lt;num&gt;\n1:         Slope β₁ = 0          β₁ = 0        16.0739  0.0000          2.101\n2:     Intercept β₀ = 0          β₀ = 0         3.4605  0.0028          2.101\n3: Overall Model F-test          β₁ = 0       258.3704  0.0000          4.414\n    Decision                               Conclusion\n      &lt;char&gt;                                   &lt;char&gt;\n1: Reject H₀          Significant linear relationship\n2: Reject H₀ Intercept significantly different from 0\n3: Reject H₀                     Model is significant\n\n\n[1] \"ANOVA Table:\"\n\n\n       Source       SS    df       MS F_Statistic P_Value\n       &lt;char&gt;    &lt;num&gt; &lt;num&gt;    &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 14948.90     1 14948.90    258.3704       0\n2:      Error  1041.45    18    57.86          NA      NA\n3:      Total 15990.35    19       NA          NA      NA\n\n\n\n\n\n# Hypothesis testing for regression parameters\nalpha &lt;- 0.05\ndf_error &lt;- n - 2\nt_critical &lt;- qt(1 - alpha / 2, df_error)\n\n# Standard errors\nSE_beta1 &lt;- sqrt(MSE / Sxx)\nSE_beta0 &lt;- sqrt(MSE * (1 / n + x_bar^2 / Sxx))\n\n# Test for slope (β₁ = 0)\nt_stat_beta1 &lt;- beta1_hat / SE_beta1\np_value_beta1 &lt;- 2 * (1 - pt(abs(t_stat_beta1), df_error))\n\n# Test for intercept (β₀ = 0)\nt_stat_beta0 &lt;- beta0_hat / SE_beta0\np_value_beta0 &lt;- 2 * (1 - pt(abs(t_stat_beta0), df_error))\n\n# F-test for overall significance\nF_stat &lt;- MSR / MSE\np_value_F &lt;- 1 - pf(F_stat, 1, df_error)\n\n# Hypothesis test results\nhypothesis_tests &lt;- data.table(\n  Test = c(\"Slope β₁ = 0\", \"Intercept β₀ = 0\", \"Overall Model F-test\"),\n  Null_Hypothesis = c(\"β₁ = 0\", \"β₀ = 0\", \"β₁ = 0\"),\n  Test_Statistic = c(round(t_stat_beta1, 4), round(t_stat_beta0, 4), round(F_stat, 4)),\n  P_Value = c(round(p_value_beta1, 4), round(p_value_beta0, 4), round(p_value_F, 4)),\n  Critical_Value = c(\n    round(t_critical, 3), round(t_critical, 3),\n    round(qf(1 - alpha, 1, df_error), 3)\n  ),\n  Decision = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_value_F &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Conclusion = c(\n    ifelse(p_value_beta1 &lt; alpha, \"Significant linear relationship\", \"No significant relationship\"),\n    ifelse(p_value_beta0 &lt; alpha, \"Intercept significantly different from 0\", \"Intercept not significant\"),\n    ifelse(p_value_F &lt; alpha, \"Model is significant\", \"Model is not significant\")\n  )\n)\n\nprint(\"Hypothesis Test Results:\")\nprint(hypothesis_tests)\n\n# ANOVA table\nanova_table &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR, 2), round(SSE, 2), round(SST, 2)),\n  df = c(1, df_error, n - 1),\n  MS = c(round(MSR, 2), round(MSE, 2), NA),\n  F_Statistic = c(round(F_stat, 4), NA, NA),\n  P_Value = c(round(p_value_F, 4), NA, NA)\n)\n\nprint(\"ANOVA Table:\")\nprint(anova_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Intervals\n\n\n\nFor Slope β₁:\n\\hat{\\beta}_1 \\pm t_{\\alpha/2, n-2} \\sqrt{\\frac{MS_E}{S_{xx}}}\nFor Intercept β₀:\n\\hat{\\beta}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(\\frac{1}{n} + \\frac{\\bar{x}^2}{S_{xx}}\\right)}\nFor Mean Response at x₀:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(\\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nCoefficient of Determination:\nR^2 = \\frac{SS_R}{SS_T} = \\frac{\\hat{\\beta}_1 S_{xy}}{S_{yy}} = 1 - \\frac{SS_E}{SS_T}\nR^2 represents the proportion of total variation explained by the regression.\n\n\n\n\n\n\n\n\n\n\n\nNotePrediction Intervals\n\n\n\nFor predicting a single future observation at x₀:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-2} \\sqrt{MS_E\\left(1 + \\frac{1}{n} + \\frac{(x_0 - \\bar{x})^2}{S_{xx}}\\right)}\nKey Differences:\n\nConfidence Interval: For the mean response (narrower)\nPrediction Interval: For individual observation (wider due to additional uncertainty)\n\nThe extra “1” in the prediction interval accounts for the variability of individual observations around the mean response.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Mean Response:\"\n\n\n   Hardness Predicted CI_Lower CI_Upper CI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   116.75   130.63    13.88\n2: 50.83012    144.16   139.31   149.00     9.69\n3: 59.97786    164.62   161.00   168.24     7.25\n4: 69.12560    185.08   180.95   189.22     8.26\n5: 78.27333    205.55   199.61   211.49    11.89\n\n\n[1] \"95% Prediction Intervals for Individual Observations:\"\n\n\n   Hardness Predicted PI_Lower PI_Upper PI_Width\n      &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1: 41.68238    123.69   106.27   141.12    34.85\n2: 50.83012    144.16   127.46   160.86    33.40\n3: 59.97786    164.62   148.23   181.01    32.77\n4: 69.12560    185.08   168.58   201.59    33.01\n5: 78.27333    205.55   188.50   222.60    34.10\n\n\n[1] \"Interval Width Comparison:\"\n\n\n   Hardness CI_Width PI_Width PI_vs_CI_Ratio\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1: 41.68238    13.88    34.85           2.51\n2: 50.83012     9.69    33.40           3.45\n3: 59.97786     7.25    32.77           4.52\n4: 69.12560     8.26    33.01           4.00\n5: 78.27333    11.89    34.10           2.87\n\n\n\n\n\n# Confidence and prediction intervals\n# Choose some specific x values for demonstration\nx_new &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 5)\n\n# Confidence intervals for mean response\nCI_results &lt;- data.table()\nPI_results &lt;- data.table()\n\nfor (x0 in x_new) {\n  # Predicted value\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval for mean response\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_CI &lt;- t_critical * SE_mean\n  CI_lower &lt;- y_hat - margin_CI\n  CI_upper &lt;- y_hat + margin_CI\n\n  # Prediction interval for individual observation\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  margin_PI &lt;- t_critical * SE_pred\n  PI_lower &lt;- y_hat - margin_PI\n  PI_upper &lt;- y_hat + margin_PI\n\n  CI_results &lt;- rbind(CI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    CI_Lower = round(CI_lower, 2),\n    CI_Upper = round(CI_upper, 2),\n    CI_Width = round(CI_upper - CI_lower, 2)\n  ))\n\n  PI_results &lt;- rbind(PI_results, data.table(\n    Hardness = x0,\n    Predicted = round(y_hat, 2),\n    PI_Lower = round(PI_lower, 2),\n    PI_Upper = round(PI_upper, 2),\n    PI_Width = round(PI_upper - PI_lower, 2)\n  ))\n}\n\nprint(\"95% Confidence Intervals for Mean Response:\")\nprint(CI_results)\n\nprint(\"95% Prediction Intervals for Individual Observations:\")\nprint(PI_results)\n\n# Compare CI and PI widths\ninterval_comparison &lt;- data.table(\n  Hardness = x_new,\n  CI_Width = CI_results$CI_Width,\n  PI_Width = PI_results$PI_Width,\n  PI_vs_CI_Ratio = round(PI_results$PI_Width / CI_results$CI_Width, 2)\n)\n\nprint(\"Interval Width Comparison:\")\nprint(interval_comparison)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Confidence and prediction intervals\n\n\n\n\n\n\n\n\n# Confidence and prediction intervals visualization\n# Create fine grid for smooth curves\nx_grid &lt;- seq(fmin(steel_data$hardness), fmax(steel_data$hardness), length.out = 100)\ninterval_data &lt;- data.table()\n\nfor (x0 in x_grid) {\n  y_hat &lt;- beta0_hat + beta1_hat * x0\n\n  # Confidence interval\n  SE_mean &lt;- sqrt(MSE * (1 / n + (x0 - x_bar)^2 / Sxx))\n  CI_lower &lt;- y_hat - t_critical * SE_mean\n  CI_upper &lt;- y_hat + t_critical * SE_mean\n\n  # Prediction interval\n  SE_pred &lt;- sqrt(MSE * (1 + 1 / n + (x0 - x_bar)^2 / Sxx))\n  PI_lower &lt;- y_hat - t_critical * SE_pred\n  PI_upper &lt;- y_hat + t_critical * SE_pred\n\n  interval_data &lt;- rbind(interval_data, data.table(\n    hardness = x0,\n    fitted = y_hat,\n    CI_lower = CI_lower,\n    CI_upper = CI_upper,\n    PI_lower = PI_lower,\n    PI_upper = PI_upper\n  ))\n}\n\nggplot() +\n  # Prediction intervals (wider, lighter)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = PI_lower, ymax = PI_upper),\n    alpha = 0.2, fill = \"red\"\n  ) +\n  # Confidence intervals (narrower, darker)\n  geom_ribbon(\n    data = interval_data, aes(x = hardness, ymin = CI_lower, ymax = CI_upper),\n    alpha = 0.3, fill = \"blue\"\n  ) +\n  # Regression line\n  geom_line(data = interval_data, aes(x = hardness, y = fitted), color = \"black\", linewidth = 1) +\n  # Data points\n  geom_point(data = steel_data, aes(x = hardness, y = strength), size = 3, alpha = 0.7) +\n  labs(\n    x = \"Hardness\",\n    y = \"Tensile Strength\",\n    title = \"Confidence and Prediction Intervals\",\n    subtitle = \"Blue = 95% CI for mean response, Red = 95% PI for individual observations\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteResidual Analysis\n\n\n\nResiduals: e_i = y_i - \\hat{y}_i\nStandardized Residuals: d_i = \\frac{e_i}{\\sqrt{MS_E}}\nStudentized Residuals: More accurate for outlier detection\nKey Diagnostic Plots:\n\nResiduals vs. Fitted Values: Check for constant variance\nNormal Q-Q Plot: Check normality assumption\nResiduals vs. Order: Check for independence\nResiduals vs. Predictor: Check linearity assumption\n\nWhat to Look For:\n\nRandom scatter (no patterns)\nConstant spread across fitted values\nPoints following straight line in Q-Q plot\nNo obvious trends or cycles\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Residual Analysis Summary:\"\n\n\n   Mean_Residuals SD_Residuals Min_Residuals Max_Residuals Mean_Std_Residuals\n            &lt;num&gt;        &lt;num&gt;         &lt;num&gt;         &lt;num&gt;              &lt;num&gt;\n1:   7.105427e-15     7.403595     -11.27576      15.14077       9.325873e-16\n   SD_Std_Residuals\n              &lt;num&gt;\n1:        0.9733285\n\n\n[1] \"No obvious outliers detected (|std residual| &gt; 2)\"\n\n\n[1] \"Shapiro-Wilk normality test for residuals:\"\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals\nW = 0.96347, p-value = 0.6152\n\n\n[1] \"Durbin-Watson statistic: 1.7519\"\n\n\n[1] \"(Values near 2 indicate no autocorrelation)\"\n\n\n\n\n\n# Model adequacy checking - residual analysis\n# Calculate residuals and standardized residuals\nresiduals_data &lt;- data.table(\n  fitted = fitted_values,\n  residuals = residuals,\n  standardized_residuals = residuals / sqrt(MSE),\n  observation = 1:n,\n  hardness = steel_data$hardness\n)\n\n# Summary statistics for residuals\nresidual_summary &lt;- residuals_data %&gt;%\n  fsummarise(\n    Mean_Residuals = fmean(residuals),\n    SD_Residuals = fsd(residuals),\n    Min_Residuals = fmin(residuals),\n    Max_Residuals = fmax(residuals),\n    Mean_Std_Residuals = fmean(standardized_residuals),\n    SD_Std_Residuals = fsd(standardized_residuals)\n  )\n\nprint(\"Residual Analysis Summary:\")\nprint(residual_summary)\n\n# Check for outliers (|standardized residual| &gt; 2)\noutliers &lt;- residuals_data[abs(standardized_residuals) &gt; 2]\nif (nrow(outliers) &gt; 0) {\n  print(\"Potential outliers (|std residual| &gt; 2):\")\n  print(outliers)\n} else {\n  print(\"No obvious outliers detected (|std residual| &gt; 2)\")\n}\n\n# Normality test for residuals\nshapiro_test &lt;- shapiro.test(residuals)\nprint(\"Shapiro-Wilk normality test for residuals:\")\nprint(shapiro_test)\n\n# Durbin-Watson test for independence (if data has natural order)\ndw_stat &lt;- sum(diff(residuals)^2) / sum(residuals^2)\nprint(paste(\"Durbin-Watson statistic:\", round(dw_stat, 4)))\nprint(\"(Values near 2 indicate no autocorrelation)\")\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: Residual analysis plots\n\n\n\n\n\n\n\n\n# Residual analysis plots\n# 1. Residuals vs Fitted\np1 &lt;- ggplot(data = residuals_data, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Normal Q-Q plot\np2 &lt;- ggplot(data = residuals_data, aes(sample = standardized_residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    x = \"Theoretical Quantiles\", y = \"Sample Quantiles\",\n    title = \"Normal Q-Q Plot of Residuals\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Scale-location plot\np3 &lt;- ggplot(data = residuals_data, aes(x = fitted, y = sqrt(abs(standardized_residuals)))) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    x = \"Fitted Values\", y = \"√|Standardized Residuals|\",\n    title = \"Scale-Location Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Residuals vs Order\np4 &lt;- ggplot(data = residuals_data, aes(x = observation, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_line(alpha = 0.5) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"Observation Order\", y = \"Residuals\", title = \"Residuals vs Order\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Combine all diagnostic plots\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCorrelation vs. Regression\n\n\n\nCorrelation Coefficient:\nr = \\frac{S_{xy}}{\\sqrt{S_{xx} S_{yy}}}\nRelationship to Regression:\n\nR^2 = r^2 in simple linear regression\nCorrelation measures strength of linear relationship\nRegression provides prediction equation\n\nImportant Notes:\n\nCorrelation ≠ Causation\nBoth assume linear relationship\nSensitive to outliers\n-1 \\leq r \\leq 1\n\nInterpretation of |r|:\n\n0.0-0.3: Weak relationship\n0.3-0.7: Moderate relationship\n0.7-1.0: Strong relationship\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Correlation vs Regression Analysis:\"\n\n\n                       Measure  Value                          Interpretation\n                        &lt;char&gt;  &lt;num&gt;                                  &lt;char&gt;\n1: Correlation coefficient (r) 0.9669              Strong linear relationship\n2:                          r² 0.9349                  93.5 % shared variance\n3:          R² from regression 0.9349 93.5 % variance explained by regression\n4:        Difference |r² - R²| 0.0000                       Perfect agreement\n\n\n[1] \"Correlation Significance Test:\"\n\n\n                       Test                             H0 t_statistic p_value\n                     &lt;char&gt;                         &lt;char&gt;       &lt;num&gt;   &lt;num&gt;\n1: Correlation significance ρ = 0 (no linear relationship)     16.0739       0\n    decision              conclusion\n      &lt;char&gt;                  &lt;char&gt;\n1: Reject H₀ Significant correlation\n\n\n\n\n\n# Correlation analysis\ncorrelation_coef &lt;- cor(steel_data$hardness, steel_data$strength)\ncorrelation_squared &lt;- correlation_coef^2\n\n# Verify relationship between correlation and R-squared\ncorrelation_results &lt;- data.table(\n  Measure = c(\"Correlation coefficient (r)\", \"r²\", \"R² from regression\", \"Difference |r² - R²|\"),\n  Value = c(\n    round(correlation_coef, 4),\n    round(correlation_squared, 4),\n    round(R_squared, 4),\n    round(abs(correlation_squared - R_squared), 6)\n  ),\n  Interpretation = c(\n    ifelse(abs(correlation_coef) &gt; 0.7, \"Strong linear relationship\",\n      ifelse(abs(correlation_coef) &gt; 0.3, \"Moderate linear relationship\", \"Weak linear relationship\")\n    ),\n    paste(round(correlation_squared * 100, 1), \"% shared variance\"),\n    paste(round(R_squared * 100, 1), \"% variance explained by regression\"),\n    ifelse(abs(correlation_squared - R_squared) &lt; 0.001, \"Perfect agreement\", \"Some difference\")\n  )\n)\n\nprint(\"Correlation vs Regression Analysis:\")\nprint(correlation_results)\n\n# Test significance of correlation\nt_stat_cor &lt;- correlation_coef * sqrt((n - 2) / (1 - correlation_coef^2))\np_value_cor &lt;- 2 * (1 - pt(abs(t_stat_cor), n - 2))\n\ncorrelation_test &lt;- data.table(\n  Test = \"Correlation significance\",\n  H0 = \"ρ = 0 (no linear relationship)\",\n  t_statistic = round(t_stat_cor, 4),\n  p_value = round(p_value_cor, 4),\n  decision = ifelse(p_value_cor &lt; alpha, \"Reject H₀\", \"Fail to reject H₀\"),\n  conclusion = ifelse(p_value_cor &lt; alpha, \"Significant correlation\", \"No significant correlation\")\n)\n\nprint(\"Correlation Significance Test:\")\nprint(correlation_test)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#multiple-regression",
    "href": "book/Ch06.html#multiple-regression",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteMultiple Linear Regression Model\n\n\n\nThe multiple regression model with k predictors:\ny_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik} + \\epsilon_i\nMatrix Form:\n\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\nwhere:\n\\mathbf{X} = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1k} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2k} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{nk}\n\\end{bmatrix}\nLeast Squares Solution:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nProperties:\n\nUnbiased: E[\\hat{\\boldsymbol{\\beta}}] = \\boldsymbol{\\beta}\nMinimum variance among linear unbiased estimators\nNormally distributed under normality assumption\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA chemical engineer studies how reaction temperature and catalyst concentration affect yield:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Chemical Process Data Summary:\"\n\n\n       n Temp_Mean  Temp_SD Conc_Mean  Conc_SD Yield_Mean Yield_SD\n   &lt;int&gt;     &lt;num&gt;    &lt;num&gt;     &lt;num&gt;    &lt;num&gt;      &lt;num&gt;    &lt;num&gt;\n1:    25  175.6184 13.62157  21.08764 6.724034   192.3572 14.55564\n\n\n[1] \"Correlation Matrix:\"\n\n\n              temperature concentration  yield\ntemperature        1.0000       -0.0674 0.7262\nconcentration     -0.0674        1.0000 0.5652\nyield              0.7262        0.5652 1.0000\n\n\n\n\n\n# Multiple regression example - Chemical process yield\nset.seed(456)\nchemical_data &lt;- data.table(\n  run = 1:25,\n  temperature = runif(25, min = 150, max = 200),\n  concentration = runif(25, min = 10, max = 30),\n  yield = NULL\n)\n\n# Create realistic multiple regression relationship\nchemical_data[, yield := 20 + 0.8 * temperature + 1.5 * concentration + rnorm(25, mean = 0, sd = 5)]\n\n# Summary statistics\nchemical_summary &lt;- chemical_data %&gt;%\n  fsummarise(\n    n = fnobs(run),\n    Temp_Mean = fmean(temperature),\n    Temp_SD = fsd(temperature),\n    Conc_Mean = fmean(concentration),\n    Conc_SD = fsd(concentration),\n    Yield_Mean = fmean(yield),\n    Yield_SD = fsd(yield)\n  )\n\nprint(\"Chemical Process Data Summary:\")\nprint(chemical_summary)\n\n# Correlation matrix\ncor_matrix &lt;- cor(chemical_data[, .(temperature, concentration, yield)])\nprint(\"Correlation Matrix:\")\nprint(round(cor_matrix, 4))\n\n# Write data to CSV\nfwrite(chemical_data, \"data/chemical_yield.csv\")\n\n\n\n\n\n\n\nMultiple Regression Model:\nYield = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Concentration + \\epsilon\nManual Matrix Calculation:\n\nDesign Matrix X: \\mathbf{X} = \\begin{bmatrix}\n1 & x_{1,temp} & x_{1,conc} \\\\\n1 & x_{2,temp} & x_{2,conc} \\\\\n\\vdots & \\vdots & \\vdots \\\\\n1 & x_{n,temp} & x_{n,conc}\n\\end{bmatrix}\nParameter Estimates: \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Manual Multiple Regression Results:\"\n\n\n            Parameter Estimate Std_Error t_value p_value\n               &lt;char&gt;    &lt;num&gt;     &lt;num&gt;   &lt;num&gt;   &lt;num&gt;\n1:     β₀ (Intercept)  20.1051   12.8589  1.5635  0.1322\n2:   β₁ (Temperature)   0.8205    0.0699 11.7400  0.0000\n3: β₂ (Concentration)   1.3355    0.1416  9.4333  0.0000\n\n\n[1] \"Model Summary Statistics:\"\n\n\n     Statistic    Value\n        &lt;char&gt;    &lt;num&gt;\n1:          R²   0.9063\n2: Adjusted R²   0.8978\n3: Residual SE   4.6530\n4: F-statistic 106.4279\n\n\n[1] \"R's lm() verification:\"\n\n\n\nCall:\nlm(formula = yield ~ temperature + concentration, data = chemical_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.6632 -1.8041 -0.8522  1.3582 10.2857 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   20.10507   12.85893   1.564    0.132    \ntemperature    0.82047    0.06989  11.740 6.05e-11 ***\nconcentration  1.33553    0.14158   9.433 3.45e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.653 on 22 degrees of freedom\nMultiple R-squared:  0.9063,    Adjusted R-squared:  0.8978 \nF-statistic: 106.4 on 2 and 22 DF,  p-value: 4.873e-12\n\n\n\n\n\n# Multiple regression - manual matrix calculations\n# Design matrix X\nn_multi &lt;- nrow(chemical_data)\nX &lt;- cbind(1, chemical_data$temperature, chemical_data$concentration)\ny &lt;- chemical_data$yield\n\n# Manual matrix calculations\nXtX &lt;- t(X) %*% X\nXty &lt;- t(X) %*% y\nbeta_hat &lt;- solve(XtX) %*% Xty\n\n# Extract coefficients\nbeta0_multi &lt;- beta_hat[1, 1]\nbeta1_multi &lt;- beta_hat[2, 1]\nbeta2_multi &lt;- beta_hat[3, 1]\n\n# Calculate fitted values and residuals\ny_fitted &lt;- X %*% beta_hat\nresiduals_multi &lt;- y - y_fitted\n\n# Sum of squares\nSST_multi &lt;- sum((y - mean(y))^2)\nSSE_multi &lt;- sum(residuals_multi^2)\nSSR_multi &lt;- SST_multi - SSE_multi\n\n# Degrees of freedom\nk &lt;- 2 # number of predictors\ndf_regression &lt;- k\ndf_error_multi &lt;- n_multi - k - 1\ndf_total &lt;- n_multi - 1\n\n# Mean squares\nMSR_multi &lt;- SSR_multi / df_regression\nMSE_multi &lt;- SSE_multi / df_error_multi\n\n# R-squared and adjusted R-squared\nR_squared_multi &lt;- SSR_multi / SST_multi\nR_squared_adj &lt;- 1 - (SSE_multi / df_error_multi) / (SST_multi / df_total)\n\n# Standard errors of coefficients\nvar_covar_matrix &lt;- MSE_multi * solve(XtX)\nSE_beta0_multi &lt;- sqrt(var_covar_matrix[1, 1])\nSE_beta1_multi &lt;- sqrt(var_covar_matrix[2, 2])\nSE_beta2_multi &lt;- sqrt(var_covar_matrix[3, 3])\n\n# Manual multiple regression results\nmultiple_manual_results &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Estimate = c(round(beta0_multi, 4), round(beta1_multi, 4), round(beta2_multi, 4)),\n  Std_Error = c(round(SE_beta0_multi, 4), round(SE_beta1_multi, 4), round(SE_beta2_multi, 4)),\n  t_value = c(\n    round(beta0_multi / SE_beta0_multi, 4),\n    round(beta1_multi / SE_beta1_multi, 4),\n    round(beta2_multi / SE_beta2_multi, 4)\n  ),\n  p_value = c(\n    round(2 * (1 - pt(abs(beta0_multi / SE_beta0_multi), df_error_multi)), 4),\n    round(2 * (1 - pt(abs(beta1_multi / SE_beta1_multi), df_error_multi)), 4),\n    round(2 * (1 - pt(abs(beta2_multi / SE_beta2_multi), df_error_multi)), 4)\n  )\n)\n\nprint(\"Manual Multiple Regression Results:\")\nprint(multiple_manual_results)\n\n# Model summary statistics\nmodel_stats &lt;- data.table(\n  Statistic = c(\"R²\", \"Adjusted R²\", \"Residual SE\", \"F-statistic\"),\n  Value = c(\n    round(R_squared_multi, 4),\n    round(R_squared_adj, 4),\n    round(sqrt(MSE_multi), 4),\n    round(MSR_multi / MSE_multi, 4)\n  )\n)\n\nprint(\"Model Summary Statistics:\")\nprint(model_stats)\n\n# Verify with R's lm function\nlm_multi &lt;- lm(yield ~ temperature + concentration, data = chemical_data)\nprint(\"R's lm() verification:\")\nprint(summary(lm_multi))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 4: Multiple regression visualization\n\n\n\n\n\n\n\n\n# Multiple regression visualization\n# Create 3D-like visualization using different perspectives\n\n# Partial regression plots\np1 &lt;- ggplot(data = chemical_data, aes(x = temperature, y = yield)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(x = \"Temperature\", y = \"Yield\", title = \"Yield vs Temperature\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2 &lt;- ggplot(data = chemical_data, aes(x = concentration, y = yield)) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(x = \"Concentration\", y = \"Yield\", title = \"Yield vs Concentration\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Fitted vs actual\nfitted_multi &lt;- as.vector(y_fitted)\np3 &lt;- ggplot(\n  data = data.table(fitted = fitted_multi, actual = y),\n  aes(x = fitted, y = actual)\n) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Fitted Values\", y = \"Actual Values\", title = \"Fitted vs Actual\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Residuals vs fitted\nresiduals_multi_vec &lt;- as.vector(residuals_multi)\np4 &lt;- ggplot(\n  data = data.table(fitted = fitted_multi, residuals = residuals_multi_vec),\n  aes(x = fitted, y = residuals)\n) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Testing in Multiple Regression\n\n\n\nOverall Significance Test:\nH₀: β₁ = β₂ = … = βₖ = 0 vs H₁: At least one βⱼ ≠ 0\nF-Test:\nF_0 = \\frac{MS_R}{MS_E} = \\frac{SS_R/k}{SS_E/(n-k-1)}\nIndividual Parameter Tests:\nH₀: βⱼ = 0 vs H₁: βⱼ ≠ 0\nt-Test:\nt_0 = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\nANOVA Table:\n\n\n\nSource\nSS\ndf\nMS\nF\n\n\n\n\nRegression\nSS_R\nk\nMS_R\nMS_R/MS_E\n\n\nError\nSS_E\nn-k-1\nMS_E\n\n\n\nTotal\nSS_T\nn-1\n\n\n\n\n\nAdjusted R²:\nR_{adj}^2 = 1 - \\frac{SS_E/(n-k-1)}{SS_T/(n-1)} = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Multiple Regression Hypothesis Test Results:\"\n\n\n            Parameter Test_Type Null_Hypothesis Test_Statistic P_Value\n               &lt;char&gt;    &lt;char&gt;          &lt;char&gt;          &lt;num&gt;   &lt;num&gt;\n1:      Overall Model    F-test     β₁ = β₂ = 0       106.4279  0.0000\n2:     β₀ (Intercept)    t-test          β₀ = 0         1.5635  0.1322\n3:   β₁ (Temperature)    t-test          β₁ = 0        11.7400  0.0000\n4: β₂ (Concentration)    t-test          β₂ = 0         9.4333  0.0000\n            Decision                   Interpretation\n              &lt;char&gt;                           &lt;char&gt;\n1:         Reject H₀             Model is significant\n2: Fail to reject H₀        Intercept not significant\n3:         Reject H₀   Temperature effect significant\n4:         Reject H₀ Concentration effect significant\n\n\n[1] \"Multiple Regression ANOVA Table:\"\n\n\n       Source      SS    df      MS F_Statistic P_Value\n       &lt;char&gt;   &lt;num&gt; &lt;num&gt;   &lt;num&gt;       &lt;num&gt;   &lt;num&gt;\n1: Regression 4608.48     2 2304.24    106.4279       0\n2:      Error  476.32    22   21.65          NA      NA\n3:      Total 5084.80    24      NA          NA      NA\n\n\n[1] \"95% Confidence Intervals for Parameters:\"\n\n\nError in Math.data.frame(structure(list(Parameter = c(\"β₀ (Intercept)\", : non-numeric-alike variable(s) in data frame: Parameter\n\n\n\n\n\n# Multiple regression inference\nalpha_multi &lt;- 0.05\nt_critical_multi &lt;- qt(1 - alpha_multi / 2, df_error_multi)\nF_critical_multi &lt;- qf(1 - alpha_multi, df_regression, df_error_multi)\n\n# Overall F-test\nF_stat_multi &lt;- MSR_multi / MSE_multi\np_value_F_multi &lt;- 1 - pf(F_stat_multi, df_regression, df_error_multi)\n\n# Individual t-tests\nt_stats &lt;- c(\n  beta0_multi / SE_beta0_multi,\n  beta1_multi / SE_beta1_multi,\n  beta2_multi / SE_beta2_multi\n)\n\np_values_t &lt;- 2 * (1 - pt(abs(t_stats), df_error_multi))\n\n# Hypothesis test results\nhypothesis_tests_multi &lt;- data.table(\n  Parameter = c(\"Overall Model\", \"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Test_Type = c(\"F-test\", \"t-test\", \"t-test\", \"t-test\"),\n  Null_Hypothesis = c(\"β₁ = β₂ = 0\", \"β₀ = 0\", \"β₁ = 0\", \"β₂ = 0\"),\n  Test_Statistic = c(round(F_stat_multi, 4), round(t_stats, 4)),\n  P_Value = c(round(p_value_F_multi, 4), round(p_values_t, 4)),\n  Decision = c(\n    ifelse(p_value_F_multi &lt; alpha_multi, \"Reject H₀\", \"Fail to reject H₀\"),\n    ifelse(p_values_t &lt; alpha_multi, \"Reject H₀\", \"Fail to reject H₀\")\n  ),\n  Interpretation = c(\n    ifelse(p_value_F_multi &lt; alpha_multi, \"Model is significant\", \"Model not significant\"),\n    ifelse(p_values_t[1] &lt; alpha_multi, \"Intercept significant\", \"Intercept not significant\"),\n    ifelse(p_values_t[2] &lt; alpha_multi, \"Temperature effect significant\", \"Temperature not significant\"),\n    ifelse(p_values_t[3] &lt; alpha_multi, \"Concentration effect significant\", \"Concentration not significant\")\n  )\n)\n\nprint(\"Multiple Regression Hypothesis Test Results:\")\nprint(hypothesis_tests_multi)\n\n# ANOVA table for multiple regression\nanova_table_multi &lt;- data.table(\n  Source = c(\"Regression\", \"Error\", \"Total\"),\n  SS = c(round(SSR_multi, 2), round(SSE_multi, 2), round(SST_multi, 2)),\n  df = c(df_regression, df_error_multi, df_total),\n  MS = c(round(MSR_multi, 2), round(MSE_multi, 2), NA),\n  F_Statistic = c(round(F_stat_multi, 4), NA, NA),\n  P_Value = c(round(p_value_F_multi, 4), NA, NA)\n)\n\nprint(\"Multiple Regression ANOVA Table:\")\nprint(anova_table_multi)\n\n# Confidence intervals for parameters\nCI_multi &lt;- data.table(\n  Parameter = c(\"β₀ (Intercept)\", \"β₁ (Temperature)\", \"β₂ (Concentration)\"),\n  Estimate = c(beta0_multi, beta1_multi, beta2_multi),\n  Lower_95CI = c(\n    beta0_multi - t_critical_multi * SE_beta0_multi,\n    beta1_multi - t_critical_multi * SE_beta1_multi,\n    beta2_multi - t_critical_multi * SE_beta2_multi\n  ),\n  Upper_95CI = c(\n    beta0_multi + t_critical_multi * SE_beta0_multi,\n    beta1_multi + t_critical_multi * SE_beta1_multi,\n    beta2_multi + t_critical_multi * SE_beta2_multi\n  )\n)\n\nprint(\"95% Confidence Intervals for Parameters:\")\nprint(round(CI_multi, 4))\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMultiple Regression Diagnostics\n\n\n\nKey Diagnostic Plots:\n\nResiduals vs. Fitted: Check linearity and constant variance\nNormal Q-Q Plot: Check normality of residuals\nScale-Location: Check homoscedasticity\nResiduals vs. Leverage: Identify influential points\n\nMulticollinearity Detection:\n\nVariance Inflation Factor (VIF):\n\nVIF_j = \\frac{1}{1-R_j^2}\nwhere R_j^2 is the R² from regressing x_j on other predictors\n\nRule of thumb: VIF &gt; 10 indicates serious multicollinearity\n\nInfluential Observations:\n\nLeverage: h_{ii} (diagonal elements of hat matrix)\nCook’s Distance: Measures influence on fitted values\nStudentized Residuals: Better for outlier detection\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Multiple Regression Diagnostic Summary:\"\n\n\n                    Diagnostic  Value Threshold            Concern\n                        &lt;char&gt;  &lt;num&gt;    &lt;char&gt;             &lt;char&gt;\n1: Max |Standardized Residual| 2.2105       2-3 Potential outliers\n2:  Max |Studentized Residual| 2.3367       2-3 Potential outliers\n3:                Max Leverage 0.1960    &gt; 0.24                 OK\n4:         Max Cook's Distance 0.2256         1                 OK\n5:             VIF Temperature 1.0046        10                 OK\n6:           VIF Concentration 1.0046        10                 OK\n\n\n[1] \"Flagged observations:\"\n   observation standardized_resid studentized_resid leverage cooks_distance\n         &lt;int&gt;              &lt;num&gt;             &lt;num&gt;    &lt;num&gt;          &lt;num&gt;\n1:           8             -1.862            -1.989    0.124          0.163\n2:          11              2.211             2.337    0.105          0.191\n3:          20              1.895             2.066    0.159          0.226\n   outlier high_leverage influential\n    &lt;lgcl&gt;        &lt;lgcl&gt;      &lt;lgcl&gt;\n1:   FALSE         FALSE        TRUE\n2:    TRUE         FALSE        TRUE\n3:   FALSE         FALSE        TRUE\n\n\n\n\n\n# Multiple regression diagnostics\n# Calculate leverages (diagonal elements of hat matrix)\nH &lt;- X %*% solve(XtX) %*% t(X)\nleverages &lt;- diag(H)\n\n# Standardized and studentized residuals\nstandardized_resid_multi &lt;- residuals_multi_vec / sqrt(MSE_multi)\nstudentized_resid_multi &lt;- residuals_multi_vec / sqrt(MSE_multi * (1 - leverages))\n\n# Cook's distance\ncooks_distance &lt;- (standardized_resid_multi^2 / (k + 1)) * (leverages / (1 - leverages))\n\n# VIF calculations (for multicollinearity)\n# VIF for temperature (regress temp on concentration)\ntemp_on_conc &lt;- lm(temperature ~ concentration, data = chemical_data)\nR2_temp &lt;- summary(temp_on_conc)$r.squared\nVIF_temp &lt;- 1 / (1 - R2_temp)\n\n# VIF for concentration (regress conc on temperature)\nconc_on_temp &lt;- lm(concentration ~ temperature, data = chemical_data)\nR2_conc &lt;- summary(conc_on_temp)$r.squared\nVIF_conc &lt;- 1 / (1 - R2_conc)\n\n# Diagnostic summary\ndiagnostic_summary &lt;- data.table(\n  Diagnostic = c(\n    \"Max |Standardized Residual|\", \"Max |Studentized Residual|\",\n    \"Max Leverage\", \"Max Cook's Distance\", \"VIF Temperature\", \"VIF Concentration\"\n  ),\n  Value = c(\n    round(max(abs(standardized_resid_multi)), 4),\n    round(max(abs(studentized_resid_multi)), 4),\n    round(max(leverages), 4),\n    round(max(cooks_distance), 4),\n    round(VIF_temp, 4),\n    round(VIF_conc, 4)\n  ),\n  Threshold = c(\"2-3\", \"2-3\", paste(\"&gt;\", round(2 * (k + 1) / n_multi, 3)), \"1\", \"10\", \"10\"),\n  Concern = c(\n    ifelse(max(abs(standardized_resid_multi)) &gt; 2, \"Potential outliers\", \"OK\"),\n    ifelse(max(abs(studentized_resid_multi)) &gt; 2, \"Potential outliers\", \"OK\"),\n    ifelse(max(leverages) &gt; 2 * (k + 1) / n_multi, \"High leverage points\", \"OK\"),\n    ifelse(max(cooks_distance) &gt; 1, \"Influential observations\", \"OK\"),\n    ifelse(VIF_temp &gt; 10, \"Serious multicollinearity\", \"OK\"),\n    ifelse(VIF_conc &gt; 10, \"Serious multicollinearity\", \"OK\")\n  )\n)\n\nprint(\"Multiple Regression Diagnostic Summary:\")\nprint(diagnostic_summary)\n\n# Identify specific problematic observations\nproblem_obs &lt;- data.table(\n  observation = 1:n_multi,\n  standardized_resid = round(standardized_resid_multi, 3),\n  studentized_resid = round(studentized_resid_multi, 3),\n  leverage = round(leverages, 3),\n  cooks_distance = round(cooks_distance, 3)\n)\n\n# Flag potential problems\nproblem_obs[, outlier := abs(standardized_resid) &gt; 2]\nproblem_obs[, high_leverage := leverage &gt; 2 * (k + 1) / n_multi]\nproblem_obs[, influential := cooks_distance &gt; 4 / n_multi] # Conservative threshold\n\nflagged_obs &lt;- problem_obs[outlier == TRUE | high_leverage == TRUE | influential == TRUE]\n\nif (nrow(flagged_obs) &gt; 0) {\n  print(\"Flagged observations:\")\n  print(flagged_obs)\n} else {\n  print(\"No observations flagged for potential problems\")\n}\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 5: Multiple regression diagnostic plots\n\n\n\n\n\n\n\n\n# Multiple regression diagnostic plots\nmulti_diagnostic_data &lt;- data.table(\n  fitted = fitted_multi,\n  residuals = residuals_multi_vec,\n  standardized_resid = standardized_resid_multi,\n  studentized_resid = studentized_resid_multi,\n  leverage = leverages,\n  cooks_distance = cooks_distance,\n  observation = 1:n_multi\n)\n\n# 1. Residuals vs Fitted\np1_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Normal Q-Q plot\np2_multi &lt;- ggplot(data = multi_diagnostic_data, aes(sample = standardized_resid)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(\n    x = \"Theoretical Quantiles\", y = \"Standardized Residuals\",\n    title = \"Normal Q-Q Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Scale-Location\np3_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = fitted, y = sqrt(abs(standardized_resid)))) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(se = FALSE, color = \"red\") +\n  labs(\n    x = \"Fitted Values\", y = \"√|Standardized Residuals|\",\n    title = \"Scale-Location\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Residuals vs Leverage\np4_multi &lt;- ggplot(data = multi_diagnostic_data, aes(x = leverage, y = studentized_resid)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 2 * (k + 1) / n_multi, linetype = \"dashed\", color = \"blue\") +\n  labs(\n    x = \"Leverage\", y = \"Studentized Residuals\",\n    title = \"Residuals vs Leverage\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1_multi, p2_multi, p3_multi, p4_multi, nrow = 2, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#other-aspects-of-regression",
    "href": "book/Ch06.html#other-aspects-of-regression",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NotePolynomial Regression\n\n\n\nWhen the relationship is curved, use polynomial models:\nSecond-Order (Quadratic):\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\epsilon\nThird-Order (Cubic):\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\epsilon\nInterpretation:\n\nβ₁: Linear effect\nβ₂: Quadratic effect (curvature)\nβ₃: Cubic effect (inflection)\n\nModel Selection:\n\nStart with linear model\nAdd higher-order terms if needed\nUse hypothesis tests to determine necessary degree\nBe cautious of overfitting\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA process engineer studies how temperature affects reaction rate and suspects a nonlinear relationship:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Reaction Rate vs Temperature Data:\"\n\n\n    temperature      rate\n          &lt;num&gt;     &lt;num&gt;\n 1:         100  69.57229\n 2:         110  66.01070\n 3:         120  77.32496\n 4:         130  82.34042\n 5:         140  84.94795\n 6:         150  88.67155\n 7:         160  92.08906\n 8:         170  97.41559\n 9:         180  98.66312\n10:         190 107.59609\n\n\n\n\n\n# Polynomial regression example\nset.seed(789)\nreaction_data &lt;- data.table(\n  temperature = seq(100, 300, by = 10),\n  rate = NULL\n)\n\n# Create nonlinear relationship\nreaction_data[, rate := 5 + 0.8 * temperature - 0.002 * temperature^2 +\n  0.000003 * temperature^3 + rnorm(21, mean = 0, sd = 3)]\n\nprint(\"Reaction Rate vs Temperature Data:\")\nprint(head(reaction_data, 10))\n\n# Write data to CSV\nfwrite(reaction_data, \"data/reaction_rate.csv\")\n\n\n\n\n\n\n\nModel Comparison:\n\nLinear: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\epsilon\nQuadratic: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Temperature^2 + \\epsilon\nCubic: Rate = \\beta_0 + \\beta_1 \\times Temperature + \\beta_2 \\times Temperature^2 + \\beta_3 \\times Temperature^3 + \\epsilon\n\n\nR OutputR Code\n\n\n\n\n[1] \"Polynomial Model Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Linear\", \"Quadratic\", : non-numeric-alike variable(s) in data frame: Model, Formula\n\n\n[1] \"F-test: Linear vs Quadratic\"\n\n\nAnalysis of Variance Table\n\nModel 1: rate ~ temperature\nModel 2: rate ~ temperature + I(temperature^2)\n  Res.Df     RSS Df Sum of Sq      F  Pr(&gt;F)  \n1     19 103.809                              \n2     18  86.945  1    16.864 3.4914 0.07805 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[1] \"F-test: Quadratic vs Cubic\"\n\n\nAnalysis of Variance Table\n\nModel 1: rate ~ temperature + I(temperature^2)\nModel 2: rate ~ temperature + I(temperature^2) + I(temperature^3)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     18 86.945                           \n2     17 86.571  1   0.37395 0.0734 0.7897\n\n\n[1] \"Best model based on Adjusted R²: Quadratic\"\n\n\n[1] \"Quadratic Model Summary:\"\n\nCall:\nlm(formula = rate ~ temperature + I(temperature^2), data = reaction_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.6671 -1.2600  0.1417  1.5729  3.1584 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      20.9024617  5.5825087   3.744  0.00148 ** \ntemperature       0.4917544  0.0592276   8.303 1.44e-07 ***\nI(temperature^2) -0.0002742  0.0001467  -1.869  0.07805 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.198 on 18 degrees of freedom\nMultiple R-squared:  0.9923,    Adjusted R-squared:  0.9915 \nF-statistic:  1165 on 2 and 18 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\n# Fit polynomial models of different degrees\n# Linear model\nlm_linear &lt;- lm(rate ~ temperature, data = reaction_data)\n\n# Quadratic model\nlm_quad &lt;- lm(rate ~ temperature + I(temperature^2), data = reaction_data)\n\n# Cubic model\nlm_cubic &lt;- lm(rate ~ temperature + I(temperature^2) + I(temperature^3), data = reaction_data)\n\n# Model comparison\nmodel_comparison &lt;- data.table(\n  Model = c(\"Linear\", \"Quadratic\", \"Cubic\"),\n  Formula = c(\n    \"rate ~ temperature\",\n    \"rate ~ temperature + temperature²\",\n    \"rate ~ temperature + temperature² + temperature³\"\n  ),\n  R_squared = c(\n    summary(lm_linear)$r.squared,\n    summary(lm_quad)$r.squared,\n    summary(lm_cubic)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_linear)$adj.r.squared,\n    summary(lm_quad)$adj.r.squared,\n    summary(lm_cubic)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_linear), AIC(lm_quad), AIC(lm_cubic)),\n  BIC = c(BIC(lm_linear), BIC(lm_quad), BIC(lm_cubic)),\n  F_statistic = c(\n    summary(lm_linear)$fstatistic[1],\n    summary(lm_quad)$fstatistic[1],\n    summary(lm_cubic)$fstatistic[1]\n  ),\n  p_value = c(\n    pf(summary(lm_linear)$fstatistic[1],\n      summary(lm_linear)$fstatistic[2],\n      summary(lm_linear)$fstatistic[3],\n      lower.tail = FALSE\n    ),\n    pf(summary(lm_quad)$fstatistic[1],\n      summary(lm_quad)$fstatistic[2],\n      summary(lm_quad)$fstatistic[3],\n      lower.tail = FALSE\n    ),\n    pf(summary(lm_cubic)$fstatistic[1],\n      summary(lm_cubic)$fstatistic[2],\n      summary(lm_cubic)$fstatistic[3],\n      lower.tail = FALSE\n    )\n  )\n)\n\nprint(\"Polynomial Model Comparison:\")\nprint(round(model_comparison, 4))\n\n# Sequential F-tests for additional terms\n# Test if quadratic term improves linear model\nanova_linear_quad &lt;- anova(lm_linear, lm_quad)\nprint(\"F-test: Linear vs Quadratic\")\nprint(anova_linear_quad)\n\n# Test if cubic term improves quadratic model\nanova_quad_cubic &lt;- anova(lm_quad, lm_cubic)\nprint(\"F-test: Quadratic vs Cubic\")\nprint(anova_quad_cubic)\n\n# Detailed summary of best model (based on adj R²)\nbest_model_idx &lt;- which.max(model_comparison$Adj_R_squared)\nbest_model_name &lt;- model_comparison$Model[best_model_idx]\nprint(paste(\"Best model based on Adjusted R²:\", best_model_name))\n\nif (best_model_name == \"Cubic\") {\n  print(\"Cubic Model Summary:\")\n  print(summary(lm_cubic))\n} else if (best_model_name == \"Quadratic\") {\n  print(\"Quadratic Model Summary:\")\n  print(summary(lm_quad))\n} else {\n  print(\"Linear Model Summary:\")\n  print(summary(lm_linear))\n}\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 6: Polynomial regression models comparison\n\n\n\n\n\n\n\n\n# Polynomial regression visualization\n# Create prediction data for smooth curves\ntemp_pred &lt;- seq(min(reaction_data$temperature), max(reaction_data$temperature), length.out = 100)\npred_data &lt;- data.table(temperature = temp_pred)\n\n# Predictions from each model\npred_linear &lt;- predict(lm_linear, newdata = pred_data)\npred_quad &lt;- predict(lm_quad, newdata = pred_data)\npred_cubic &lt;- predict(lm_cubic, newdata = pred_data)\n\n# Combine predictions\nplot_data &lt;- data.table(\n  temperature = rep(temp_pred, 3),\n  predicted_rate = c(pred_linear, pred_quad, pred_cubic),\n  model = rep(c(\"Linear\", \"Quadratic\", \"Cubic\"), each = length(temp_pred))\n)\n\n# Plot all models\nggplot() +\n  geom_point(\n    data = reaction_data, aes(x = temperature, y = rate),\n    size = 3, alpha = 0.7, color = \"black\"\n  ) +\n  geom_line(\n    data = plot_data, aes(x = temperature, y = predicted_rate, color = model),\n    linewidth = 1.2\n  ) +\n  scale_color_manual(values = c(\"Linear\" = \"red\", \"Quadratic\" = \"blue\", \"Cubic\" = \"green\")) +\n  labs(\n    x = \"Temperature\",\n    y = \"Reaction Rate\",\n    title = \"Polynomial Regression Models Comparison\",\n    subtitle = \"Comparing linear, quadratic, and cubic fits\",\n    color = \"Model\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDummy Variables for Categorical Predictors\n\n\n\nWhen predictors are categorical, use dummy variables:\nBinary Categorical Variable (2 levels):\nFor variable with levels A and B:\n\nx = 0 for level A\nx = 1 for level B\n\nMulti-level Categorical Variable (k levels):\nUse k-1 dummy variables:\nx_1 = \\begin{cases} 1 & \\text{if level 2} \\\\ 0 & \\text{otherwise} \\end{cases}\nx_2 = \\begin{cases} 1 & \\text{if level 3} \\\\ 0 & \\text{otherwise} \\end{cases}\n\\vdots\nInterpretation:\n\nReference level: All dummy variables = 0\nβ₀: Mean response for reference level\nβⱼ: Difference in mean response between level j+1 and reference level\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA quality engineer studies how supplier and heat treatment affect strength:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Categorical Variables Example - First 15 observations:\"\n\n\n    observation supplier heat_treatment  strength\n          &lt;int&gt;   &lt;char&gt;         &lt;char&gt;     &lt;num&gt;\n 1:           1        A             No  96.20567\n 2:           2        A            Yes 121.76799\n 3:           3        A             No  98.99465\n 4:           4        A            Yes 112.79896\n 5:           5        A             No 106.48668\n 6:           6        A            Yes 106.10137\n 7:           7        A             No  95.80303\n 8:           8        A            Yes 119.79145\n 9:           9        A             No 102.14922\n10:          10        A            Yes 117.88237\n11:          11        A             No 100.10912\n12:          12        A            Yes 115.25777\n13:          13        A             No  92.78922\n14:          14        A            Yes 130.48557\n15:          15        A             No 103.01518\n\n\n[1] \"Summary by Supplier and Heat Treatment:\"\n\n\n   supplier heat_treatment     n mean_strength sd_strength min_strength\n     &lt;char&gt;         &lt;char&gt; &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:        A             No    10      99.89853    4.317355     92.78922\n2:        A            Yes    10     117.80654    6.941354    106.10137\n3:        B             No    10     111.01700    5.039307    100.85786\n4:        B            Yes    10     125.45412    4.000820    117.65973\n5:        C             No    10     102.42515    4.899730     94.05402\n6:        C            Yes    10     131.78177    6.143025    119.69643\n   max_strength\n          &lt;num&gt;\n1:     106.4867\n2:     130.4856\n3:     117.6057\n4:     129.8654\n5:     110.6014\n6:     141.1987\n\n\n\n\n\n# Categorical variables example\nset.seed(101112)\nstrength_cat_data &lt;- data.table(\n  observation = 1:60,\n  supplier = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  heat_treatment = rep(c(\"No\", \"Yes\"), 30),\n  strength = NULL\n)\n\n# Create realistic effects\n# Base strength: 100\n# Supplier B: +10, Supplier C: +5 (vs A)\n# Heat treatment: +15\n# Some interaction: Supplier C with heat treatment gets extra +8\nstrength_cat_data[, strength := 100 +\n  ifelse(supplier == \"B\", 10, 0) +\n  ifelse(supplier == \"C\", 5, 0) +\n  ifelse(heat_treatment == \"Yes\", 15, 0) +\n  ifelse(supplier == \"C\" & heat_treatment == \"Yes\", 8, 0) +\n  rnorm(60, mean = 0, sd = 5)]\n\nprint(\"Categorical Variables Example - First 15 observations:\")\nprint(head(strength_cat_data, 15))\n\n# Summary by groups\ngroup_summary &lt;- strength_cat_data %&gt;%\n  fgroup_by(supplier, heat_treatment) %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    mean_strength = fmean(strength),\n    sd_strength = fsd(strength),\n    min_strength = fmin(strength),\n    max_strength = fmax(strength)\n  )\n\nprint(\"Summary by Supplier and Heat Treatment:\")\nprint(group_summary)\n\n# Write data to CSV\nfwrite(strength_cat_data, \"data/strength_categorical.csv\")\n\n\n\n\n\n\n\nModel with Categorical Variables:\nStrength = \\beta_0 + \\beta_1 \\times Supplier_B + \\beta_2 \\times Supplier_C + \\beta_3 \\times HeatTreat_{Yes} + \\epsilon\nInterpretation:\n\nβ₀: Mean strength for Supplier A, no heat treatment\nβ₁: Effect of Supplier B vs. Supplier A\nβ₂: Effect of Supplier C vs. Supplier A\nβ₃: Effect of heat treatment\n\n\nR OutputR Code\n\n\n\n\n[1] \"Main Effects Model:\"\n\n\n\nCall:\nlm(formula = strength ~ supplier + heat_treatment, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.0348  -4.0101   0.5405   3.7302  13.8116 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         98.569      1.596  61.766  &lt; 2e-16 ***\nsupplierB            9.383      1.954   4.801 1.22e-05 ***\nsupplierC            8.251      1.954   4.222 8.99e-05 ***\nheat_treatmentYes   20.567      1.596  12.888  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.181 on 56 degrees of freedom\nMultiple R-squared:  0.7756,    Adjusted R-squared:  0.7636 \nF-statistic: 64.52 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Model with Interaction:\"\n\n\n\nCall:\nlm(formula = strength ~ supplier * heat_treatment, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.0853  -2.9002   0.1333   3.1805  12.6790 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   99.899      1.683  59.354  &lt; 2e-16 ***\nsupplierB                     11.118      2.380   4.671 2.03e-05 ***\nsupplierC                      2.527      2.380   1.061  0.29319    \nheat_treatmentYes             17.908      2.380   7.524 5.82e-10 ***\nsupplierB:heat_treatmentYes   -3.471      3.366  -1.031  0.30709    \nsupplierC:heat_treatmentYes   11.449      3.366   3.401  0.00127 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.322 on 54 degrees of freedom\nMultiple R-squared:  0.8395,    Adjusted R-squared:  0.8247 \nF-statistic: 56.51 on 5 and 54 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Manual Dummy Variable Model (should match main effects):\"\n\n\n\nCall:\nlm(formula = strength ~ supplier_B + supplier_C + heat_yes, data = strength_cat_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.0348  -4.0101   0.5405   3.7302  13.8116 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   98.569      1.596  61.766  &lt; 2e-16 ***\nsupplier_B     9.383      1.954   4.801 1.22e-05 ***\nsupplier_C     8.251      1.954   4.222 8.99e-05 ***\nheat_yes      20.567      1.596  12.888  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.181 on 56 degrees of freedom\nMultiple R-squared:  0.7756,    Adjusted R-squared:  0.7636 \nF-statistic: 64.52 on 3 and 56 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Coefficient Interpretation:\"\n\n\n      Coefficient Value                                  Interpretation\n           &lt;char&gt; &lt;num&gt;                                          &lt;char&gt;\n1:      Intercept 98.57 Mean strength for Supplier A, no heat treatment\n2:     Supplier B  9.38         Additional strength for Supplier B vs A\n3:     Supplier C  8.25         Additional strength for Supplier C vs A\n4: Heat Treatment 20.57         Additional strength from heat treatment\n\n\n[1] \"F-test for overall supplier effect:\"\n\n\nAnalysis of Variance Table\n\nModel 1: strength ~ heat_treatment\nModel 2: strength ~ supplier + heat_treatment\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     58 3188.6                                  \n2     56 2139.2  2    1049.3 13.735 1.401e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n[1] \"F-test for interaction effect:\"\n\n\nAnalysis of Variance Table\n\nModel 1: strength ~ supplier + heat_treatment\nModel 2: strength ~ supplier * heat_treatment\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     56 2139.2                                  \n2     54 1529.7  2    609.52 10.758 0.0001168 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n# Regression with categorical variables\n# R automatically creates dummy variables with factor variables\nstrength_cat_data[, supplier := factor(supplier)]\nstrength_cat_data[, heat_treatment := factor(heat_treatment)]\n\n# Main effects model\nlm_cat_main &lt;- lm(strength ~ supplier + heat_treatment, data = strength_cat_data)\n\n# Model with interaction\nlm_cat_interact &lt;- lm(strength ~ supplier * heat_treatment, data = strength_cat_data)\n\nprint(\"Main Effects Model:\")\nprint(summary(lm_cat_main))\n\nprint(\"Model with Interaction:\")\nprint(summary(lm_cat_interact))\n\n# Manual dummy variable creation for illustration\nstrength_cat_data[, supplier_B := ifelse(supplier == \"B\", 1, 0)]\nstrength_cat_data[, supplier_C := ifelse(supplier == \"C\", 1, 0)]\nstrength_cat_data[, heat_yes := ifelse(heat_treatment == \"Yes\", 1, 0)]\n\n# Manual model with dummy variables\nlm_manual_dummy &lt;- lm(strength ~ supplier_B + supplier_C + heat_yes, data = strength_cat_data)\n\nprint(\"Manual Dummy Variable Model (should match main effects):\")\nprint(summary(lm_manual_dummy))\n\n# Interpret coefficients\ncoef_interpretation &lt;- data.table(\n  Coefficient = c(\"Intercept\", \"Supplier B\", \"Supplier C\", \"Heat Treatment\"),\n  Value = round(coef(lm_cat_main), 2),\n  Interpretation = c(\n    \"Mean strength for Supplier A, no heat treatment\",\n    \"Additional strength for Supplier B vs A\",\n    \"Additional strength for Supplier C vs A\",\n    \"Additional strength from heat treatment\"\n  )\n)\n\nprint(\"Coefficient Interpretation:\")\nprint(coef_interpretation)\n\n# Test significance of categorical variables\n# Test overall supplier effect\nanova_supplier &lt;- anova(lm(strength ~ heat_treatment, data = strength_cat_data), lm_cat_main)\nprint(\"F-test for overall supplier effect:\")\nprint(anova_supplier)\n\n# Test interaction effect\nanova_interaction &lt;- anova(lm_cat_main, lm_cat_interact)\nprint(\"F-test for interaction effect:\")\nprint(anova_interaction)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Model Statistics Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Main Effects\", \"With Interaction\": non-numeric-alike variable(s) in data frame: Model\n\n\n\n\n\n\n\n\nFigure 7: Categorical variables in regression\n\n\n\n\n\n\n\n\n# Visualization of categorical variables\n# Box plots by groups\np1_cat &lt;- ggplot(data = strength_cat_data, aes(x = supplier, y = strength, fill = heat_treatment)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(\n    x = \"Supplier\", y = \"Strength\", fill = \"Heat Treatment\",\n    title = \"Strength by Supplier and Heat Treatment\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Interaction plot\ninteraction_means &lt;- strength_cat_data %&gt;%\n  fgroup_by(supplier, heat_treatment) %&gt;%\n  fsummarise(mean_strength = fmean(strength))\n\np2_cat &lt;- ggplot(data = interaction_means, aes(\n  x = supplier, y = mean_strength,\n  color = heat_treatment, group = heat_treatment\n)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 4) +\n  labs(\n    x = \"Supplier\", y = \"Mean Strength\", color = \"Heat Treatment\",\n    title = \"Interaction Plot\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Residuals from interaction model\nresid_interact &lt;- residuals(lm_cat_interact)\nfitted_interact &lt;- fitted(lm_cat_interact)\n\np3_cat &lt;- ggplot(\n  data = data.table(fitted = fitted_interact, residuals = resid_interact),\n  aes(x = fitted, y = residuals)\n) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Residuals vs Fitted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Model comparison table\nmodel_stats_cat &lt;- data.table(\n  Model = c(\"Main Effects\", \"With Interaction\"),\n  R_squared = c(summary(lm_cat_main)$r.squared, summary(lm_cat_interact)$r.squared),\n  Adj_R_squared = c(summary(lm_cat_main)$adj.r.squared, summary(lm_cat_interact)$adj.r.squared),\n  AIC = c(AIC(lm_cat_main), AIC(lm_cat_interact)),\n  F_statistic = c(summary(lm_cat_main)$fstatistic[1], summary(lm_cat_interact)$fstatistic[1])\n)\n\nprint(\"Model Statistics Comparison:\")\nprint(round(model_stats_cat, 3))\n\ngrid.arrange(p1_cat, p2_cat, p3_cat, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteModel Selection Methods\n\n\n\nForward Selection:\n\nStart with no variables\nAdd variables one by one\nStop when no improvement\n\nBackward Elimination:\n\nStart with all variables\nRemove variables one by one\nStop when all remaining are significant\n\nStepwise Selection:\n\nCombine forward and backward\nCan add or remove at each step\nMore flexible approach\n\nSelection Criteria:\n\nAIC (Akaike Information Criterion): AIC = n \\ln(SS_E/n) + 2(k+1)\nBIC (Bayesian Information Criterion): BIC = n \\ln(SS_E/n) + (k+1)\\ln(n)\nAdjusted R²: Penalizes for additional variables\nMallows’ Cp: C_p = \\frac{SS_E}{MS_E} + 2(k+1) - n\n\nBest Practices:\n\nUse multiple criteria\nCross-validate final model\nConsider subject matter knowledge\nAvoid overfitting\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nAn engineer has multiple potential predictors for product quality and wants to identify the most important ones:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Quality Prediction Data Summary:\"\n\n\n[1] \"Variable Summary:\"\n\n\n[1] \"Correlation Matrix:\"\n\n\n              temperature pressure flow_rate     pH catalyst_conc   time\ntemperature         1.000   -0.143     0.075  0.022         0.142  0.023\npressure           -0.143    1.000    -0.035  0.369        -0.114 -0.172\nflow_rate           0.075   -0.035     1.000  0.370         0.084 -0.061\npH                  0.022    0.369     0.370  1.000         0.121 -0.140\ncatalyst_conc       0.142   -0.114     0.084  0.121         1.000  0.071\ntime                0.023   -0.172    -0.061 -0.140         0.071  1.000\nquality             0.771    0.284     0.157  0.381         0.282 -0.017\n              quality\ntemperature     0.771\npressure        0.284\nflow_rate       0.157\npH              0.381\ncatalyst_conc   0.282\ntime           -0.017\nquality         1.000\n\n\n\n\n\n# Variable selection example\nset.seed(131415)\nquality_data &lt;- data.table(\n  observation = 1:50,\n  temperature = rnorm(50, mean = 200, sd = 20),\n  pressure = rnorm(50, mean = 10, sd = 2),\n  flow_rate = rnorm(50, mean = 5, sd = 1),\n  pH = rnorm(50, mean = 7, sd = 0.5),\n  catalyst_conc = rnorm(50, mean = 0.1, sd = 0.02),\n  time = rnorm(50, mean = 60, sd = 10),\n  quality = NULL\n)\n\n# Create realistic relationship where only some variables matter\n# Important: temperature, pressure, catalyst_conc\n# Less important: pH\n# Not important: flow_rate, time\nquality_data[, quality := 50 +\n  0.3 * temperature +\n  2.0 * pressure +\n  100 * catalyst_conc +\n  3.0 * pH +\n  0.1 * flow_rate + # weak effect\n  0.05 * time + # very weak effect\n  rnorm(50, mean = 0, sd = 3)]\n\nprint(\"Quality Prediction Data Summary:\")\npredictors &lt;- c(\"temperature\", \"pressure\", \"flow_rate\", \"pH\", \"catalyst_conc\", \"time\")\nsummary_stats &lt;- quality_data[, lapply(.SD, function(x) list(mean = mean(x), sd = sd(x))),\n  .SDcols = c(predictors, \"quality\")\n]\n\nprint(\"Variable Summary:\")\nfor (var in names(summary_stats)) {\n  cat(sprintf(\n    \"%s: Mean = %.3f, SD = %.3f\\n\",\n    var, summary_stats[[var]]$mean, summary_stats[[var]]$sd\n  ))\n}\n\n# Correlation matrix\ncor_matrix_quality &lt;- cor(quality_data[, .SD, .SDcols = c(predictors, \"quality\")])\nprint(\"Correlation Matrix:\")\nprint(round(cor_matrix_quality, 3))\n\n# Write data to CSV\nfwrite(quality_data, \"data/quality_predictors.csv\")\n\n\n\n\n\n\n\nAvailable Predictors:\n\nTemperature, Pressure, Flow_Rate, pH, Catalyst_Conc, Time\n\nSelection Process:\n\nAll Possible Models: Evaluate all 2^k possible models\nStepwise Selection: Use automated procedures\nCriterion-Based: Compare AIC, BIC, Cp values\n\n\nR OutputR Code\n\n\n\n\n[1] \"Model Selection Summary:\"\n\n\nError in Math.data.frame(structure(list(Method = c(\"Full Model\", \"Stepwise AIC\", : non-numeric-alike variable(s) in data frame: Method\n\n\n[1] \"Final Selected Model Summary:\"\n\n\n\nCall:\nlm(formula = quality ~ temperature + pressure + pH + catalyst_conc, \n    data = quality_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5183 -2.0431 -0.2991  2.2480  6.2915 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    64.4306     7.9554   8.099 2.47e-10 ***\ntemperature     0.2677     0.0208  12.872  &lt; 2e-16 ***\npressure        1.3271     0.2602   5.100 6.58e-06 ***\npH              3.5314     1.0675   3.308  0.00185 ** \ncatalyst_conc  78.7116    26.5539   2.964  0.00484 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.074 on 45 degrees of freedom\nMultiple R-squared:  0.838, Adjusted R-squared:  0.8236 \nF-statistic: 58.19 on 4 and 45 DF,  p-value: &lt; 2.2e-16\n\n\n[1] \"Selected Variables:\"\n\n\n[1] \"temperature\"   \"pressure\"      \"pH\"            \"catalyst_conc\"\n\n\n\n\n\n# Variable selection procedures\n\n# Full model with all predictors\nlm_full &lt;- lm(quality ~ temperature + pressure + flow_rate + pH + catalyst_conc + time,\n  data = quality_data\n)\n\n# Stepwise selection using AIC\nstep_aic &lt;- step(lm_full, direction = \"both\", trace = FALSE)\n\n# Forward selection starting from intercept only\nlm_intercept &lt;- lm(quality ~ 1, data = quality_data)\nstep_forward &lt;- step(lm_intercept,\n  scope = list(lower = lm_intercept, upper = lm_full),\n  direction = \"forward\", trace = FALSE\n)\n\n# Backward elimination starting from full model\nstep_backward &lt;- step(lm_full, direction = \"backward\", trace = FALSE)\n\n# Compare selected models\nmodel_selection_summary &lt;- data.table(\n  Method = c(\"Full Model\", \"Stepwise AIC\", \"Forward AIC\", \"Backward AIC\"),\n  R_squared = c(\n    summary(lm_full)$r.squared,\n    summary(step_aic)$r.squared,\n    summary(step_forward)$r.squared,\n    summary(step_backward)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_full)$adj.r.squared,\n    summary(step_aic)$adj.r.squared,\n    summary(step_forward)$adj.r.squared,\n    summary(step_backward)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_full), AIC(step_aic), AIC(step_forward), AIC(step_backward)),\n  BIC = c(BIC(lm_full), BIC(step_aic), BIC(step_forward), BIC(step_backward)),\n  n_parameters = c(\n    length(coef(lm_full)),\n    length(coef(step_aic)),\n    length(coef(step_forward)),\n    length(coef(step_backward))\n  )\n)\n\nprint(\"Model Selection Summary:\")\nprint(round(model_selection_summary[, .(Method, R_squared, Adj_R_squared, AIC, BIC, n_parameters)], 4))\n\n# Final selected model (based on AIC stepwise)\nprint(\"Final Selected Model Summary:\")\nprint(summary(step_aic))\n\n# Extract selected variables\nselected_vars &lt;- names(coef(step_aic))[-1] # Remove intercept\nprint(\"Selected Variables:\")\nprint(selected_vars)\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 8: Variable selection results\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Variable selection results\n\n\n\n\n\n\n\n\n# Variable selection visualization\n\n# Model comparison plot\nmodel_comp_data &lt;- model_selection_summary[, .(Method, AIC, BIC, Adj_R_squared)]\nmodel_comp_long &lt;- melt(model_comp_data,\n  id.vars = \"Method\",\n  measure.vars = c(\"AIC\", \"BIC\"),\n  variable.name = \"Criterion\", value.name = \"Value\"\n)\n\np1_var &lt;- ggplot(data = model_comp_long, aes(x = Method, y = Value, fill = Criterion)) +\n  geom_col(position = \"dodge\", alpha = 0.7) +\n  labs(\n    x = \"Model Selection Method\", y = \"Criterion Value\",\n    title = \"Model Selection Criteria\", fill = \"Criterion\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Adjusted R² comparison\np2_var &lt;- ggplot(data = model_selection_summary, aes(x = reorder(Method, -Adj_R_squared), y = Adj_R_squared)) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_text(aes(label = round(Adj_R_squared, 3)), vjust = -0.5) +\n  labs(\n    x = \"Model Selection Method\", y = \"Adjusted R²\",\n    title = \"Model Performance Comparison\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    axis.text.x = element_text(angle = 45, hjust = 1)\n  )\n\n# Final model coefficients\nfinal_coefs &lt;- coef(step_aic)[-1] # Remove intercept\ncoef_data &lt;- data.table(\n  variable = names(final_coefs),\n  coefficient = as.numeric(final_coefs)\n)\n\np3_var &lt;- ggplot(data = coef_data, aes(x = reorder(variable, abs(coefficient)), y = coefficient)) +\n  geom_col(fill = \"orange\", alpha = 0.7) +\n  coord_flip() +\n  labs(\n    x = \"Variable\", y = \"Coefficient\",\n    title = \"Final Model Coefficients\"\n  ) +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Correlation plot of selected variables\nselected_cor &lt;- cor_matrix_quality[c(selected_vars, \"quality\"), c(selected_vars, \"quality\")]\n\np4_var &lt;- corrplot(selected_cor,\n  method = \"color\", type = \"upper\",\n  order = \"hclust\", tl.cex = 0.8, tl.col = \"black\"\n)\n\ngrid.arrange(p1_var, p2_var, p3_var, nrow = 2, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#comprehensive-example-complete-regression-analysis",
    "href": "book/Ch06.html#comprehensive-example-complete-regression-analysis",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteComprehensive Example\n\n\n\nA manufacturing engineer conducts a complete regression analysis to optimize a production process:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Production Optimization Data Summary:\"\n\n\n       n temp_mean  temp_sd pressure_mean pressure_sd time_mean  time_sd\n   &lt;int&gt;     &lt;num&gt;    &lt;num&gt;         &lt;num&gt;       &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:    40  182.9235 13.28839      7.768272    1.368849  44.61594 6.506526\n   yield_mean yield_sd\n        &lt;num&gt;    &lt;num&gt;\n1:   153.0184 7.917538\n\n\n[1] \"Yield by Operator:\"\n\n\n   operator     n mean_yield sd_yield\n     &lt;char&gt; &lt;int&gt;      &lt;num&gt;    &lt;num&gt;\n1:        A    12   153.8781 6.764170\n2:        B    13   153.7998 8.222506\n3:        C    15   151.6535 8.805235\n\n\n\n\n\n# Comprehensive regression analysis example\nset.seed(161718)\nproduction_data &lt;- data.table(\n  batch = 1:40,\n  temp = rnorm(40, mean = 180, sd = 15),\n  pressure = rnorm(40, mean = 8, sd = 1.5),\n  time = rnorm(40, mean = 45, sd = 8),\n  operator = sample(c(\"A\", \"B\", \"C\"), 40, replace = TRUE),\n  yield = NULL\n)\n\n# Complex relationship with interactions and quadratic terms\nproduction_data[, yield := 60 +\n  0.5 * temp +\n  3.0 * pressure +\n  0.3 * time +\n  0.01 * temp * pressure + # interaction\n  -0.0015 * temp^2 + # quadratic term\n  ifelse(operator == \"B\", 5, 0) + # operator effect\n  ifelse(operator == \"C\", -2, 0) +\n  rnorm(40, mean = 0, sd = 4)]\n\nprint(\"Production Optimization Data Summary:\")\nprod_summary &lt;- production_data %&gt;%\n  fsummarise(\n    n = fnobs(batch),\n    temp_mean = fmean(temp),\n    temp_sd = fsd(temp),\n    pressure_mean = fmean(pressure),\n    pressure_sd = fsd(pressure),\n    time_mean = fmean(time),\n    time_sd = fsd(time),\n    yield_mean = fmean(yield),\n    yield_sd = fsd(yield)\n  )\nprint(prod_summary)\n\n# Operator summary\noperator_summary &lt;- production_data %&gt;%\n  fgroup_by(operator) %&gt;%\n  fsummarise(\n    n = fnobs(yield),\n    mean_yield = fmean(yield),\n    sd_yield = fsd(yield)\n  )\nprint(\"Yield by Operator:\")\nprint(operator_summary)\n\n# Write data to CSV\nfwrite(production_data, \"data/production_optimization.csv\")\n\n\n\n\n\n\n\nComplete Analysis Steps:\n\nExploratory Data Analysis\nSimple Linear Regression\nMultiple Regression\nPolynomial Terms\nModel Diagnostics\nVariable Selection\nFinal Model Validation\n\n\nR OutputR Code\n\n\n\n\n[1] \"=== COMPREHENSIVE REGRESSION ANALYSIS ===\"\n\n\n[1] \"\\n--- STEP 1: EXPLORATORY DATA ANALYSIS ---\"\n\n\n[1] \"Correlation Matrix (numeric variables):\"\n\n\n          temp pressure  time yield\ntemp     1.000    0.202 0.326 0.256\npressure 0.202    1.000 0.081 0.865\ntime     0.326    0.081 1.000 0.193\nyield    0.256    0.865 0.193 1.000\n\n\n[1] \"\\n--- STEP 2: SIMPLE LINEAR REGRESSIONS ---\"\n\n\n[1] \"Simple Linear Regression Results:\"\n\n\nError in Math.data.frame(structure(list(Predictor = c(\"Temperature\", \"Pressure\", : non-numeric-alike variable(s) in data frame: Predictor\n\n\n[1] \"\\n--- STEP 3: MULTIPLE REGRESSION - MAIN EFFECTS ---\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator, data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.9510 -1.9880 -0.6332  1.6444  7.3494 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 100.92275    8.32329  12.125 6.75e-14 ***\ntemp          0.02179    0.04351   0.501   0.6198    \npressure      5.16863    0.40476  12.770 1.58e-14 ***\ntime          0.18107    0.09638   1.879   0.0689 .  \noperatorB     2.70940    1.50947   1.795   0.0816 .  \noperatorC    -2.66870    1.39023  -1.920   0.0633 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.339 on 34 degrees of freedom\nMultiple R-squared:  0.845, Adjusted R-squared:  0.8222 \nF-statistic: 37.06 on 5 and 34 DF,  p-value: 7.99e-13\n\n\n[1] \"\\n--- STEP 4: POLYNOMIAL TERMS ---\"\n\n\n[1] \"Model with quadratic terms:\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator + I(temp^2) + \n    I(pressure^2), data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.8800 -2.3948 -0.3778  1.8195  6.9391 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    1.016e+02  9.148e+01   1.111   0.2750  \ntemp           1.551e-01  9.902e-01   0.157   0.8765  \npressure       1.953e+00  4.266e+00   0.458   0.6501  \ntime           1.820e-01  9.871e-02   1.844   0.0744 .\noperatorB      2.769e+00  1.608e+00   1.722   0.0947 .\noperatorC     -2.831e+00  1.454e+00  -1.948   0.0603 .\nI(temp^2)     -3.837e-04  2.745e-03  -0.140   0.8897  \nI(pressure^2)  2.063e-01  2.720e-01   0.758   0.4539  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.41 on 32 degrees of freedom\nMultiple R-squared:  0.8478,    Adjusted R-squared:  0.8145 \nF-statistic: 25.46 on 7 and 32 DF,  p-value: 2.249e-11\n\n\n[1] \"F-test for polynomial terms:\"\n\n\nAnalysis of Variance Table\n\nModel 1: yield ~ temp + pressure + time + operator\nModel 2: yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2)\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     34 379.04                           \n2     32 372.12  2    6.9196 0.2975 0.7447\n\n\n[1] \"\\n--- STEP 5: INTERACTION TERMS ---\"\n\n\n[1] \"Model with interaction:\"\n\n\n\nCall:\nlm(formula = yield ~ temp + pressure + time + operator + I(temp^2) + \n    temp:pressure, data = production_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0145 -1.9143 -0.6736  1.6542  7.3233 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)   88.1805758 90.6278053   0.973   0.3379  \ntemp           0.1572700  1.0375790   0.152   0.8805  \npressure       5.3005491  7.3022345   0.726   0.4732  \ntime           0.1820613  0.0996318   1.827   0.0770 .\noperatorB      2.7710622  1.6318944   1.698   0.0992 .\noperatorC     -2.6475067  1.4768318  -1.793   0.0825 .\nI(temp^2)     -0.0003614  0.0032065  -0.113   0.9110  \ntemp:pressure -0.0006904  0.0393592  -0.018   0.9861  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.441 on 32 degrees of freedom\nMultiple R-squared:  0.8451,    Adjusted R-squared:  0.8112 \nF-statistic: 24.93 on 7 and 32 DF,  p-value: 2.967e-11\n\n\n[1] \"F-test for interaction term:\"\n\n\nAnalysis of Variance Table\n\nModel 1: yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2)\nModel 2: yield ~ temp + pressure + time + operator + I(temp^2) + temp:pressure\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1     32 372.12                      \n2     32 378.80  0   -6.6817         \n\n\n[1] \"\\n--- STEP 6: MODEL SELECTION ---\"\n\n\n[1] \"Model Comparison:\"\n\n\nError in Math.data.frame(structure(list(Model = c(\"Main Effects\", \"Polynomial\", : non-numeric-alike variable(s) in data frame: Model\n\n\n[1] \"Best model by AIC: Main Effects\"\n\n\n[1] \"\\n--- STEP 7: FINAL MODEL DIAGNOSTICS ---\"\n\n\n[1] \"Final Model Diagnostic Summary:\"\n\n\n[1] \"Residual standard error:\"\n\n\nError in print.default(\"Multiple R-squared:\", round(summary(final_model)$r.squared, : invalid printing digits 0\n\n\nError in print.default(\"Adjusted R-squared:\", round(summary(final_model)$adj.r.squared, : invalid printing digits 0\n\n\n\n\n\n# Comprehensive regression analysis steps\n\nprint(\"=== COMPREHENSIVE REGRESSION ANALYSIS ===\")\n\n# Step 1: Exploratory Data Analysis\nprint(\"\\n--- STEP 1: EXPLORATORY DATA ANALYSIS ---\")\ncor_matrix_prod &lt;- cor(production_data[, .(temp, pressure, time, yield)])\nprint(\"Correlation Matrix (numeric variables):\")\nprint(round(cor_matrix_prod, 3))\n\n# Step 2: Simple linear regressions\nprint(\"\\n--- STEP 2: SIMPLE LINEAR REGRESSIONS ---\")\nlm_temp &lt;- lm(yield ~ temp, data = production_data)\nlm_pressure &lt;- lm(yield ~ pressure, data = production_data)\nlm_time &lt;- lm(yield ~ time, data = production_data)\n\nsimple_models &lt;- data.table(\n  Predictor = c(\"Temperature\", \"Pressure\", \"Time\"),\n  R_squared = c(\n    summary(lm_temp)$r.squared,\n    summary(lm_pressure)$r.squared,\n    summary(lm_time)$r.squared\n  ),\n  F_statistic = c(\n    summary(lm_temp)$fstatistic[1],\n    summary(lm_pressure)$fstatistic[1],\n    summary(lm_time)$fstatistic[1]\n  ),\n  p_value = c(\n    pf(summary(lm_temp)$fstatistic[1], 1, 38, lower.tail = FALSE),\n    pf(summary(lm_pressure)$fstatistic[1], 1, 38, lower.tail = FALSE),\n    pf(summary(lm_time)$fstatistic[1], 1, 38, lower.tail = FALSE)\n  )\n)\nprint(\"Simple Linear Regression Results:\")\nprint(round(simple_models, 4))\n\n# Step 3: Multiple regression with main effects\nprint(\"\\n--- STEP 3: MULTIPLE REGRESSION - MAIN EFFECTS ---\")\nproduction_data[, operator := factor(operator)]\nlm_main &lt;- lm(yield ~ temp + pressure + time + operator, data = production_data)\nprint(summary(lm_main))\n\n# Step 4: Add polynomial terms\nprint(\"\\n--- STEP 4: POLYNOMIAL TERMS ---\")\nlm_poly &lt;- lm(yield ~ temp + pressure + time + operator + I(temp^2) + I(pressure^2),\n  data = production_data\n)\nprint(\"Model with quadratic terms:\")\nprint(summary(lm_poly))\n\n# Test if polynomial terms are needed\nanova_poly &lt;- anova(lm_main, lm_poly)\nprint(\"F-test for polynomial terms:\")\nprint(anova_poly)\n\n# Step 5: Add interaction terms\nprint(\"\\n--- STEP 5: INTERACTION TERMS ---\")\nlm_interact &lt;- lm(yield ~ temp + pressure + time + operator + I(temp^2) + temp:pressure,\n  data = production_data\n)\nprint(\"Model with interaction:\")\nprint(summary(lm_interact))\n\n# Test if interaction is needed\nanova_interact &lt;- anova(lm_poly, lm_interact)\nprint(\"F-test for interaction term:\")\nprint(anova_interact)\n\n# Step 6: Model selection\nprint(\"\\n--- STEP 6: MODEL SELECTION ---\")\ncomprehensive_models &lt;- data.table(\n  Model = c(\"Main Effects\", \"Polynomial\", \"With Interaction\"),\n  R_squared = c(\n    summary(lm_main)$r.squared,\n    summary(lm_poly)$r.squared,\n    summary(lm_interact)$r.squared\n  ),\n  Adj_R_squared = c(\n    summary(lm_main)$adj.r.squared,\n    summary(lm_poly)$adj.r.squared,\n    summary(lm_interact)$adj.r.squared\n  ),\n  AIC = c(AIC(lm_main), AIC(lm_poly), AIC(lm_interact)),\n  BIC = c(BIC(lm_main), BIC(lm_poly), BIC(lm_interact)),\n  RMSE = c(\n    sqrt(mean(residuals(lm_main)^2)),\n    sqrt(mean(residuals(lm_poly)^2)),\n    sqrt(mean(residuals(lm_interact)^2))\n  )\n)\n\nprint(\"Model Comparison:\")\nprint(round(comprehensive_models, 4))\n\n# Select best model (lowest AIC)\nbest_model_comp &lt;- which.min(comprehensive_models$AIC)\nprint(paste(\"Best model by AIC:\", comprehensive_models$Model[best_model_comp]))\n\n# Step 7: Final model diagnostics\nprint(\"\\n--- STEP 7: FINAL MODEL DIAGNOSTICS ---\")\nfinal_model &lt;- lm_interact # Assuming this is the best\n\n# Basic diagnostics\nresiduals_final &lt;- residuals(final_model)\nfitted_final &lt;- fitted(final_model)\nn_final &lt;- nrow(production_data)\nk_final &lt;- length(coef(final_model)) - 1\n\ndiagnostic_final &lt;- data.table(\n  observation = 1:n_final,\n  residuals = residuals_final,\n  fitted = fitted_final,\n  standardized_resid = residuals_final / sd(residuals_final)\n)\n\nprint(\"Final Model Diagnostic Summary:\")\nprint(\"Residual standard error:\", round(summary(final_model)$sigma, 3))\nprint(\"Multiple R-squared:\", round(summary(final_model)$r.squared, 3))\nprint(\"Adjusted R-squared:\", round(summary(final_model)$adj.r.squared, 3))\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 10: Comprehensive regression analysis dashboard\n\n\n\n\n\n\n\n\n# Comprehensive regression analysis dashboard\n\n# 1. Pairwise scatter plots\np1_comp &lt;- ggplot(data = production_data, aes(x = temp, y = yield, color = operator)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Temperature\", y = \"Yield\", title = \"Yield vs Temperature\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\np2_comp &lt;- ggplot(data = production_data, aes(x = pressure, y = yield, color = operator)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Pressure\", y = \"Yield\", title = \"Yield vs Pressure\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 2. Model comparison plot\nmodel_comp_plot &lt;- ggplot(\n  data = comprehensive_models,\n  aes(x = reorder(Model, -Adj_R_squared), y = Adj_R_squared)\n) +\n  geom_col(fill = \"steelblue\", alpha = 0.7) +\n  geom_text(aes(label = round(Adj_R_squared, 3)), vjust = -0.5) +\n  labs(x = \"Model\", y = \"Adjusted R²\", title = \"Model Comparison\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 3. Final model diagnostics\np3_comp &lt;- ggplot(data = diagnostic_final, aes(x = fitted, y = residuals)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  geom_smooth(se = FALSE, color = \"blue\") +\n  labs(x = \"Fitted Values\", y = \"Residuals\", title = \"Final Model Diagnostics\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# 4. Actual vs Predicted\np4_comp &lt;- ggplot(\n  data = data.table(actual = production_data$yield, predicted = fitted_final),\n  aes(x = predicted, y = actual)\n) +\n  geom_point(size = 3, alpha = 0.7, color = \"blue\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted Yield\", y = \"Actual Yield\", title = \"Actual vs Predicted\") +\n  theme_classic() +\n  theme(plot.title = element_text(hjust = 0.5))\n\ngrid.arrange(p1_comp, p2_comp, model_comp_plot, p3_comp, nrow = 2, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#key-formulas-and-decision-framework",
    "href": "book/Ch06.html#key-formulas-and-decision-framework",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "NoteEssential Formulas Summary\n\n\n\nSimple Linear Regression:\n\\hat{\\beta}_1 = \\frac{S_{xy}}{S_{xx}}, \\quad \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\nMultiple Regression:\n\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\nCoefficient of Determination:\nR^2 = \\frac{SS_R}{SS_T}, \\quad R_{adj}^2 = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\nPrediction Interval:\n\\hat{y}_0 \\pm t_{\\alpha/2, n-k-1} \\sqrt{MS_E\\left(1 + \\mathbf{x}_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{x}_0\\right)}\nF-Test for Overall Significance:\nF_0 = \\frac{MS_R}{MS_E} = \\frac{SS_R/k}{SS_E/(n-k-1)}\nt-Test for Individual Parameters:\nt_0 = \\frac{\\hat{\\beta}_j}{SE(\\hat{\\beta}_j)}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Key Formulas and Functions Summary:\"\n\n\n                Purpose                Key_Formula    R_Function\n                 &lt;char&gt;                     &lt;char&gt;        &lt;char&gt;\n1: Parameter Estimation             β̂ = (X'X)⁻¹X'y          lm()\n2:   Hypothesis Testing              t = β̂ⱼ/SE(β̂ⱼ)     summary()\n3:      Model Selection AIC = n ln(SSE/n) + 2(k+1) step(), AIC()\n4:           Prediction     ŷ ± t_{α/2} × SE(pred)     predict()\n5:          Diagnostics            VIF = 1/(1-R²ⱼ) vif(), plot()\n              Interpretation\n                      &lt;char&gt;\n1:   Least squares estimates\n2:   Test significance of βⱼ\n3: Balance fit vs complexity\n4:     Individual prediction\n5:   Check multicollinearity\n\n\n\n\nTable 1: Model selection criteria comparison\n\n\n\n\nRegression Model Types and Applications\n\n\nModel_Type\nExample\nKey_Feature\nR_squared_Range\nMain_Diagnostic\nWhen_to_Use\n\n\n\n\nSimple Linear\nSteel Strength\nSingle predictor\n0.60-0.80\nLinearity\nExploring relationships\n\n\nMultiple Linear\nChemical Yield\nMultiple predictors\n0.75-0.90\nMulticollinearity\nMultiple continuous predictors\n\n\nPolynomial\nReaction Rate\nNonlinear relationship\n0.85-0.95\nCurvature\nCurved relationships\n\n\nCategorical\nStrength by Supplier\nFactor variables\n0.70-0.85\nGroup effects\nComparing groups/categories\n\n\nVariable Selection\nQuality Prediction\nAutomated selection\n0.80-0.92\nOverfitting\nMany potential predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Model selection criteria comparison table\nall_models_summary &lt;- data.table(\n  Model_Type = c(\"Simple Linear\", \"Multiple Linear\", \"Polynomial\", \"Categorical\", \"Variable Selection\"),\n  Example = c(\"Steel Strength\", \"Chemical Yield\", \"Reaction Rate\", \"Strength by Supplier\", \"Quality Prediction\"),\n  Key_Feature = c(\"Single predictor\", \"Multiple predictors\", \"Nonlinear relationship\", \"Factor variables\", \"Automated selection\"),\n  R_squared_Range = c(\"0.60-0.80\", \"0.75-0.90\", \"0.85-0.95\", \"0.70-0.85\", \"0.80-0.92\"),\n  Main_Diagnostic = c(\"Linearity\", \"Multicollinearity\", \"Curvature\", \"Group effects\", \"Overfitting\"),\n  When_to_Use = c(\n    \"Exploring relationships\",\n    \"Multiple continuous predictors\",\n    \"Curved relationships\",\n    \"Comparing groups/categories\",\n    \"Many potential predictors\"\n  )\n)\n\nkbl(all_models_summary,\n  caption = \"Regression Model Types and Applications\"\n) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(1, width = \"2.5cm\") %&gt;%\n  column_spec(2, width = \"2.5cm\") %&gt;%\n  column_spec(3, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"2cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\") %&gt;%\n  column_spec(6, width = \"3.5cm\")\n\n# Summary of all key formulas and criteria\nformula_summary &lt;- data.table(\n  Purpose = c(\"Parameter Estimation\", \"Hypothesis Testing\", \"Model Selection\", \"Prediction\", \"Diagnostics\"),\n  Key_Formula = c(\n    \"β̂ = (X'X)⁻¹X'y\",\n    \"t = β̂ⱼ/SE(β̂ⱼ)\",\n    \"AIC = n ln(SSE/n) + 2(k+1)\",\n    \"ŷ ± t_{α/2} × SE(pred)\",\n    \"VIF = 1/(1-R²ⱼ)\"\n  ),\n  R_Function = c(\"lm()\", \"summary()\", \"step(), AIC()\", \"predict()\", \"vif(), plot()\"),\n  Interpretation = c(\n    \"Least squares estimates\",\n    \"Test significance of βⱼ\",\n    \"Balance fit vs complexity\",\n    \"Individual prediction\",\n    \"Check multicollinearity\"\n  )\n)\n\nprint(\"Key Formulas and Functions Summary:\")\nprint(formula_summary)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch06.html#chapter-summary",
    "href": "book/Ch06.html#chapter-summary",
    "title": "1 Building Empirical Models",
    "section": "",
    "text": "This chapter covered the fundamental concepts of building empirical models:\n\nSimple Linear Regression - Modeling relationships between two variables\nParameter Estimation - Using least squares to fit models\nHypothesis Testing - Testing significance of model parameters\nModel Validation - Checking assumptions through residual analysis\nMultiple Regression - Extending to multiple predictors\nAdvanced Topics - Polynomial models, categorical variables, variable selection\n\nKey Principles:\n\nAlways validate model assumptions\nUse appropriate diagnostic tools\nConsider both statistical and practical significance\nApply subject matter knowledge in model building\nValidate models on independent data when possible",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Building Empirical Models"
    ]
  },
  {
    "objectID": "book/Ch04.html",
    "href": "book/Ch04.html",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "TipMajor Themes of Chapter 4\n\n\n\n\nStatistical Inference: Making decisions about population parameters based on sample data\nPoint Estimation: Estimating specific values of population parameters\nHypothesis Testing: Testing claims about population parameters\nConfidence Intervals: Estimating ranges of plausible values for parameters\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nUnderstand the concepts of statistical inference and point estimation.\nFormulate and test statistical hypotheses using appropriate procedures.\nCalculate and interpret P-values for hypothesis tests.\nDistinguish between one-sided and two-sided hypothesis tests.\nConduct inference on the mean of a population when variance is known and unknown.\nCalculate Type II error probabilities and determine sample sizes.\nConstruct and interpret confidence intervals for means, variances, and proportions.\nPerform inference on population variance and population proportions.\nConstruct prediction intervals and tolerance intervals.\nTest for goodness of fit using chi-square tests.\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Inference\n\n\n\nStatistical inference is the process of drawing conclusions about a population based on information obtained from a sample. The two main branches are:\n\nParameter Estimation\n\n\nEstimating the value(s) of unknown population parameter(s)\n\n\nHypothesis Testing\n\n\nTesting assumptions (hypotheses) about population parameter(s)\n\nThe foundation of statistical inference relies on the sampling distribution of statistics computed from the sample.\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Quality Control Data Summary:\"\n\n\n       n Mean_Diameter SD_Diameter Mean_Strength SD_Strength Defect_Rate\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;         &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1:    30      10.48587   0.2943092      252.6751    12.52692  0.06666667\n   Min_Diameter Max_Diameter Q1_Diameter Q3_Diameter\n          &lt;num&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1:     9.910015     11.03607    10.29858    10.64659\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\n\n# Quality control example - statistical inference introduction\nset.seed(123)\nquality_data &lt;- data.table(\n  sample_id = 1:30,\n  diameter = rnorm(30, mean = 10.5, sd = 0.3),\n  strength = rnorm(30, mean = 250, sd = 15),\n  defective = sample(c(0, 1), 30, replace = TRUE, prob = c(0.95, 0.05))\n)\n\n# Summary statistics using preferred style\nquality_summary &lt;- quality_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    Mean_Diameter = fmean(diameter),\n    SD_Diameter = fsd(diameter),\n    Mean_Strength = fmean(strength),\n    SD_Strength = fsd(strength),\n    Defect_Rate = fmean(defective),\n    Min_Diameter = fmin(diameter),\n    Max_Diameter = fmax(diameter),\n    Q1_Diameter = fquantile(diameter, 0.25),\n    Q3_Diameter = fquantile(diameter, 0.75)\n  )\n\nprint(\"Quality Control Data Summary:\")\nprint(quality_summary)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePoint Estimation\n\n\n\nA point estimator is a statistic used to estimate a population parameter. The point estimate is the specific numerical value of the estimator for a given sample.\nKey Properties of Good Estimators:\n\nUnbiased: E[θ̂] = θ\nEfficient: Has minimum variance among all unbiased estimators\nConsistent: θ̂ converges to θ as n → ∞\nSufficient: Uses all information in the sample about θ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Point Estimates:\"\n\n\n       n Sample_Mean Sample_Median Sample_Variance Sample_SD Sample_Range\n   &lt;int&gt;       &lt;num&gt;         &lt;num&gt;           &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:    10       12.05         12.05      0.09166667  0.302765          0.9\n   Sample_IQR\n        &lt;num&gt;\n1:       0.45\n\n\n[1] \"Unbiasedness Check (1000 simulations):\"\n\n\n   True_Mu Mean_of_Sample_Means   Bias_Mean True_Sigma_Squared\n     &lt;num&gt;                &lt;num&gt;       &lt;num&gt;              &lt;num&gt;\n1:      12             12.00797 0.007970092               0.25\n   Mean_of_Sample_Vars Bias_Variance\n                 &lt;num&gt;         &lt;num&gt;\n1:           0.2505641  0.0005641324\n\n\n\n\n\n# Point estimation examples\n# Common point estimators and their properties\n\n# Sample data for demonstration\nsample_data &lt;- data.table(\n  values = c(12.1, 11.8, 12.3, 11.9, 12.5, 12.0, 11.7, 12.2, 12.4, 11.6)\n)\n\n# Point estimators\npoint_estimates &lt;- sample_data %&gt;%\n  fsummarise(\n    n = fnobs(values),\n    Sample_Mean = fmean(values), # Estimator for μ\n    Sample_Median = fmedian(values), # Alternative estimator for μ\n    Sample_Variance = fvar(values), # Estimator for σ²\n    Sample_SD = fsd(values), # Estimator for σ\n    Sample_Range = fmax(values) - fmin(values), # Simple variability measure\n    Sample_IQR = fquantile(values, 0.75) - fquantile(values, 0.25)\n  )\n\nprint(\"Point Estimates:\")\nprint(point_estimates)\n\n# Demonstrate unbiasedness with simulation\nset.seed(456)\nn_simulations &lt;- 1000\nsample_size &lt;- 10\ntrue_mu &lt;- 12\ntrue_sigma &lt;- 0.5\n\nsimulation_results &lt;- data.table()\nfor (i in 1:n_simulations) {\n  sample_vals &lt;- rnorm(sample_size, mean = true_mu, sd = true_sigma)\n  sample_mean &lt;- mean(sample_vals)\n  sample_var &lt;- var(sample_vals)\n  simulation_results &lt;- rbind(\n    simulation_results,\n    data.table(sim = i, x_bar = sample_mean, s_squared = sample_var)\n  )\n}\n\n# Check unbiasedness\nunbiasedness_check &lt;- simulation_results %&gt;%\n  fsummarise(\n    True_Mu = true_mu,\n    Mean_of_Sample_Means = fmean(x_bar),\n    Bias_Mean = fmean(x_bar) - true_mu,\n    True_Sigma_Squared = true_sigma^2,\n    Mean_of_Sample_Vars = fmean(s_squared),\n    Bias_Variance = fmean(s_squared) - true_sigma^2\n  )\n\nprint(\"Unbiasedness Check (1000 simulations):\")\nprint(unbiasedness_check)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStatistical Hypotheses\n\n\n\nA statistical hypothesis is a statement about the parameter(s) of a population.\n\nNull Hypothesis (H₀): The hypothesis being tested (assumed true until evidence suggests otherwise)\nAlternative Hypothesis (H₁ or Hₐ): The hypothesis we conclude if there is sufficient evidence against H₀\n\nExample:\n\nH₀: μ = 50 (The population mean equals 50)\nH₁: μ ≠ 50 (The population mean does not equal 50)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Testing Framework\n\n\n\nSteps in Hypothesis Testing:\n\nState the null and alternative hypotheses\nChoose the significance level (α)\nDetermine the test statistic and its distribution\nDefine the rejection region\nCalculate the test statistic from sample data\nMake a decision (reject or fail to reject H₀)\nDraw conclusions in context of the problem\n\nTypes of Errors:\n\nType I Error (α): Rejecting H₀ when it is true\nType II Error (β): Failing to reject H₀ when it is false\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Hypothesis Testing Framework:\"\n\n\n   Null_Hypothesis Alternative_Hypothesis Significance_Level Sample_Size\n            &lt;char&gt;                 &lt;char&gt;              &lt;num&gt;       &lt;num&gt;\n1:     H0: μ = 100            H1: μ ≠ 100               0.05          25\n   Population_SD Critical_Z_Value Critical_Lower_Bound Critical_Upper_Bound\n           &lt;num&gt;            &lt;num&gt;                &lt;num&gt;                &lt;num&gt;\n1:            10             1.96                96.08               103.92\n                         Rejection_Rule\n                                 &lt;char&gt;\n1: Reject H0 if x̄ &lt; 96.08 or x̄ &gt; 103.92\n\n\n\n\n\n# Hypothesis testing framework demonstration\n# Type I and Type II errors illustration\n\nalpha &lt;- 0.05\nmu0 &lt;- 100\nsigma &lt;- 10\nn &lt;- 25\n\n# Critical values for two-sided test\nz_critical &lt;- qnorm(1 - alpha / 2)\ncritical_lower &lt;- mu0 - z_critical * (sigma / sqrt(n))\ncritical_upper &lt;- mu0 + z_critical * (sigma / sqrt(n))\n\nhypothesis_framework &lt;- data.table(\n  Null_Hypothesis = \"H0: μ = 100\",\n  Alternative_Hypothesis = \"H1: μ ≠ 100\",\n  Significance_Level = alpha,\n  Sample_Size = n,\n  Population_SD = sigma,\n  Critical_Z_Value = round(z_critical, 3),\n  Critical_Lower_Bound = round(critical_lower, 2),\n  Critical_Upper_Bound = round(critical_upper, 2),\n  Rejection_Rule = paste(\"Reject H0 if x̄ &lt;\", round(critical_lower, 2), \"or x̄ &gt;\", round(critical_upper, 2))\n)\n\nprint(\"Hypothesis Testing Framework:\")\nprint(hypothesis_framework)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteP-Values\n\n\n\nThe P-value is the probability of observing a test statistic as extreme or more extreme than the observed value, assuming H₀ is true.\nInterpretation:\n\nSmall P-value (≤ α): Strong evidence against H₀\nLarge P-value (&gt; α): Insufficient evidence against H₀\n\nDecision Rule: Reject H₀ if P-value ≤ α\n\n\n\n\n\n\n\n\n\n\n\nNoteTypes of Hypothesis Tests\n\n\n\nTwo-Sided (Two-Tailed) Test:\n\nH₀: θ = θ₀\nH₁: θ ≠ θ₀\nRejection region: |test statistic| &gt; critical value\n\nOne-Sided (One-Tailed) Tests:\nUpper-tailed:\n\nH₀: θ = θ₀ (or θ ≤ θ₀)\nH₁: θ &gt; θ₀\nRejection region: test statistic &gt; critical value\n\nLower-tailed:\n\nH₀: θ = θ₀ (or θ ≥ θ₀)\nH₁: θ &lt; θ₀\nRejection region: test statistic &lt; critical value\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of one-sided and two-sided tests\n\n\n\n\n\n\n\n\n# Visualization of one-sided vs two-sided tests\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x)\n\n# Create data for visualization\ntest_comparison_data &lt;- data.table(\n  x = rep(x, 3),\n  y = rep(y, 3),\n  test_type = rep(c(\"Two-sided (α = 0.05)\", \"One-sided Upper (α = 0.05)\", \"One-sided Lower (α = 0.05)\"), each = length(x))\n)\n\n# Add critical regions\nalpha_vis &lt;- 0.05\nz_alpha2 &lt;- qnorm(1 - alpha_vis / 2)\nz_alpha &lt;- qnorm(1 - alpha_vis)\n\nggplot(data = test_comparison_data, mapping = aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"blue\") +\n  facet_wrap(~test_type, nrow = 3) +\n  geom_area(\n    data = test_comparison_data[test_type == \"Two-sided (α = 0.05)\" & (x &lt;= -z_alpha2 | x &gt;= z_alpha2)],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_area(\n    data = test_comparison_data[test_type == \"One-sided Upper (α = 0.05)\" & x &gt;= z_alpha],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_area(\n    data = test_comparison_data[test_type == \"One-sided Lower (α = 0.05)\" & x &lt;= -z_alpha],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"Two-sided (α = 0.05)\", x = c(-z_alpha2, z_alpha2)),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"One-sided Upper (α = 0.05)\", x = z_alpha),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"One-sided Lower (α = 0.05)\", x = -z_alpha),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  labs(\n    x = \"Z-score\",\n    y = \"Density\",\n    title = \"Comparison of One-sided and Two-sided Tests\",\n    subtitle = \"Red areas represent rejection regions\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    strip.background = element_rect(fill = \"lightgray\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteZ-Test for Population Mean (σ known)\n\n\n\nWhen the population is normal with known variance σ², or when n is large, the test statistic is:\nZ_0 = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\nUnder H₀: μ = μ₀, Z₀ ~ N(0,1)\nRejection Regions:\n\nTwo-sided: |Z₀| &gt; z_{α/2}\nUpper-tailed: Z₀ &gt; z_α\nLower-tailed: Z₀ &lt; -z_α\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA beverage company fills bottles with a target volume of 355 ml. The filling process has a known standard deviation of σ = 1.5 ml. A quality engineer samples 25 bottles and wants to test if the mean fill volume differs from the target.\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Fill Volume Data Summary:\"\n\n\n       n   Min    Q1 Median    Mean    Q3   Max        SD Range\n   &lt;int&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n1:    25 354.3 354.7  354.9 354.908 355.2 355.5 0.3239341   1.2\n\n\n\n\n\n# Fill volume testing example (Z-test, sigma known)\nfill_volume_data &lt;- data.table(\n  bottle_id = 1:25,\n  volume = c(\n    354.8, 355.2, 354.5, 355.1, 354.9, 355.3, 354.7, 355.0, 354.6, 355.4,\n    354.3, 355.5, 354.8, 354.9, 355.2, 354.4, 355.1, 354.7, 355.3, 354.5,\n    355.0, 354.8, 355.2, 354.6, 354.9\n  )\n)\n\n# Summary statistics\nfill_summary &lt;- fill_volume_data %&gt;%\n  fsummarise(\n    n = fnobs(volume),\n    Min = fmin(volume),\n    Q1 = fquantile(volume, 0.25),\n    Median = fmedian(volume),\n    Mean = fmean(volume),\n    Q3 = fquantile(volume, 0.75),\n    Max = fmax(volume),\n    SD = fsd(volume),\n    Range = fmax(volume) - fmin(volume)\n  )\n\nprint(\"Fill Volume Data Summary:\")\nprint(fill_summary)\n\n\n\n\n\n\n\nManual Calculation:\nGiven: n = 25, σ = 1.5, μ₀ = 355, α = 0.05\n\nHypotheses:\n\nH₀: μ = 355\nH₁: μ ≠ 355\n\nTest Statistic:\n\nIf x̄ = 354.2, then Z₀ = (354.2 - 355)/(1.5/√25) = -2.67\n\nCritical Value: z₀.₀₂₅ = 1.96\nDecision: |Z₀| = 2.67 &gt; 1.96, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Z-Test Results for Fill Volume:\"\n\n\n          Test_Type     n Sample_Mean Null_Mean Population_SD Z_Statistic\n             &lt;char&gt; &lt;int&gt;       &lt;num&gt;     &lt;num&gt;         &lt;num&gt;       &lt;num&gt;\n1: Z-test (σ known)    25     354.908       355           1.5     -0.3067\n   P_Value Alpha Z_Critical          Decision                       Conclusion\n     &lt;num&gt; &lt;num&gt;      &lt;num&gt;            &lt;char&gt;                           &lt;char&gt;\n1:  0.7591  0.05       1.96 Fail to reject H0 Insufficient evidence against H0\n\n\n[1] \"95% Confidence Interval: [ 354.32 ,  355.496 ]\"\n\n\n\n\n\n# Z-test for fill volume (manual and R calculations)\nn_fill &lt;- fnobs(fill_volume_data$volume)\nxbar_fill &lt;- fmean(fill_volume_data$volume)\nsigma_fill &lt;- 1.5 # Known population standard deviation\nmu0_fill &lt;- 355\nalpha_fill &lt;- 0.05\n\n# Manual calculation\nz_stat_fill &lt;- (xbar_fill - mu0_fill) / (sigma_fill / sqrt(n_fill))\np_value_fill &lt;- 2 * (1 - pnorm(abs(z_stat_fill))) # Two-sided test\nz_critical_fill &lt;- qnorm(1 - alpha_fill / 2)\n\n# Test results\nz_test_results &lt;- data.table(\n  Test_Type = \"Z-test (σ known)\",\n  n = n_fill,\n  Sample_Mean = round(xbar_fill, 3),\n  Null_Mean = mu0_fill,\n  Population_SD = sigma_fill,\n  Z_Statistic = round(z_stat_fill, 4),\n  P_Value = round(p_value_fill, 4),\n  Alpha = alpha_fill,\n  Z_Critical = round(z_critical_fill, 3),\n  Decision = ifelse(abs(z_stat_fill) &gt; z_critical_fill, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_fill &lt; alpha_fill,\n    \"Significant evidence against H0\",\n    \"Insufficient evidence against H0\"\n  )\n)\n\nprint(\"Z-Test Results for Fill Volume:\")\nprint(z_test_results)\n\n# Confidence interval\nmargin_error_fill &lt;- z_critical_fill * (sigma_fill / sqrt(n_fill))\nci_lower_fill &lt;- xbar_fill - margin_error_fill\nci_upper_fill &lt;- xbar_fill + margin_error_fill\n\nprint(paste(\"95% Confidence Interval: [\", round(ci_lower_fill, 3), \", \", round(ci_upper_fill, 3), \"]\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteType II Error and Power\n\n\n\nType II Error (β): The probability of failing to reject H₀ when it is false.\nPower (1-β): The probability of correctly rejecting a false H₀.\nFor a two-sided test:\n\\beta = P\\left(-z_{\\alpha/2} - \\frac{\\delta}{\\sigma/\\sqrt{n}} \\leq Z \\leq z_{\\alpha/2} - \\frac{\\delta}{\\sigma/\\sqrt{n}}\\right)\nwhere δ = |μ₁ - μ₀| is the difference we want to detect.\nSample Size Formula: n = \\frac{(z_{\\alpha/2} + z_\\beta)^2 \\sigma^2}{\\delta^2}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Power curves for different sample sizes\n\n\n\n\n\n\n\n\n# Power curves for different sample sizes\nmu_values &lt;- seq(353, 357, by = 0.1)\nsample_sizes &lt;- c(10, 25, 50, 100)\nalpha_power &lt;- 0.05\nsigma_power &lt;- 1.5\nmu0_power &lt;- 355\n\n# Function to calculate power for two-sided Z-test\ncalculate_power_z &lt;- function(mu1, n, mu0, sigma, alpha) {\n  delta &lt;- abs(mu1 - mu0)\n  z_alpha2 &lt;- qnorm(1 - alpha / 2)\n  se &lt;- sigma / sqrt(n)\n\n  # Power calculation for two-sided test\n  power_upper &lt;- 1 - pnorm(z_alpha2 - delta / se)\n  power_lower &lt;- pnorm(-z_alpha2 - delta / se)\n  power_total &lt;- power_upper + power_lower\n\n  return(power_total)\n}\n\n# Generate power curve data\npower_curve_data &lt;- data.table()\nfor (n_val in sample_sizes) {\n  for (mu in mu_values) {\n    power_val &lt;- calculate_power_z(mu, n_val, mu0_power, sigma_power, alpha_power)\n    power_curve_data &lt;- rbind(\n      power_curve_data,\n      data.table(mu = mu, n = factor(n_val), power = power_val)\n    )\n  }\n}\n\n# Plot power curves\nggplot(data = power_curve_data, mapping = aes(x = mu, y = power, color = n)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = mu0_power, linetype = \"dashed\", alpha = 0.7) +\n  geom_hline(yintercept = 0.8, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = 356.5, y = 0.82, label = \"Power = 0.8\", hjust = 0) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(breaks = pretty_breaks(n = 6), limits = c(0, 1)) +\n  labs(\n    x = \"True Mean (μ)\",\n    y = \"Power (1 - β)\",\n    color = \"Sample Size\",\n    title = \"Power Curves for Z-Test (Fill Volume)\",\n    subtitle = \"H₀: μ = 355 vs H₁: μ ≠ 355, α = 0.05, σ = 1.5\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLarge-Sample Test\n\n\n\nWhen n is large (typically n ≥ 30), the Central Limit Theorem allows us to use the normal distribution even when: - The population distribution is unknown - The population variance is unknown (use sample variance s²)\nThe test statistic becomes: Z_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\n\n\n\n\n\n\n\n\n\n\n\nNotePractical Considerations\n\n\n\n\nStatistical vs. Practical Significance: A statistically significant result may not be practically important\nEffect Size: Consider the magnitude of the difference, not just its significance\nSample Size: Larger samples can detect smaller differences\nAssumptions: Verify that test assumptions are met\nMultiple Testing: Adjust significance levels when conducting multiple tests\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ (σ known)\n\n\n\nA 100(1-α)% confidence interval for μ when σ is known:\n\\overline{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\nInterpretation: We are 100(1-α)% confident that the true population mean lies within this interval.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Confidence Intervals for Different Confidence Levels:\"\n\n\n   Confidence_Level Alpha Z_Critical Margin_Error Lower_Bound Upper_Bound Width\n             &lt;char&gt; &lt;num&gt;      &lt;num&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:              90%  0.10      1.645       0.4935     354.415     355.401 0.987\n2:              95%  0.05      1.960       0.5880     354.320     355.496 1.176\n3:              99%  0.01      2.576       0.7727     354.135     355.681 1.545\n\n\n\n\n\n# Confidence interval examples for different confidence levels\nconfidence_levels &lt;- c(0.90, 0.95, 0.99)\nci_results &lt;- data.table()\n\nfor (conf_level in confidence_levels) {\n  alpha_ci &lt;- 1 - conf_level\n  z_crit &lt;- qnorm(1 - alpha_ci / 2)\n  margin_error &lt;- z_crit * (sigma_fill / sqrt(n_fill))\n  ci_lower &lt;- xbar_fill - margin_error\n  ci_upper &lt;- xbar_fill + margin_error\n  ci_width &lt;- ci_upper - ci_lower\n\n  ci_results &lt;- rbind(\n    ci_results,\n    data.table(\n      Confidence_Level = paste0(conf_level * 100, \"%\"),\n      Alpha = alpha_ci,\n      Z_Critical = round(z_crit, 3),\n      Margin_Error = round(margin_error, 4),\n      Lower_Bound = round(ci_lower, 3),\n      Upper_Bound = round(ci_upper, 3),\n      Width = round(ci_width, 3)\n    )\n  )\n}\n\nprint(\"Confidence Intervals for Different Confidence Levels:\")\nprint(ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteGeneral CI Method\n\n\n\nSteps to derive a confidence interval:\n\nFind a statistic θ̂ that estimates θ\nDetermine the sampling distribution of θ̂\nFind constants a and b such that P(a ≤ θ̂ ≤ b) = 1-α\nRearrange the inequality to isolate θ\nThe resulting interval is the confidence interval for θ\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotet-Test for Population Mean (σ unknown)\n\n\n\nWhen the population is normal with unknown variance, the test statistic is:\nT_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\nUnder H₀: μ = μ₀, T₀ ~ t_{n-1}\nProperties of t-distribution:\n\nSymmetric about 0\nMore spread than standard normal\nApproaches standard normal as df → ∞\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nAn engineer tests the compressive strength of concrete. A sample of 16 specimens yields a mean strength of 3250 psi with a standard deviation of 180 psi. Test if the mean strength differs from 3200 psi.\nManual Calculation:\nGiven: n = 16, x̄ = 3250, s = 180, μ₀ = 3200, α = 0.05\n\nHypotheses:\n\nH₀: μ = 3200\nH₁: μ ≠ 3200\n\nTest Statistic:\n\nT₀ = (3250 - 3200)/(180/√16) = 50/45 = 1.11\n\nCritical Value: t₀.₀₂₅,₁₅ = 2.131\nDecision: |T₀| = 1.11 &lt; 2.131, so fail to reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Concrete Strength Data Summary:\"\n\n\n       n   Min     Q1 Median    Mean     Q3   Max      SD      Var  SE_Mean\n   &lt;int&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;   &lt;num&gt;  &lt;num&gt; &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:    16  3180 3207.5   3235 3233.75 3262.5  3290 37.3943 1398.333 9.348574\n\n\n\n\n\n# Concrete strength testing example (t-test, sigma unknown)\nconcrete_data &lt;- data.table(\n  specimen_id = 1:16,\n  strength = c(\n    3180, 3240, 3290, 3210, 3260, 3190, 3270, 3220,\n    3250, 3200, 3280, 3230, 3240, 3180, 3290, 3210\n  )\n)\n\n# Summary statistics\nconcrete_summary &lt;- concrete_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength),\n    SE_Mean = fsd(strength) / sqrt(fnobs(strength))\n  )\n\nprint(\"Concrete Strength Data Summary:\")\nprint(concrete_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"t-Test Results for Concrete Strength:\"\n\n\n            Test_Type     n Sample_Mean Sample_SD Null_Mean t_Statistic\n               &lt;char&gt; &lt;int&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt;       &lt;num&gt;\n1: t-test (σ unknown)    16     3233.75     37.39      3200      3.6102\n   Degrees_Freedom P_Value Alpha t_Critical  Decision\n             &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:              15  0.0026  0.05      2.131 Reject H0\n                         Conclusion\n                             &lt;char&gt;\n1: Significant difference from 3200\n\n\n[1] \"R's t.test verification:\"\n\n\n\n    One Sample t-test\n\ndata:  concrete_data$strength\nt = 3.6102, df = 15, p-value = 0.002571\nalternative hypothesis: true mean is not equal to 3200\n95 percent confidence interval:\n 3213.824 3253.676\nsample estimates:\nmean of x \n  3233.75 \n\n\n\n\n\n# t-test for concrete strength\nn_concrete &lt;- fnobs(concrete_data$strength)\nxbar_concrete &lt;- fmean(concrete_data$strength)\ns_concrete &lt;- fsd(concrete_data$strength)\nmu0_concrete &lt;- 3200\nalpha_concrete &lt;- 0.05\n\n# Manual calculation\nt_stat_concrete &lt;- (xbar_concrete - mu0_concrete) / (s_concrete / sqrt(n_concrete))\ndf_concrete &lt;- n_concrete - 1\np_value_concrete &lt;- 2 * (1 - pt(abs(t_stat_concrete), df = df_concrete)) # Two-sided test\nt_critical_concrete &lt;- qt(1 - alpha_concrete / 2, df = df_concrete)\n\n# Test results\nt_test_results &lt;- data.table(\n  Test_Type = \"t-test (σ unknown)\",\n  n = n_concrete,\n  Sample_Mean = round(xbar_concrete, 2),\n  Sample_SD = round(s_concrete, 2),\n  Null_Mean = mu0_concrete,\n  t_Statistic = round(t_stat_concrete, 4),\n  Degrees_Freedom = df_concrete,\n  P_Value = round(p_value_concrete, 4),\n  Alpha = alpha_concrete,\n  t_Critical = round(t_critical_concrete, 3),\n  Decision = ifelse(abs(t_stat_concrete) &gt; t_critical_concrete, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_concrete &lt; alpha_concrete,\n    \"Significant difference from 3200\",\n    \"No significant difference from 3200\"\n  )\n)\n\nprint(\"t-Test Results for Concrete Strength:\")\nprint(t_test_results)\n\n# Using R's built-in t.test function for verification\nt_test_r &lt;- t.test(concrete_data$strength, mu = mu0_concrete, conf.level = 1 - alpha_concrete)\nprint(\"R's t.test verification:\")\nprint(t_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePower for t-Test\n\n\n\nFor the t-test, power calculations are more complex due to the non-central t-distribution. Approximate sample size formula:\nn \\approx \\frac{(t_{\\alpha/2,\\nu} + t_{\\beta,\\nu})^2 s^2}{\\delta^2}\nwhere ν = n-1 degrees of freedom (requires iteration).\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ (σ unknown)\n\n\n\nA 100(1-α)% confidence interval for μ when σ is unknown:\n\\overline{x} \\pm t_{\\alpha/2,n-1} \\frac{s}{\\sqrt{n}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Concrete Strength Mean:\"\n\n\n             Parameter Confidence_Level Sample_Mean Standard_Error t_Critical\n                &lt;char&gt;           &lt;char&gt;       &lt;num&gt;          &lt;num&gt;      &lt;num&gt;\n1: Population Mean (μ)              95%     3233.75          9.349      2.131\n   Margin_Error Lower_Bound Upper_Bound Width\n          &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:       19.926     3213.82     3253.68 39.85\n                                        Interpretation\n                                                &lt;char&gt;\n1: 95% confident that μ is between 3213.82 and 3253.68\n\n\n\n\n\n# Confidence intervals for t-test\nt_alpha2_concrete &lt;- qt(1 - alpha_concrete / 2, df = df_concrete)\nmargin_error_t &lt;- t_alpha2_concrete * (s_concrete / sqrt(n_concrete))\nci_lower_t &lt;- xbar_concrete - margin_error_t\nci_upper_t &lt;- xbar_concrete + margin_error_t\n\nt_ci_results &lt;- data.table(\n  Parameter = \"Population Mean (μ)\",\n  Confidence_Level = \"95%\",\n  Sample_Mean = round(xbar_concrete, 2),\n  Standard_Error = round(s_concrete / sqrt(n_concrete), 3),\n  t_Critical = round(t_alpha2_concrete, 3),\n  Margin_Error = round(margin_error_t, 3),\n  Lower_Bound = round(ci_lower_t, 2),\n  Upper_Bound = round(ci_upper_t, 2),\n  Width = round(ci_upper_t - ci_lower_t, 2),\n  Interpretation = paste(\"95% confident that μ is between\", round(ci_lower_t, 2), \"and\", round(ci_upper_t, 2))\n)\n\nprint(\"95% Confidence Interval for Concrete Strength Mean:\")\nprint(t_ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChi-Square Test for Variance\n\n\n\nTo test hypotheses about σ², we use:\n\\chi_0^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\nUnder H₀: σ² = σ₀², this follows χ²_{n-1}\nProperties of χ² distribution:\n\nAlways positive\nRight-skewed\nShape depends on degrees of freedom\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA manufacturing process should have a variance in part diameter of σ² = 0.01 mm². A sample of 20 parts gives s² = 0.015 mm². Test if the variance exceeds the specification.\nManual Calculation:\nGiven: n = 20, s² = 0.015, σ₀² = 0.01, α = 0.05\n\nHypotheses:\n\nH₀: σ² = 0.01\nH₁: σ² &gt; 0.01\n\nTest Statistic:\n\nχ₀² = (19)(0.015)/0.01 = 28.5\n\nCritical Value: χ²₀.₀₅,₁₉ = 30.14\nDecision: χ₀² = 28.5 &lt; 30.14, so fail to reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Part Diameter Data Summary:\"\n\n\n       n   Min    Q1 Median   Mean    Q3   Max         SD         Var Range\n   &lt;int&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;      &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:    20  9.95  9.98 10.005 10.006 10.03 10.07 0.03283131 0.001077895  0.12\n\n\n\n\n\n# Part diameter variance testing example\npart_diameter_data &lt;- data.table(\n  part_id = 1:20,\n  diameter = c(\n    10.02, 9.98, 10.05, 9.97, 10.03, 10.01, 9.99, 10.04, 10.00, 9.96,\n    10.07, 9.95, 10.02, 10.01, 9.98, 10.03, 9.99, 10.05, 9.97, 10.00\n  )\n)\n\n# Summary statistics\ndiameter_summary &lt;- part_diameter_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    Min = fmin(diameter),\n    Q1 = fquantile(diameter, 0.25),\n    Median = fmedian(diameter),\n    Mean = fmean(diameter),\n    Q3 = fquantile(diameter, 0.75),\n    Max = fmax(diameter),\n    SD = fsd(diameter),\n    Var = fvar(diameter),\n    Range = fmax(diameter) - fmin(diameter)\n  )\n\nprint(\"Part Diameter Data Summary:\")\nprint(diameter_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Chi-square Test Results for Variance:\"\n\n\n                    Test_Type     n Sample_Variance Sample_SD Null_Variance\n                       &lt;char&gt; &lt;int&gt;           &lt;num&gt;     &lt;num&gt;         &lt;num&gt;\n1: Chi-square test (variance)    20        0.001078    0.0328          0.01\n   Chi2_Statistic Degrees_Freedom P_Value Alpha Chi2_Lower Chi2_Upper  Decision\n            &lt;num&gt;           &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:          2.048              19       0  0.05     8.9065    32.8523 Reject H0\n                                   Conclusion\n                                       &lt;char&gt;\n1: Variance significantly different from 0.01\n\n\n\n\n\n# Chi-square test for variance\nn_diameter &lt;- fnobs(part_diameter_data$diameter)\ns2_diameter &lt;- fvar(part_diameter_data$diameter)\nsigma2_0 &lt;- 0.01 # Null hypothesis: σ² = 0.01\nalpha_chi &lt;- 0.05\n\n# Manual calculation\nchi2_stat &lt;- (n_diameter - 1) * s2_diameter / sigma2_0\ndf_chi &lt;- n_diameter - 1\n\n# P-value for two-sided test\np_value_lower_chi &lt;- pchisq(chi2_stat, df = df_chi)\np_value_upper_chi &lt;- 1 - pchisq(chi2_stat, df = df_chi)\np_value_chi2 &lt;- 2 * min(p_value_lower_chi, p_value_upper_chi)\n\n# Critical values\nchi2_lower &lt;- qchisq(alpha_chi / 2, df = df_chi)\nchi2_upper &lt;- qchisq(1 - alpha_chi / 2, df = df_chi)\n\n# Test results\nchi2_test_results &lt;- data.table(\n  Test_Type = \"Chi-square test (variance)\",\n  n = n_diameter,\n  Sample_Variance = round(s2_diameter, 6),\n  Sample_SD = round(sqrt(s2_diameter), 4),\n  Null_Variance = sigma2_0,\n  Chi2_Statistic = round(chi2_stat, 4),\n  Degrees_Freedom = df_chi,\n  P_Value = round(p_value_chi2, 4),\n  Alpha = alpha_chi,\n  Chi2_Lower = round(chi2_lower, 4),\n  Chi2_Upper = round(chi2_upper, 4),\n  Decision = ifelse(chi2_stat &lt; chi2_lower | chi2_stat &gt; chi2_upper,\n    \"Reject H0\", \"Fail to reject H0\"\n  ),\n  Conclusion = ifelse(p_value_chi2 &lt; alpha_chi,\n    \"Variance significantly different from 0.01\",\n    \"Variance not significantly different from 0.01\"\n  )\n)\n\nprint(\"Chi-square Test Results for Variance:\")\nprint(chi2_test_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for σ²\n\n\n\nA 100(1-α)% confidence interval for σ²:\n\\frac{(n-1)s^2}{\\chi_{\\alpha/2,n-1}^2} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2,n-1}^2}\nFor σ, take the square root of the endpoints.\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Variance and Standard Deviation:\"\n\n\n                Parameter Sample_Estimate Confidence_Level Lower_Bound\n                   &lt;char&gt;           &lt;num&gt;           &lt;char&gt;       &lt;num&gt;\n1:          Variance (σ²)        0.001078              95%    0.000623\n2: Standard Deviation (σ)        0.032800              95%    0.025000\n   Upper_Bound    Width\n         &lt;num&gt;    &lt;num&gt;\n1:    0.002299 0.001676\n2:    0.048000 0.023000\n\n\n\n\n\n# Confidence interval for variance and standard deviation\nci_var_lower &lt;- (df_chi * s2_diameter) / chi2_upper\nci_var_upper &lt;- (df_chi * s2_diameter) / chi2_lower\nci_sd_lower &lt;- sqrt(ci_var_lower)\nci_sd_upper &lt;- sqrt(ci_var_upper)\n\nvariance_ci_results &lt;- data.table(\n  Parameter = c(\"Variance (σ²)\", \"Standard Deviation (σ)\"),\n  Sample_Estimate = c(round(s2_diameter, 6), round(sqrt(s2_diameter), 4)),\n  Confidence_Level = c(\"95%\", \"95%\"),\n  Lower_Bound = c(round(ci_var_lower, 6), round(ci_sd_lower, 4)),\n  Upper_Bound = c(round(ci_var_upper, 6), round(ci_sd_upper, 4)),\n  Width = c(round(ci_var_upper - ci_var_lower, 6), round(ci_sd_upper - ci_sd_lower, 4))\n)\n\nprint(\"95% Confidence Intervals for Variance and Standard Deviation:\")\nprint(variance_ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLarge-Sample Test for Proportion\n\n\n\nFor large n, the test statistic for H₀: p = p₀ is:\nZ_0 = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\nwhere \\hat{p} = X/n is the sample proportion.\nRule of thumb: Use when np₀ ≥ 5 and n(1-p₀) ≥ 5\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA supplier claims their defect rate is 3%. An inspector examines 400 items and finds 18 defective. Test if the defect rate exceeds 3%.\nManual Calculation:\nGiven: n = 400, x = 18, p₀ = 0.03, α = 0.05\n\nHypotheses:\n\nH₀: p = 0.03\nH₁: p &gt; 0.03\n\nSample Proportion:\n\np̂ = 18/400 = 0.045\n\nTest Statistic:\n\nZ₀ = (0.045 - 0.03)/√(0.03×0.97/400) = 0.015/0.0085 = 1.76\n\nCritical Value: z₀.₀₅ = 1.645\nDecision: Z₀ = 1.76 &gt; 1.645, so reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Defect Rate Data Summary:\"\n\n\n       n     x Sample_Proportion Sample_Percent\n   &lt;num&gt; &lt;num&gt;             &lt;num&gt;          &lt;num&gt;\n1:   400    18             0.045            4.5\n\n\n\n\n\n# Defect rate testing example\ndefect_data &lt;- data.table(\n  inspector_id = 1,\n  items_examined = 400,\n  defective_found = 18,\n  p_hat = 18 / 400\n)\n\n# Summary\ndefect_summary &lt;- defect_data %&gt;%\n  fsummarise(\n    n = items_examined,\n    x = defective_found,\n    Sample_Proportion = round(fmean(p_hat), 4),\n    Sample_Percent = round(fmean(p_hat) * 100, 2)\n  )\n\nprint(\"Defect Rate Data Summary:\")\nprint(defect_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Proportion Test Results:\"\n\n\n             Test_Type     n     x Sample_Proportion Null_Proportion   np0\n                &lt;char&gt; &lt;num&gt; &lt;num&gt;             &lt;num&gt;           &lt;num&gt; &lt;num&gt;\n1: Z-test (proportion)   400    18             0.045            0.03    12\n   n_1_p0 Conditions_Met Z_Statistic P_Value Alpha Z_Critical  Decision\n    &lt;num&gt;         &lt;lgcl&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:    388           TRUE      1.7586  0.0393  0.05      1.645 Reject H0\n                             Conclusion\n                                 &lt;char&gt;\n1: Defect rate significantly exceeds 3%\n\n\n[1] \"R's prop.test verification:\"\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x_prop out of n_prop, null probability p0_prop\nX-squared = 2.5988, df = 1, p-value = 0.05347\nalternative hypothesis: true p is greater than 0.03\n95 percent confidence interval:\n 0.02977218 1.00000000\nsample estimates:\n    p \n0.045 \n\n\n\n\n\n# Z-test for proportion\nn_prop &lt;- defect_data$items_examined\nx_prop &lt;- defect_data$defective_found\np_hat_prop &lt;- x_prop / n_prop\np0_prop &lt;- 0.03 # Null hypothesis: p = 0.03\nalpha_prop &lt;- 0.05\n\n# Check conditions for large-sample test\nnp0 &lt;- n_prop * p0_prop\nn_1_p0 &lt;- n_prop * (1 - p0_prop)\nconditions_met &lt;- (np0 &gt;= 5) & (n_1_p0 &gt;= 5)\n\n# Manual calculation\nz_stat_prop &lt;- (p_hat_prop - p0_prop) / sqrt(p0_prop * (1 - p0_prop) / n_prop)\np_value_prop &lt;- 1 - pnorm(z_stat_prop) # One-sided upper test\nz_critical_prop &lt;- qnorm(1 - alpha_prop)\n\n# Test results\nprop_test_results &lt;- data.table(\n  Test_Type = \"Z-test (proportion)\",\n  n = n_prop,\n  x = x_prop,\n  Sample_Proportion = round(p_hat_prop, 4),\n  Null_Proportion = p0_prop,\n  np0 = np0,\n  n_1_p0 = n_1_p0,\n  Conditions_Met = conditions_met,\n  Z_Statistic = round(z_stat_prop, 4),\n  P_Value = round(p_value_prop, 4),\n  Alpha = alpha_prop,\n  Z_Critical = round(z_critical_prop, 3),\n  Decision = ifelse(z_stat_prop &gt; z_critical_prop, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_prop &lt; alpha_prop,\n    \"Defect rate significantly exceeds 3%\",\n    \"No significant evidence that defect rate exceeds 3%\"\n  )\n)\n\nprint(\"Proportion Test Results:\")\nprint(prop_test_results)\n\n# Using R's prop.test for verification\nprop_test_r &lt;- prop.test(x = x_prop, n = n_prop, p = p0_prop, alternative = \"greater\")\nprint(\"R's prop.test verification:\")\nprint(prop_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Proportion Test\n\n\n\nFor specified α, β, and difference δ = |p₁ - p₀|:\nn = \\frac{[z_\\alpha\\sqrt{p_0(1-p_0)} + z_\\beta\\sqrt{p_1(1-p_1)}]^2}{\\delta^2}\nFor a two-sided test, replace z_α with z_{α/2}.\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for p\n\n\n\nA large-sample 100(1-α)% confidence interval for p:\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nWilson Score Interval (better for small samples): \\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}}}{1 + \\frac{z_{\\alpha/2}^2}{n}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Proportion:\"\n\n\n            Method Sample_Proportion Confidence_Level Lower_Bound Upper_Bound\n            &lt;char&gt;             &lt;num&gt;           &lt;char&gt;       &lt;num&gt;       &lt;num&gt;\n1:     Standard CI             0.045              95%      0.0247      0.0653\n2: Wilson Score CI             0.045              95%      0.0287      0.0700\n   Lower_Percent Upper_Percent  Width\n           &lt;num&gt;         &lt;num&gt;  &lt;num&gt;\n1:          2.47          6.53 0.0406\n2:          2.87          7.00 0.0414\n\n\n\n\n\n# Confidence intervals for proportion\nz_alpha2_prop &lt;- qnorm(1 - alpha_prop / 2)\n\n# Standard confidence interval\nmargin_error_prop &lt;- z_alpha2_prop * sqrt(p_hat_prop * (1 - p_hat_prop) / n_prop)\nci_lower_prop &lt;- p_hat_prop - margin_error_prop\nci_upper_prop &lt;- p_hat_prop + margin_error_prop\n\n# Wilson score interval (more accurate for small samples)\nwilson_adjustment &lt;- (z_alpha2_prop^2) / (2 * n_prop)\nwilson_denominator &lt;- 1 + (z_alpha2_prop^2) / n_prop\nwilson_center &lt;- (p_hat_prop + wilson_adjustment) / wilson_denominator\nwilson_margin &lt;- z_alpha2_prop * sqrt((p_hat_prop * (1 - p_hat_prop) / n_prop + (z_alpha2_prop^2) / (4 * n_prop^2)) / wilson_denominator^2)\nwilson_lower &lt;- wilson_center - wilson_margin\nwilson_upper &lt;- wilson_center + wilson_margin\n\nprop_ci_results &lt;- data.table(\n  Method = c(\"Standard CI\", \"Wilson Score CI\"),\n  Sample_Proportion = c(round(p_hat_prop, 4), round(p_hat_prop, 4)),\n  Confidence_Level = c(\"95%\", \"95%\"),\n  Lower_Bound = c(round(ci_lower_prop, 4), round(wilson_lower, 4)),\n  Upper_Bound = c(round(ci_upper_prop, 4), round(wilson_upper, 4)),\n  Lower_Percent = c(round(ci_lower_prop * 100, 2), round(wilson_lower * 100, 2)),\n  Upper_Percent = c(round(ci_upper_prop * 100, 2), round(wilson_upper * 100, 2)),\n  Width = c(round(ci_upper_prop - ci_lower_prop, 4), round(wilson_upper - wilson_lower, 4))\n)\n\nprint(\"95% Confidence Intervals for Proportion:\")\nprint(prop_ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePrediction Interval\n\n\n\nA prediction interval provides bounds for a future observation from the same population.\nFor a normal population with unknown σ: \\overline{x} \\pm t_{\\alpha/2,n-1}s\\sqrt{1 + \\frac{1}{n}}\nNote: Prediction intervals are always wider than confidence intervals because they account for both sampling variability and the variability of individual observations.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Comparison of Confidence and Prediction Intervals:\"\n\n\n             Interval_Type              Purpose Sample_Mean Margin_Error\n                    &lt;char&gt;               &lt;char&gt;       &lt;num&gt;        &lt;num&gt;\n1: 95% Confidence Interval  For population mean     3233.75        19.93\n2: 95% Prediction Interval For next observation     3233.75        82.16\n   Lower_Bound Upper_Bound  Width\n         &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     3213.82     3253.68  39.85\n2:     3151.59     3315.91 164.31\n\n\n\n\n\n# Prediction interval example\n# Using concrete strength data\nprediction_data &lt;- concrete_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    Sample_Mean = fmean(strength),\n    Sample_SD = fsd(strength)\n  )\n\n# Calculate prediction interval for next observation\nt_alpha2_pred &lt;- qt(1 - alpha_concrete / 2, df = prediction_data$n - 1)\nprediction_margin &lt;- t_alpha2_pred * prediction_data$Sample_SD * sqrt(1 + 1 / prediction_data$n)\npred_lower &lt;- prediction_data$Sample_Mean - prediction_margin\npred_upper &lt;- prediction_data$Sample_Mean + prediction_margin\n\n# Compare with confidence interval\nconf_margin &lt;- t_alpha2_pred * prediction_data$Sample_SD / sqrt(prediction_data$n)\nconf_lower &lt;- prediction_data$Sample_Mean - conf_margin\nconf_upper &lt;- prediction_data$Sample_Mean + conf_margin\n\ninterval_comparison &lt;- data.table(\n  Interval_Type = c(\"95% Confidence Interval\", \"95% Prediction Interval\"),\n  Purpose = c(\"For population mean\", \"For next observation\"),\n  Sample_Mean = c(round(prediction_data$Sample_Mean, 2), round(prediction_data$Sample_Mean, 2)),\n  Margin_Error = c(round(conf_margin, 2), round(prediction_margin, 2)),\n  Lower_Bound = c(round(conf_lower, 2), round(pred_lower, 2)),\n  Upper_Bound = c(round(conf_upper, 2), round(pred_upper, 2)),\n  Width = c(round(conf_upper - conf_lower, 2), round(pred_upper - pred_lower, 2))\n)\n\nprint(\"Comparison of Confidence and Prediction Intervals:\")\nprint(interval_comparison)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTolerance Intervals\n\n\n\nA tolerance interval is an interval that contains at least a specified proportion P of the population with confidence level 100(1-α)%.\nTwo-sided tolerance interval: \\overline{x} \\pm ks\nwhere k is a tolerance interval factor that depends on n, P, and 1-α.\nCommon applications:\n\nQuality control specifications\nProcess capability studies\nEngineering design limits\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Tolerance Interval Results:\"\n\n\n                Interval_Type                                        Purpose\n                       &lt;char&gt;                                         &lt;char&gt;\n1: 95%/95% Tolerance Interval Contains 95% of population with 95% confidence\n   Sample_Mean Sample_SD k_Factor Lower_Bound Upper_Bound  Width\n         &lt;num&gt;     &lt;num&gt;    &lt;num&gt;       &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     3233.75     37.39    2.903     3125.19     3342.31 217.11\n                                                                 Interpretation\n                                                                         &lt;char&gt;\n1: 95% confident that 95% of concrete strengths are between 3125.19 and 3342.31\n\n\n\n\n\n# Tolerance interval example\n# For 95% of population with 95% confidence\n# Using tolerance interval factors (simplified version)\n\n# Approximate tolerance interval factors for n=16, 95% content, 95% confidence\nk_factor &lt;- 2.903 # This would typically come from statistical tables\n\ntolerance_lower &lt;- prediction_data$Sample_Mean - k_factor * prediction_data$Sample_SD\ntolerance_upper &lt;- prediction_data$Sample_Mean + k_factor * prediction_data$Sample_SD\n\ntolerance_results &lt;- data.table(\n  Interval_Type = \"95%/95% Tolerance Interval\",\n  Purpose = \"Contains 95% of population with 95% confidence\",\n  Sample_Mean = round(prediction_data$Sample_Mean, 2),\n  Sample_SD = round(prediction_data$Sample_SD, 2),\n  k_Factor = k_factor,\n  Lower_Bound = round(tolerance_lower, 2),\n  Upper_Bound = round(tolerance_upper, 2),\n  Width = round(tolerance_upper - tolerance_lower, 2),\n  Interpretation = paste(\n    \"95% confident that 95% of concrete strengths are between\",\n    round(tolerance_lower, 2), \"and\", round(tolerance_upper, 2)\n  )\n)\n\nprint(\"Tolerance Interval Results:\")\nprint(tolerance_results)\n\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 1: Summary of inference procedures for a single sample\n\n\n\n\nSummary of Inference Procedures for a Single Sample\n\n\nParameter\nTest_Statistic\nDistribution\nConfidence_Interval\nAssumptions\n\n\n\n\nMean (σ known)\nZ = (x̄ - μ₀)/(σ/√n)\nN(0,1)\nx̄ ± z_{α/2}σ/√n\nNormal population or large n\n\n\nMean (σ unknown)\nt = (x̄ - μ₀)/(s/√n)\nt_{n-1}\nx̄ ± t_{α/2,n-1}s/√n\nNormal population\n\n\nVariance\nχ² = (n-1)s²/σ₀²\nχ²_{n-1}\n((n-1)s²/χ²_{α/2}, (n-1)s²/χ²_{1-α/2})\nNormal population\n\n\nProportion\nZ = (p̂ - p₀)/√(p₀(1-p₀)/n)\nN(0,1)\np̂ ± z_{α/2}√(p̂(1-p̂)/n)\nLarge n, np≥5, n(1-p)≥5\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Summary table of inference procedures\ninference_summary &lt;- data.table(\n  Parameter = c(\"Mean (σ known)\", \"Mean (σ unknown)\", \"Variance\", \"Proportion\"),\n  Test_Statistic = c(\n    \"Z = (x̄ - μ₀)/(σ/√n)\", \"t = (x̄ - μ₀)/(s/√n)\",\n    \"χ² = (n-1)s²/σ₀²\", \"Z = (p̂ - p₀)/√(p₀(1-p₀)/n)\"\n  ),\n  Distribution = c(\"N(0,1)\", \"t_{n-1}\", \"χ²_{n-1}\", \"N(0,1)\"),\n  Confidence_Interval = c(\n    \"x̄ ± z_{α/2}σ/√n\", \"x̄ ± t_{α/2,n-1}s/√n\",\n    \"((n-1)s²/χ²_{α/2}, (n-1)s²/χ²_{1-α/2})\",\n    \"p̂ ± z_{α/2}√(p̂(1-p̂)/n)\"\n  ),\n  Assumptions = c(\n    \"Normal population or large n\", \"Normal population\",\n    \"Normal population\", \"Large n, np≥5, n(1-p)≥5\"\n  )\n)\n\nkbl(inference_summary,\n  caption = \"Summary of Inference Procedures for a Single Sample\"\n) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(2, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"3cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteChi-Square Goodness-of-Fit Test\n\n\n\nTests whether sample data fits a specified distribution.\nTest Statistic: \\chi_0^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\nwhere:\n\nO_i = observed frequency in cell i\nE_i = expected frequency in cell i\nk = number of cells\n\nUnder H₀, χ₀² ~ χ²_{k-1-p} where p = number of estimated parameters.\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nTest whether the following 50 observations follow a normal distribution:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Data Summary for Normality Test:\"\n\n\n       n      Min       Q1   Median     Mean       Q3      Max       SD\n   &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:    50 77.39232 93.06884 98.11524 100.3605 106.7775 124.0762 10.42499\n    Skewness   Kurtosis\n       &lt;num&gt;      &lt;num&gt;\n1: 0.4022879 -0.4221122\n\n\n\n\n\n# Goodness of fit test example\nset.seed(789)\nnormality_test_data &lt;- data.table(\n  observation_id = 1:50,\n  value = c(rnorm(40, mean = 100, sd = 10), rnorm(10, mean = 110, sd = 8)) # Mixed to test normality\n)\n\n# Summary statistics\nnormality_summary &lt;- normality_test_data %&gt;%\n  fsummarise(\n    n = fnobs(value),\n    Min = fmin(value),\n    Q1 = fquantile(value, 0.25),\n    Median = fmedian(value),\n    Mean = fmean(value),\n    Q3 = fquantile(value, 0.75),\n    Max = fmax(value),\n    SD = fsd(value),\n    Skewness = fsum((value - fmean(value))^3) / (fnobs(value) * fsd(value)^3),\n    Kurtosis = fsum((value - fmean(value))^4) / (fnobs(value) * fsd(value)^4) - 3\n  )\n\nprint(\"Data Summary for Normality Test:\")\nprint(normality_summary)\n\n\n\n\n\n\n\nSteps: 1. Estimate μ and σ from the sample 2. Define class intervals 3. Calculate expected frequencies under normality 4. Compute chi-square statistic 5. Compare to critical value\n\nR OutputR Code\n\n\n\n\n[1] \"Goodness of Fit Test Results:\"\n\n\n                         Test                  Null_Hypothesis     n Classes\n                       &lt;char&gt;                           &lt;char&gt; &lt;int&gt;   &lt;int&gt;\n1: Chi-square goodness of fit Data follows normal distribution    50       7\n   Chi2_Statistic Degrees_Freedom P_Value Alpha                   Decision\n            &lt;num&gt;           &lt;num&gt;   &lt;num&gt; &lt;num&gt;                     &lt;char&gt;\n1:          3.331               4   0.504  0.05 Fail to reject H0 (normal)\n                                 Conclusion\n                                     &lt;char&gt;\n1: Data consistent with normal distribution\n\n\n[1] \"Observed vs Expected Frequencies:\"\n\n\nKey: &lt;class_interval&gt;\n   class_interval     N  expected chi2_component\n           &lt;fctr&gt; &lt;int&gt;     &lt;num&gt;          &lt;num&gt;\n1:    [76.4,83.3]     1  2.029651    0.522346378\n2:    (83.3,90.3]     7  5.798476    0.248972473\n3:    (90.3,97.3]    14 10.782517    0.960091219\n4:     (97.3,104]    10 13.056560    0.715545084\n5:      (104,111]    10 10.296736    0.008551451\n6:      (111,118]     5  5.287598    0.015642709\n7:      (118,125]     3  1.767273    0.859864587\n\n\n[1] \"Shapiro-Wilk Test for comparison:\"\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  normality_test_data$value\nW = 0.971, p-value = 0.2541\n\n\n\n\n\n# Chi-square goodness of fit test for normality\nn_norm &lt;- fnobs(normality_test_data$value)\nsample_mean &lt;- fmean(normality_test_data$value)\nsample_sd &lt;- fsd(normality_test_data$value)\n\n# Create class intervals (using Sturges' rule for number of classes)\nk_classes &lt;- ceiling(1 + log2(n_norm)) # Sturges' rule\nclass_breaks &lt;- seq(\n  from = fmin(normality_test_data$value) - 1,\n  to = fmax(normality_test_data$value) + 1,\n  length.out = k_classes + 1\n)\n\n# Calculate observed frequencies\nnormality_test_data[, class_interval := cut(value, breaks = class_breaks, include.lowest = TRUE)]\nobserved_freq &lt;- normality_test_data[, .N, by = class_interval][order(class_interval)]\n\n# Calculate expected frequencies under normality\nexpected_freq &lt;- data.table()\nfor (i in 1:(length(class_breaks) - 1)) {\n  prob_class &lt;- pnorm(class_breaks[i + 1], mean = sample_mean, sd = sample_sd) -\n    pnorm(class_breaks[i], mean = sample_mean, sd = sample_sd)\n  expected_count &lt;- n_norm * prob_class\n  expected_freq &lt;- rbind(\n    expected_freq,\n    data.table(\n      class_interval = observed_freq$class_interval[i],\n      expected = expected_count\n    )\n  )\n}\n\n# Combine observed and expected\ngoodness_fit_table &lt;- merge(observed_freq, expected_freq, by = \"class_interval\")\ngoodness_fit_table[, chi2_component := (N - expected)^2 / expected]\n\n# Calculate chi-square statistic\nchi2_gof &lt;- sum(goodness_fit_table$chi2_component)\ndf_gof &lt;- nrow(goodness_fit_table) - 1 - 2 # -2 for estimated μ and σ\np_value_gof &lt;- 1 - pchisq(chi2_gof, df = df_gof)\n\ngoodness_fit_results &lt;- data.table(\n  Test = \"Chi-square goodness of fit\",\n  Null_Hypothesis = \"Data follows normal distribution\",\n  n = n_norm,\n  Classes = nrow(goodness_fit_table),\n  Chi2_Statistic = round(chi2_gof, 4),\n  Degrees_Freedom = df_gof,\n  P_Value = round(p_value_gof, 4),\n  Alpha = 0.05,\n  Decision = ifelse(p_value_gof &lt; 0.05, \"Reject H0 (not normal)\", \"Fail to reject H0 (normal)\"),\n  Conclusion = ifelse(p_value_gof &lt; 0.05,\n    \"Data does not follow normal distribution\",\n    \"Data consistent with normal distribution\"\n  )\n)\n\nprint(\"Goodness of Fit Test Results:\")\nprint(goodness_fit_results)\nprint(\"Observed vs Expected Frequencies:\")\nprint(goodness_fit_table)\n\n# Alternative tests for normality\nshapiro_test &lt;- shapiro.test(normality_test_data$value)\nprint(\"Shapiro-Wilk Test for comparison:\")\nprint(shapiro_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteComprehensive Example\n\n\n\nA manufacturing company wants to perform a complete statistical analysis of their production process. They collect data on 100 products measuring diameter, strength, and defect status.\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Comprehensive Quality Control Data Summary:\"\n\n\n       n Mean_Diameter SD_Diameter Min_Diameter Max_Diameter Mean_Strength\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:   100      10.02267   0.1838186     9.477965     10.97406      246.6658\n   SD_Strength Min_Strength Max_Strength Total_Defects Defect_Rate\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;       &lt;num&gt;\n1:    20.84695     191.4787     285.4509             8        0.08\n   Defect_Percent\n            &lt;num&gt;\n1:              8\n\n\nError in eval(e, .data, pe): object 'p_value' not found\n\n\nError in pchisq(chi2_stat, df = df): Non-numeric argument to mathematical function\n\n\nError in eval(e, .data, pe): object 'z_stat' not found\n\n\nError: object 'diameter_test' not found\n\n\n[1] \"Comprehensive Hypothesis Test Results:\"\n\n\nError: object 'comprehensive_test_results' not found\n\n\n\n\n\n# Comprehensive quality control analysis\nset.seed(456)\ncomprehensive_qc_data &lt;- data.table(\n  product_id = 1:100,\n  diameter = rnorm(100, mean = 10.0, sd = 0.15),\n  strength = rnorm(100, mean = 250, sd = 20),\n  defective = sample(c(0, 1), 100, replace = TRUE, prob = c(0.92, 0.08))\n)\n\n# Add some outliers to make it realistic\ncomprehensive_qc_data[sample(.N, 3), diameter := diameter + rnorm(3, 0, 0.5)]\ncomprehensive_qc_data[sample(.N, 2), strength := strength - 50]\n\n# Comprehensive summary\nqc_summary &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    # Diameter statistics\n    Mean_Diameter = fmean(diameter),\n    SD_Diameter = fsd(diameter),\n    Min_Diameter = fmin(diameter),\n    Max_Diameter = fmax(diameter),\n    # Strength statistics\n    Mean_Strength = fmean(strength),\n    SD_Strength = fsd(strength),\n    Min_Strength = fmin(strength),\n    Max_Strength = fmax(strength),\n    # Defect statistics\n    Total_Defects = fsum(defective),\n    Defect_Rate = fmean(defective),\n    Defect_Percent = fmean(defective) * 100\n  )\n\nprint(\"Comprehensive Quality Control Data Summary:\")\nprint(qc_summary)\n\n# Multiple hypothesis tests\nalpha_qc &lt;- 0.05\n\n# 1. Test diameter mean vs specification (10.0)\ndiameter_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    xbar = fmean(diameter),\n    s = fsd(diameter),\n    mu0 = 10.0,\n    t_stat = (fmean(diameter) - 10.0) / (fsd(diameter) / sqrt(fnobs(diameter))),\n    df = fnobs(diameter) - 1,\n    p_value = 2 * (1 - pt(abs((fmean(diameter) - 10.0) / (fsd(diameter) / sqrt(fnobs(diameter)))),\n      df = fnobs(diameter) - 1\n    )),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\n# 2. Test strength variance\nstrength_var_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    s2 = fvar(strength),\n    sigma2_0 = 400, # Target variance\n    chi2_stat = (fnobs(strength) - 1) * fvar(strength) / 400,\n    df = fnobs(strength) - 1,\n    p_value = 2 * min(pchisq(chi2_stat, df = df), 1 - pchisq(chi2_stat, df = df)),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\n# 3. Test defect proportion\ndefect_prop_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(defective),\n    x = fsum(defective),\n    p_hat = fmean(defective),\n    p0 = 0.05, # Target defect rate\n    z_stat = (fmean(defective) - 0.05) / sqrt(0.05 * 0.95 / fnobs(defective)),\n    p_value = 2 * (1 - pnorm(abs(z_stat))),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\ncomprehensive_test_results &lt;- data.table(\n  Test = c(\"Diameter Mean\", \"Strength Variance\", \"Defect Proportion\"),\n  Parameter = c(\"μ = 10.0\", \"σ² = 400\", \"p = 0.05\"),\n  Test_Statistic = c(\n    round(diameter_test$t_stat, 3),\n    round(strength_var_test$chi2_stat, 3),\n    round(defect_prop_test$z_stat, 3)\n  ),\n  P_Value = c(\n    round(diameter_test$p_value, 4),\n    round(strength_var_test$p_value, 4),\n    round(defect_prop_test$p_value, 4)\n  ),\n  Decision = c(diameter_test$decision, strength_var_test$decision, defect_prop_test$decision),\n  Distribution = c(\n    paste0(\"t(\", diameter_test$df, \")\"),\n    paste0(\"χ²(\", strength_var_test$df, \")\"),\n    \"N(0,1)\"\n  )\n)\n\nprint(\"Comprehensive Hypothesis Test Results:\")\nprint(comprehensive_test_results)\n\n\n\n\n\n\n\nComplete Analysis: 1. Test mean diameter vs. specification 2. Test variance in strength 3. Test defect proportion 4. Construct confidence intervals 5. Create prediction intervals 6. Establish tolerance intervals 7. Test for normality\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: Comprehensive quality control analysis dashboard\n\n\n\n\n\n\n\n\n# Comprehensive quality control dashboard\nlibrary(gridExtra)\n\n# Diameter histogram with normal overlay\np1 &lt;- ggplot(data = comprehensive_qc_data, mapping = aes(x = diameter)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), bins = 20,\n    fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(comprehensive_qc_data$diameter),\n      sd = sd(comprehensive_qc_data$diameter)\n    ),\n    color = \"red\", linewidth = 1\n  ) +\n  geom_vline(xintercept = 10.0, linetype = \"dashed\", color = \"green\", linewidth = 1) +\n  labs(x = \"Diameter\", y = \"Density\", title = \"Diameter Distribution\") +\n  theme_classic()\n\n# Strength boxplot\np2 &lt;- ggplot(data = comprehensive_qc_data, mapping = aes(x = \"\", y = strength)) +\n  geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +\n  geom_hline(yintercept = 250, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"\", y = \"Strength\", title = \"Strength Distribution\") +\n  theme_classic() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# Defect rate bar chart\ndefect_summary_plot &lt;- comprehensive_qc_data[, .(Count = .N), by = defective]\ndefect_summary_plot[, Status := ifelse(defective == 0, \"Good\", \"Defective\")]\n\np3 &lt;- ggplot(data = defect_summary_plot, mapping = aes(x = Status, y = Count, fill = Status)) +\n  geom_col(alpha = 0.7) +\n  scale_fill_manual(values = c(\"Good\" = \"green\", \"Defective\" = \"red\")) +\n  labs(x = \"Product Status\", y = \"Count\", title = \"Defect Analysis\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n# Control chart simulation\nset.seed(123)\ncontrol_data &lt;- data.table(\n  sample_num = 1:30,\n  sample_mean = rnorm(30, mean = 10.0, sd = 0.05)\n)\ncontrol_data[c(25, 28), sample_mean := c(10.3, 9.7)] # Add out-of-control points\n\nucl &lt;- 10.0 + 3 * 0.05\nlcl &lt;- 10.0 - 3 * 0.05\n\np4 &lt;- ggplot(data = control_data, mapping = aes(x = sample_num, y = sample_mean)) +\n  geom_line(color = \"blue\", linewidth = 0.8) +\n  geom_point(size = 2, color = \"red\") +\n  geom_hline(yintercept = 10.0, color = \"green\", linewidth = 1) +\n  geom_hline(yintercept = ucl, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = lcl, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Sample Number\", y = \"Sample Mean\", title = \"Control Chart\") +\n  theme_classic()\n\n# Combine plots\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEssential Formulas Summary\n\n\n\nTest Statistics:\n\nZ-test (σ known): Z_0 = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\nt-test (σ unknown): T_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\nChi-square (variance): \\chi_0^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\nZ-test (proportion): Z_0 = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\nConfidence Intervals:\n\nMean (σ known): \\overline{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\nMean (σ unknown): \\overline{x} \\pm t_{\\alpha/2,n-1} \\frac{s}{\\sqrt{n}}\nVariance: \\frac{(n-1)s^2}{\\chi_{\\alpha/2,n-1}^2} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2,n-1}^2}\nProportion: \\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\nOther Intervals:\n\nPrediction: \\overline{x} \\pm t_{\\alpha/2,n-1}s\\sqrt{1 + \\frac{1}{n}}\nTolerance: \\overline{x} \\pm ks (k from tables)\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 3: Critical values for common significance levels\n\n\n\n\nCritical Values for Common Significance Levels\n\n\n\n\n\n\n\n\n\n\n\n\nNormal\n\n\nt-distribution\n\n\nChi-square\n\n\n\nα\ndf\nz_{α/2}\nt_{α/2,df}\nχ²_{α/2,df}\nχ²_{1-α/2,df}\n\n\n\n\nα = 0.05\n\n\n0.05\n5\n1.960\n2.571\n0.831\n12.833\n\n\n0.05\n10\n1.960\n2.228\n3.247\n20.483\n\n\n0.05\n20\n1.960\n2.086\n9.591\n34.170\n\n\n0.05\n30\n1.960\n2.042\n16.791\n46.979\n\n\n0.05\n100\n1.960\n1.984\n74.222\n129.561\n\n\nα = 0.01\n\n\n0.01\n5\n2.576\n4.032\n0.412\n16.750\n\n\n0.01\n10\n2.576\n3.169\n2.156\n25.188\n\n\n0.01\n20\n2.576\n2.845\n7.434\n39.997\n\n\n0.01\n30\n2.576\n2.750\n13.787\n53.672\n\n\n0.01\n100\n2.576\n2.626\n67.328\n140.169\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4\n\n\n# Critical values table\nalpha_levels &lt;- c(0.10, 0.05, 0.01)\ndf_values &lt;- c(1, 2, 5, 10, 15, 20, 30, 50, 100, 1000)\n\ncritical_values_table &lt;- data.table()\nfor (alpha_val in alpha_levels) {\n  for (df_val in df_values) {\n    z_crit &lt;- qnorm(1 - alpha_val / 2)\n    t_crit &lt;- qt(1 - alpha_val / 2, df = df_val)\n    chi2_lower &lt;- qchisq(alpha_val / 2, df = df_val)\n    chi2_upper &lt;- qchisq(1 - alpha_val / 2, df = df_val)\n\n    critical_values_table &lt;- rbind(\n      critical_values_table,\n      data.table(\n        alpha = alpha_val,\n        df = df_val,\n        z_alpha2 = round(z_crit, 3),\n        t_alpha2 = round(t_crit, 3),\n        chi2_alpha2 = round(chi2_lower, 3),\n        chi2_1_alpha2 = round(chi2_upper, 3)\n      )\n    )\n  }\n}\n\n# Create formatted table for display\ncritical_display &lt;- critical_values_table[alpha %in% c(0.05, 0.01) &\n  df %in% c(5, 10, 20, 30, 100)]\n\nkbl(critical_display,\n  col.names = c(\"α\", \"df\", \"z_{α/2}\", \"t_{α/2,df}\", \"χ²_{α/2,df}\", \"χ²_{1-α/2,df}\"),\n  caption = \"Critical Values for Common Significance Levels\"\n) %&gt;%\n  kable_styling() %&gt;%\n  add_header_above(c(\" \" = 2, \"Normal\" = 1, \"t-distribution\" = 1, \"Chi-square\" = 2)) %&gt;%\n  pack_rows(\"α = 0.05\", 1, 5) %&gt;%\n  pack_rows(\"α = 0.01\", 6, 10)\n\n# Sample size determination examples",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#statistical-inference",
    "href": "book/Ch04.html#statistical-inference",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteStatistical Inference\n\n\n\nStatistical inference is the process of drawing conclusions about a population based on information obtained from a sample. The two main branches are:\n\nParameter Estimation\n\n\nEstimating the value(s) of unknown population parameter(s)\n\n\nHypothesis Testing\n\n\nTesting assumptions (hypotheses) about population parameter(s)\n\nThe foundation of statistical inference relies on the sampling distribution of statistics computed from the sample.\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Quality Control Data Summary:\"\n\n\n       n Mean_Diameter SD_Diameter Mean_Strength SD_Strength Defect_Rate\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;         &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1:    30      10.48587   0.2943092      252.6751    12.52692  0.06666667\n   Min_Diameter Max_Diameter Q1_Diameter Q3_Diameter\n          &lt;num&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt;\n1:     9.910015     11.03607    10.29858    10.64659\n\n\n\n\n\nlibrary(collapse)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(scales)\nlibrary(kableExtra)\nlibrary(gridExtra)\n\n# Quality control example - statistical inference introduction\nset.seed(123)\nquality_data &lt;- data.table(\n  sample_id = 1:30,\n  diameter = rnorm(30, mean = 10.5, sd = 0.3),\n  strength = rnorm(30, mean = 250, sd = 15),\n  defective = sample(c(0, 1), 30, replace = TRUE, prob = c(0.95, 0.05))\n)\n\n# Summary statistics using preferred style\nquality_summary &lt;- quality_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    Mean_Diameter = fmean(diameter),\n    SD_Diameter = fsd(diameter),\n    Mean_Strength = fmean(strength),\n    SD_Strength = fsd(strength),\n    Defect_Rate = fmean(defective),\n    Min_Diameter = fmin(diameter),\n    Max_Diameter = fmax(diameter),\n    Q1_Diameter = fquantile(diameter, 0.25),\n    Q3_Diameter = fquantile(diameter, 0.75)\n  )\n\nprint(\"Quality Control Data Summary:\")\nprint(quality_summary)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#point-estimation",
    "href": "book/Ch04.html#point-estimation",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NotePoint Estimation\n\n\n\nA point estimator is a statistic used to estimate a population parameter. The point estimate is the specific numerical value of the estimator for a given sample.\nKey Properties of Good Estimators:\n\nUnbiased: E[θ̂] = θ\nEfficient: Has minimum variance among all unbiased estimators\nConsistent: θ̂ converges to θ as n → ∞\nSufficient: Uses all information in the sample about θ\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Point Estimates:\"\n\n\n       n Sample_Mean Sample_Median Sample_Variance Sample_SD Sample_Range\n   &lt;int&gt;       &lt;num&gt;         &lt;num&gt;           &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:    10       12.05         12.05      0.09166667  0.302765          0.9\n   Sample_IQR\n        &lt;num&gt;\n1:       0.45\n\n\n[1] \"Unbiasedness Check (1000 simulations):\"\n\n\n   True_Mu Mean_of_Sample_Means   Bias_Mean True_Sigma_Squared\n     &lt;num&gt;                &lt;num&gt;       &lt;num&gt;              &lt;num&gt;\n1:      12             12.00797 0.007970092               0.25\n   Mean_of_Sample_Vars Bias_Variance\n                 &lt;num&gt;         &lt;num&gt;\n1:           0.2505641  0.0005641324\n\n\n\n\n\n# Point estimation examples\n# Common point estimators and their properties\n\n# Sample data for demonstration\nsample_data &lt;- data.table(\n  values = c(12.1, 11.8, 12.3, 11.9, 12.5, 12.0, 11.7, 12.2, 12.4, 11.6)\n)\n\n# Point estimators\npoint_estimates &lt;- sample_data %&gt;%\n  fsummarise(\n    n = fnobs(values),\n    Sample_Mean = fmean(values), # Estimator for μ\n    Sample_Median = fmedian(values), # Alternative estimator for μ\n    Sample_Variance = fvar(values), # Estimator for σ²\n    Sample_SD = fsd(values), # Estimator for σ\n    Sample_Range = fmax(values) - fmin(values), # Simple variability measure\n    Sample_IQR = fquantile(values, 0.75) - fquantile(values, 0.25)\n  )\n\nprint(\"Point Estimates:\")\nprint(point_estimates)\n\n# Demonstrate unbiasedness with simulation\nset.seed(456)\nn_simulations &lt;- 1000\nsample_size &lt;- 10\ntrue_mu &lt;- 12\ntrue_sigma &lt;- 0.5\n\nsimulation_results &lt;- data.table()\nfor (i in 1:n_simulations) {\n  sample_vals &lt;- rnorm(sample_size, mean = true_mu, sd = true_sigma)\n  sample_mean &lt;- mean(sample_vals)\n  sample_var &lt;- var(sample_vals)\n  simulation_results &lt;- rbind(\n    simulation_results,\n    data.table(sim = i, x_bar = sample_mean, s_squared = sample_var)\n  )\n}\n\n# Check unbiasedness\nunbiasedness_check &lt;- simulation_results %&gt;%\n  fsummarise(\n    True_Mu = true_mu,\n    Mean_of_Sample_Means = fmean(x_bar),\n    Bias_Mean = fmean(x_bar) - true_mu,\n    True_Sigma_Squared = true_sigma^2,\n    Mean_of_Sample_Vars = fmean(s_squared),\n    Bias_Variance = fmean(s_squared) - true_sigma^2\n  )\n\nprint(\"Unbiasedness Check (1000 simulations):\")\nprint(unbiasedness_check)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#hypothesis-testing",
    "href": "book/Ch04.html#hypothesis-testing",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteStatistical Hypotheses\n\n\n\nA statistical hypothesis is a statement about the parameter(s) of a population.\n\nNull Hypothesis (H₀): The hypothesis being tested (assumed true until evidence suggests otherwise)\nAlternative Hypothesis (H₁ or Hₐ): The hypothesis we conclude if there is sufficient evidence against H₀\n\nExample:\n\nH₀: μ = 50 (The population mean equals 50)\nH₁: μ ≠ 50 (The population mean does not equal 50)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteHypothesis Testing Framework\n\n\n\nSteps in Hypothesis Testing:\n\nState the null and alternative hypotheses\nChoose the significance level (α)\nDetermine the test statistic and its distribution\nDefine the rejection region\nCalculate the test statistic from sample data\nMake a decision (reject or fail to reject H₀)\nDraw conclusions in context of the problem\n\nTypes of Errors:\n\nType I Error (α): Rejecting H₀ when it is true\nType II Error (β): Failing to reject H₀ when it is false\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Hypothesis Testing Framework:\"\n\n\n   Null_Hypothesis Alternative_Hypothesis Significance_Level Sample_Size\n            &lt;char&gt;                 &lt;char&gt;              &lt;num&gt;       &lt;num&gt;\n1:     H0: μ = 100            H1: μ ≠ 100               0.05          25\n   Population_SD Critical_Z_Value Critical_Lower_Bound Critical_Upper_Bound\n           &lt;num&gt;            &lt;num&gt;                &lt;num&gt;                &lt;num&gt;\n1:            10             1.96                96.08               103.92\n                         Rejection_Rule\n                                 &lt;char&gt;\n1: Reject H0 if x̄ &lt; 96.08 or x̄ &gt; 103.92\n\n\n\n\n\n# Hypothesis testing framework demonstration\n# Type I and Type II errors illustration\n\nalpha &lt;- 0.05\nmu0 &lt;- 100\nsigma &lt;- 10\nn &lt;- 25\n\n# Critical values for two-sided test\nz_critical &lt;- qnorm(1 - alpha / 2)\ncritical_lower &lt;- mu0 - z_critical * (sigma / sqrt(n))\ncritical_upper &lt;- mu0 + z_critical * (sigma / sqrt(n))\n\nhypothesis_framework &lt;- data.table(\n  Null_Hypothesis = \"H0: μ = 100\",\n  Alternative_Hypothesis = \"H1: μ ≠ 100\",\n  Significance_Level = alpha,\n  Sample_Size = n,\n  Population_SD = sigma,\n  Critical_Z_Value = round(z_critical, 3),\n  Critical_Lower_Bound = round(critical_lower, 2),\n  Critical_Upper_Bound = round(critical_upper, 2),\n  Rejection_Rule = paste(\"Reject H0 if x̄ &lt;\", round(critical_lower, 2), \"or x̄ &gt;\", round(critical_upper, 2))\n)\n\nprint(\"Hypothesis Testing Framework:\")\nprint(hypothesis_framework)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteP-Values\n\n\n\nThe P-value is the probability of observing a test statistic as extreme or more extreme than the observed value, assuming H₀ is true.\nInterpretation:\n\nSmall P-value (≤ α): Strong evidence against H₀\nLarge P-value (&gt; α): Insufficient evidence against H₀\n\nDecision Rule: Reject H₀ if P-value ≤ α\n\n\n\n\n\n\n\n\n\n\n\nNoteTypes of Hypothesis Tests\n\n\n\nTwo-Sided (Two-Tailed) Test:\n\nH₀: θ = θ₀\nH₁: θ ≠ θ₀\nRejection region: |test statistic| &gt; critical value\n\nOne-Sided (One-Tailed) Tests:\nUpper-tailed:\n\nH₀: θ = θ₀ (or θ ≤ θ₀)\nH₁: θ &gt; θ₀\nRejection region: test statistic &gt; critical value\n\nLower-tailed:\n\nH₀: θ = θ₀ (or θ ≥ θ₀)\nH₁: θ &lt; θ₀\nRejection region: test statistic &lt; critical value\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 1: Comparison of one-sided and two-sided tests\n\n\n\n\n\n\n\n\n# Visualization of one-sided vs two-sided tests\nx &lt;- seq(-4, 4, length.out = 1000)\ny &lt;- dnorm(x)\n\n# Create data for visualization\ntest_comparison_data &lt;- data.table(\n  x = rep(x, 3),\n  y = rep(y, 3),\n  test_type = rep(c(\"Two-sided (α = 0.05)\", \"One-sided Upper (α = 0.05)\", \"One-sided Lower (α = 0.05)\"), each = length(x))\n)\n\n# Add critical regions\nalpha_vis &lt;- 0.05\nz_alpha2 &lt;- qnorm(1 - alpha_vis / 2)\nz_alpha &lt;- qnorm(1 - alpha_vis)\n\nggplot(data = test_comparison_data, mapping = aes(x = x, y = y)) +\n  geom_line(linewidth = 1, color = \"blue\") +\n  facet_wrap(~test_type, nrow = 3) +\n  geom_area(\n    data = test_comparison_data[test_type == \"Two-sided (α = 0.05)\" & (x &lt;= -z_alpha2 | x &gt;= z_alpha2)],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_area(\n    data = test_comparison_data[test_type == \"One-sided Upper (α = 0.05)\" & x &gt;= z_alpha],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_area(\n    data = test_comparison_data[test_type == \"One-sided Lower (α = 0.05)\" & x &lt;= -z_alpha],\n    mapping = aes(x = x, y = y), fill = \"red\", alpha = 0.3\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"Two-sided (α = 0.05)\", x = c(-z_alpha2, z_alpha2)),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"One-sided Upper (α = 0.05)\", x = z_alpha),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  geom_vline(\n    data = data.table(test_type = \"One-sided Lower (α = 0.05)\", x = -z_alpha),\n    mapping = aes(xintercept = x), linetype = \"dashed\", color = \"red\"\n  ) +\n  labs(\n    x = \"Z-score\",\n    y = \"Density\",\n    title = \"Comparison of One-sided and Two-sided Tests\",\n    subtitle = \"Red areas represent rejection regions\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    strip.background = element_rect(fill = \"lightgray\")\n  )",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#inference-on-the-mean-of-a-population-variance-known",
    "href": "book/Ch04.html#inference-on-the-mean-of-a-population-variance-known",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteZ-Test for Population Mean (σ known)\n\n\n\nWhen the population is normal with known variance σ², or when n is large, the test statistic is:\nZ_0 = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\nUnder H₀: μ = μ₀, Z₀ ~ N(0,1)\nRejection Regions:\n\nTwo-sided: |Z₀| &gt; z_{α/2}\nUpper-tailed: Z₀ &gt; z_α\nLower-tailed: Z₀ &lt; -z_α\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA beverage company fills bottles with a target volume of 355 ml. The filling process has a known standard deviation of σ = 1.5 ml. A quality engineer samples 25 bottles and wants to test if the mean fill volume differs from the target.\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Fill Volume Data Summary:\"\n\n\n       n   Min    Q1 Median    Mean    Q3   Max        SD Range\n   &lt;int&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;   &lt;num&gt; &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;num&gt;\n1:    25 354.3 354.7  354.9 354.908 355.2 355.5 0.3239341   1.2\n\n\n\n\n\n# Fill volume testing example (Z-test, sigma known)\nfill_volume_data &lt;- data.table(\n  bottle_id = 1:25,\n  volume = c(\n    354.8, 355.2, 354.5, 355.1, 354.9, 355.3, 354.7, 355.0, 354.6, 355.4,\n    354.3, 355.5, 354.8, 354.9, 355.2, 354.4, 355.1, 354.7, 355.3, 354.5,\n    355.0, 354.8, 355.2, 354.6, 354.9\n  )\n)\n\n# Summary statistics\nfill_summary &lt;- fill_volume_data %&gt;%\n  fsummarise(\n    n = fnobs(volume),\n    Min = fmin(volume),\n    Q1 = fquantile(volume, 0.25),\n    Median = fmedian(volume),\n    Mean = fmean(volume),\n    Q3 = fquantile(volume, 0.75),\n    Max = fmax(volume),\n    SD = fsd(volume),\n    Range = fmax(volume) - fmin(volume)\n  )\n\nprint(\"Fill Volume Data Summary:\")\nprint(fill_summary)\n\n\n\n\n\n\n\nManual Calculation:\nGiven: n = 25, σ = 1.5, μ₀ = 355, α = 0.05\n\nHypotheses:\n\nH₀: μ = 355\nH₁: μ ≠ 355\n\nTest Statistic:\n\nIf x̄ = 354.2, then Z₀ = (354.2 - 355)/(1.5/√25) = -2.67\n\nCritical Value: z₀.₀₂₅ = 1.96\nDecision: |Z₀| = 2.67 &gt; 1.96, so reject H₀\n\n\nR OutputR Code\n\n\n\n\n[1] \"Z-Test Results for Fill Volume:\"\n\n\n          Test_Type     n Sample_Mean Null_Mean Population_SD Z_Statistic\n             &lt;char&gt; &lt;int&gt;       &lt;num&gt;     &lt;num&gt;         &lt;num&gt;       &lt;num&gt;\n1: Z-test (σ known)    25     354.908       355           1.5     -0.3067\n   P_Value Alpha Z_Critical          Decision                       Conclusion\n     &lt;num&gt; &lt;num&gt;      &lt;num&gt;            &lt;char&gt;                           &lt;char&gt;\n1:  0.7591  0.05       1.96 Fail to reject H0 Insufficient evidence against H0\n\n\n[1] \"95% Confidence Interval: [ 354.32 ,  355.496 ]\"\n\n\n\n\n\n# Z-test for fill volume (manual and R calculations)\nn_fill &lt;- fnobs(fill_volume_data$volume)\nxbar_fill &lt;- fmean(fill_volume_data$volume)\nsigma_fill &lt;- 1.5 # Known population standard deviation\nmu0_fill &lt;- 355\nalpha_fill &lt;- 0.05\n\n# Manual calculation\nz_stat_fill &lt;- (xbar_fill - mu0_fill) / (sigma_fill / sqrt(n_fill))\np_value_fill &lt;- 2 * (1 - pnorm(abs(z_stat_fill))) # Two-sided test\nz_critical_fill &lt;- qnorm(1 - alpha_fill / 2)\n\n# Test results\nz_test_results &lt;- data.table(\n  Test_Type = \"Z-test (σ known)\",\n  n = n_fill,\n  Sample_Mean = round(xbar_fill, 3),\n  Null_Mean = mu0_fill,\n  Population_SD = sigma_fill,\n  Z_Statistic = round(z_stat_fill, 4),\n  P_Value = round(p_value_fill, 4),\n  Alpha = alpha_fill,\n  Z_Critical = round(z_critical_fill, 3),\n  Decision = ifelse(abs(z_stat_fill) &gt; z_critical_fill, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_fill &lt; alpha_fill,\n    \"Significant evidence against H0\",\n    \"Insufficient evidence against H0\"\n  )\n)\n\nprint(\"Z-Test Results for Fill Volume:\")\nprint(z_test_results)\n\n# Confidence interval\nmargin_error_fill &lt;- z_critical_fill * (sigma_fill / sqrt(n_fill))\nci_lower_fill &lt;- xbar_fill - margin_error_fill\nci_upper_fill &lt;- xbar_fill + margin_error_fill\n\nprint(paste(\"95% Confidence Interval: [\", round(ci_lower_fill, 3), \", \", round(ci_upper_fill, 3), \"]\"))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteType II Error and Power\n\n\n\nType II Error (β): The probability of failing to reject H₀ when it is false.\nPower (1-β): The probability of correctly rejecting a false H₀.\nFor a two-sided test:\n\\beta = P\\left(-z_{\\alpha/2} - \\frac{\\delta}{\\sigma/\\sqrt{n}} \\leq Z \\leq z_{\\alpha/2} - \\frac{\\delta}{\\sigma/\\sqrt{n}}\\right)\nwhere δ = |μ₁ - μ₀| is the difference we want to detect.\nSample Size Formula: n = \\frac{(z_{\\alpha/2} + z_\\beta)^2 \\sigma^2}{\\delta^2}\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 2: Power curves for different sample sizes\n\n\n\n\n\n\n\n\n# Power curves for different sample sizes\nmu_values &lt;- seq(353, 357, by = 0.1)\nsample_sizes &lt;- c(10, 25, 50, 100)\nalpha_power &lt;- 0.05\nsigma_power &lt;- 1.5\nmu0_power &lt;- 355\n\n# Function to calculate power for two-sided Z-test\ncalculate_power_z &lt;- function(mu1, n, mu0, sigma, alpha) {\n  delta &lt;- abs(mu1 - mu0)\n  z_alpha2 &lt;- qnorm(1 - alpha / 2)\n  se &lt;- sigma / sqrt(n)\n\n  # Power calculation for two-sided test\n  power_upper &lt;- 1 - pnorm(z_alpha2 - delta / se)\n  power_lower &lt;- pnorm(-z_alpha2 - delta / se)\n  power_total &lt;- power_upper + power_lower\n\n  return(power_total)\n}\n\n# Generate power curve data\npower_curve_data &lt;- data.table()\nfor (n_val in sample_sizes) {\n  for (mu in mu_values) {\n    power_val &lt;- calculate_power_z(mu, n_val, mu0_power, sigma_power, alpha_power)\n    power_curve_data &lt;- rbind(\n      power_curve_data,\n      data.table(mu = mu, n = factor(n_val), power = power_val)\n    )\n  }\n}\n\n# Plot power curves\nggplot(data = power_curve_data, mapping = aes(x = mu, y = power, color = n)) +\n  geom_line(linewidth = 1.2) +\n  geom_vline(xintercept = mu0_power, linetype = \"dashed\", alpha = 0.7) +\n  geom_hline(yintercept = 0.8, linetype = \"dotted\", alpha = 0.7) +\n  annotate(\"text\", x = 356.5, y = 0.82, label = \"Power = 0.8\", hjust = 0) +\n  scale_x_continuous(breaks = pretty_breaks(n = 8)) +\n  scale_y_continuous(breaks = pretty_breaks(n = 6), limits = c(0, 1)) +\n  labs(\n    x = \"True Mean (μ)\",\n    y = \"Power (1 - β)\",\n    color = \"Sample Size\",\n    title = \"Power Curves for Z-Test (Fill Volume)\",\n    subtitle = \"H₀: μ = 355 vs H₁: μ ≠ 355, α = 0.05, σ = 1.5\"\n  ) +\n  theme_classic() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLarge-Sample Test\n\n\n\nWhen n is large (typically n ≥ 30), the Central Limit Theorem allows us to use the normal distribution even when: - The population distribution is unknown - The population variance is unknown (use sample variance s²)\nThe test statistic becomes: Z_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\n\n\n\n\n\n\n\n\n\n\n\nNotePractical Considerations\n\n\n\n\nStatistical vs. Practical Significance: A statistically significant result may not be practically important\nEffect Size: Consider the magnitude of the difference, not just its significance\nSample Size: Larger samples can detect smaller differences\nAssumptions: Verify that test assumptions are met\nMultiple Testing: Adjust significance levels when conducting multiple tests\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ (σ known)\n\n\n\nA 100(1-α)% confidence interval for μ when σ is known:\n\\overline{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\nInterpretation: We are 100(1-α)% confident that the true population mean lies within this interval.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Confidence Intervals for Different Confidence Levels:\"\n\n\n   Confidence_Level Alpha Z_Critical Margin_Error Lower_Bound Upper_Bound Width\n             &lt;char&gt; &lt;num&gt;      &lt;num&gt;        &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:              90%  0.10      1.645       0.4935     354.415     355.401 0.987\n2:              95%  0.05      1.960       0.5880     354.320     355.496 1.176\n3:              99%  0.01      2.576       0.7727     354.135     355.681 1.545\n\n\n\n\n\n# Confidence interval examples for different confidence levels\nconfidence_levels &lt;- c(0.90, 0.95, 0.99)\nci_results &lt;- data.table()\n\nfor (conf_level in confidence_levels) {\n  alpha_ci &lt;- 1 - conf_level\n  z_crit &lt;- qnorm(1 - alpha_ci / 2)\n  margin_error &lt;- z_crit * (sigma_fill / sqrt(n_fill))\n  ci_lower &lt;- xbar_fill - margin_error\n  ci_upper &lt;- xbar_fill + margin_error\n  ci_width &lt;- ci_upper - ci_lower\n\n  ci_results &lt;- rbind(\n    ci_results,\n    data.table(\n      Confidence_Level = paste0(conf_level * 100, \"%\"),\n      Alpha = alpha_ci,\n      Z_Critical = round(z_crit, 3),\n      Margin_Error = round(margin_error, 4),\n      Lower_Bound = round(ci_lower, 3),\n      Upper_Bound = round(ci_upper, 3),\n      Width = round(ci_width, 3)\n    )\n  )\n}\n\nprint(\"Confidence Intervals for Different Confidence Levels:\")\nprint(ci_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteGeneral CI Method\n\n\n\nSteps to derive a confidence interval:\n\nFind a statistic θ̂ that estimates θ\nDetermine the sampling distribution of θ̂\nFind constants a and b such that P(a ≤ θ̂ ≤ b) = 1-α\nRearrange the inequality to isolate θ\nThe resulting interval is the confidence interval for θ",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#inference-on-the-mean-of-a-population-variance-unknown",
    "href": "book/Ch04.html#inference-on-the-mean-of-a-population-variance-unknown",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "Notet-Test for Population Mean (σ unknown)\n\n\n\nWhen the population is normal with unknown variance, the test statistic is:\nT_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\nUnder H₀: μ = μ₀, T₀ ~ t_{n-1}\nProperties of t-distribution:\n\nSymmetric about 0\nMore spread than standard normal\nApproaches standard normal as df → ∞\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nAn engineer tests the compressive strength of concrete. A sample of 16 specimens yields a mean strength of 3250 psi with a standard deviation of 180 psi. Test if the mean strength differs from 3200 psi.\nManual Calculation:\nGiven: n = 16, x̄ = 3250, s = 180, μ₀ = 3200, α = 0.05\n\nHypotheses:\n\nH₀: μ = 3200\nH₁: μ ≠ 3200\n\nTest Statistic:\n\nT₀ = (3250 - 3200)/(180/√16) = 50/45 = 1.11\n\nCritical Value: t₀.₀₂₅,₁₅ = 2.131\nDecision: |T₀| = 1.11 &lt; 2.131, so fail to reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Concrete Strength Data Summary:\"\n\n\n       n   Min     Q1 Median    Mean     Q3   Max      SD      Var  SE_Mean\n   &lt;int&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt;   &lt;num&gt;  &lt;num&gt; &lt;num&gt;   &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:    16  3180 3207.5   3235 3233.75 3262.5  3290 37.3943 1398.333 9.348574\n\n\n\n\n\n# Concrete strength testing example (t-test, sigma unknown)\nconcrete_data &lt;- data.table(\n  specimen_id = 1:16,\n  strength = c(\n    3180, 3240, 3290, 3210, 3260, 3190, 3270, 3220,\n    3250, 3200, 3280, 3230, 3240, 3180, 3290, 3210\n  )\n)\n\n# Summary statistics\nconcrete_summary &lt;- concrete_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    Min = fmin(strength),\n    Q1 = fquantile(strength, 0.25),\n    Median = fmedian(strength),\n    Mean = fmean(strength),\n    Q3 = fquantile(strength, 0.75),\n    Max = fmax(strength),\n    SD = fsd(strength),\n    Var = fvar(strength),\n    SE_Mean = fsd(strength) / sqrt(fnobs(strength))\n  )\n\nprint(\"Concrete Strength Data Summary:\")\nprint(concrete_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"t-Test Results for Concrete Strength:\"\n\n\n            Test_Type     n Sample_Mean Sample_SD Null_Mean t_Statistic\n               &lt;char&gt; &lt;int&gt;       &lt;num&gt;     &lt;num&gt;     &lt;num&gt;       &lt;num&gt;\n1: t-test (σ unknown)    16     3233.75     37.39      3200      3.6102\n   Degrees_Freedom P_Value Alpha t_Critical  Decision\n             &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:              15  0.0026  0.05      2.131 Reject H0\n                         Conclusion\n                             &lt;char&gt;\n1: Significant difference from 3200\n\n\n[1] \"R's t.test verification:\"\n\n\n\n    One Sample t-test\n\ndata:  concrete_data$strength\nt = 3.6102, df = 15, p-value = 0.002571\nalternative hypothesis: true mean is not equal to 3200\n95 percent confidence interval:\n 3213.824 3253.676\nsample estimates:\nmean of x \n  3233.75 \n\n\n\n\n\n# t-test for concrete strength\nn_concrete &lt;- fnobs(concrete_data$strength)\nxbar_concrete &lt;- fmean(concrete_data$strength)\ns_concrete &lt;- fsd(concrete_data$strength)\nmu0_concrete &lt;- 3200\nalpha_concrete &lt;- 0.05\n\n# Manual calculation\nt_stat_concrete &lt;- (xbar_concrete - mu0_concrete) / (s_concrete / sqrt(n_concrete))\ndf_concrete &lt;- n_concrete - 1\np_value_concrete &lt;- 2 * (1 - pt(abs(t_stat_concrete), df = df_concrete)) # Two-sided test\nt_critical_concrete &lt;- qt(1 - alpha_concrete / 2, df = df_concrete)\n\n# Test results\nt_test_results &lt;- data.table(\n  Test_Type = \"t-test (σ unknown)\",\n  n = n_concrete,\n  Sample_Mean = round(xbar_concrete, 2),\n  Sample_SD = round(s_concrete, 2),\n  Null_Mean = mu0_concrete,\n  t_Statistic = round(t_stat_concrete, 4),\n  Degrees_Freedom = df_concrete,\n  P_Value = round(p_value_concrete, 4),\n  Alpha = alpha_concrete,\n  t_Critical = round(t_critical_concrete, 3),\n  Decision = ifelse(abs(t_stat_concrete) &gt; t_critical_concrete, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_concrete &lt; alpha_concrete,\n    \"Significant difference from 3200\",\n    \"No significant difference from 3200\"\n  )\n)\n\nprint(\"t-Test Results for Concrete Strength:\")\nprint(t_test_results)\n\n# Using R's built-in t.test function for verification\nt_test_r &lt;- t.test(concrete_data$strength, mu = mu0_concrete, conf.level = 1 - alpha_concrete)\nprint(\"R's t.test verification:\")\nprint(t_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePower for t-Test\n\n\n\nFor the t-test, power calculations are more complex due to the non-central t-distribution. Approximate sample size formula:\nn \\approx \\frac{(t_{\\alpha/2,\\nu} + t_{\\beta,\\nu})^2 s^2}{\\delta^2}\nwhere ν = n-1 degrees of freedom (requires iteration).\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for μ (σ unknown)\n\n\n\nA 100(1-α)% confidence interval for μ when σ is unknown:\n\\overline{x} \\pm t_{\\alpha/2,n-1} \\frac{s}{\\sqrt{n}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Interval for Concrete Strength Mean:\"\n\n\n             Parameter Confidence_Level Sample_Mean Standard_Error t_Critical\n                &lt;char&gt;           &lt;char&gt;       &lt;num&gt;          &lt;num&gt;      &lt;num&gt;\n1: Population Mean (μ)              95%     3233.75          9.349      2.131\n   Margin_Error Lower_Bound Upper_Bound Width\n          &lt;num&gt;       &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:       19.926     3213.82     3253.68 39.85\n                                        Interpretation\n                                                &lt;char&gt;\n1: 95% confident that μ is between 3213.82 and 3253.68\n\n\n\n\n\n# Confidence intervals for t-test\nt_alpha2_concrete &lt;- qt(1 - alpha_concrete / 2, df = df_concrete)\nmargin_error_t &lt;- t_alpha2_concrete * (s_concrete / sqrt(n_concrete))\nci_lower_t &lt;- xbar_concrete - margin_error_t\nci_upper_t &lt;- xbar_concrete + margin_error_t\n\nt_ci_results &lt;- data.table(\n  Parameter = \"Population Mean (μ)\",\n  Confidence_Level = \"95%\",\n  Sample_Mean = round(xbar_concrete, 2),\n  Standard_Error = round(s_concrete / sqrt(n_concrete), 3),\n  t_Critical = round(t_alpha2_concrete, 3),\n  Margin_Error = round(margin_error_t, 3),\n  Lower_Bound = round(ci_lower_t, 2),\n  Upper_Bound = round(ci_upper_t, 2),\n  Width = round(ci_upper_t - ci_lower_t, 2),\n  Interpretation = paste(\"95% confident that μ is between\", round(ci_lower_t, 2), \"and\", round(ci_upper_t, 2))\n)\n\nprint(\"95% Confidence Interval for Concrete Strength Mean:\")\nprint(t_ci_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#inference-on-the-variance-of-a-normal-population",
    "href": "book/Ch04.html#inference-on-the-variance-of-a-normal-population",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteChi-Square Test for Variance\n\n\n\nTo test hypotheses about σ², we use:\n\\chi_0^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\nUnder H₀: σ² = σ₀², this follows χ²_{n-1}\nProperties of χ² distribution:\n\nAlways positive\nRight-skewed\nShape depends on degrees of freedom\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA manufacturing process should have a variance in part diameter of σ² = 0.01 mm². A sample of 20 parts gives s² = 0.015 mm². Test if the variance exceeds the specification.\nManual Calculation:\nGiven: n = 20, s² = 0.015, σ₀² = 0.01, α = 0.05\n\nHypotheses:\n\nH₀: σ² = 0.01\nH₁: σ² &gt; 0.01\n\nTest Statistic:\n\nχ₀² = (19)(0.015)/0.01 = 28.5\n\nCritical Value: χ²₀.₀₅,₁₉ = 30.14\nDecision: χ₀² = 28.5 &lt; 30.14, so fail to reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Part Diameter Data Summary:\"\n\n\n       n   Min    Q1 Median   Mean    Q3   Max         SD         Var Range\n   &lt;int&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;      &lt;num&gt;       &lt;num&gt; &lt;num&gt;\n1:    20  9.95  9.98 10.005 10.006 10.03 10.07 0.03283131 0.001077895  0.12\n\n\n\n\n\n# Part diameter variance testing example\npart_diameter_data &lt;- data.table(\n  part_id = 1:20,\n  diameter = c(\n    10.02, 9.98, 10.05, 9.97, 10.03, 10.01, 9.99, 10.04, 10.00, 9.96,\n    10.07, 9.95, 10.02, 10.01, 9.98, 10.03, 9.99, 10.05, 9.97, 10.00\n  )\n)\n\n# Summary statistics\ndiameter_summary &lt;- part_diameter_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    Min = fmin(diameter),\n    Q1 = fquantile(diameter, 0.25),\n    Median = fmedian(diameter),\n    Mean = fmean(diameter),\n    Q3 = fquantile(diameter, 0.75),\n    Max = fmax(diameter),\n    SD = fsd(diameter),\n    Var = fvar(diameter),\n    Range = fmax(diameter) - fmin(diameter)\n  )\n\nprint(\"Part Diameter Data Summary:\")\nprint(diameter_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Chi-square Test Results for Variance:\"\n\n\n                    Test_Type     n Sample_Variance Sample_SD Null_Variance\n                       &lt;char&gt; &lt;int&gt;           &lt;num&gt;     &lt;num&gt;         &lt;num&gt;\n1: Chi-square test (variance)    20        0.001078    0.0328          0.01\n   Chi2_Statistic Degrees_Freedom P_Value Alpha Chi2_Lower Chi2_Upper  Decision\n            &lt;num&gt;           &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:          2.048              19       0  0.05     8.9065    32.8523 Reject H0\n                                   Conclusion\n                                       &lt;char&gt;\n1: Variance significantly different from 0.01\n\n\n\n\n\n# Chi-square test for variance\nn_diameter &lt;- fnobs(part_diameter_data$diameter)\ns2_diameter &lt;- fvar(part_diameter_data$diameter)\nsigma2_0 &lt;- 0.01 # Null hypothesis: σ² = 0.01\nalpha_chi &lt;- 0.05\n\n# Manual calculation\nchi2_stat &lt;- (n_diameter - 1) * s2_diameter / sigma2_0\ndf_chi &lt;- n_diameter - 1\n\n# P-value for two-sided test\np_value_lower_chi &lt;- pchisq(chi2_stat, df = df_chi)\np_value_upper_chi &lt;- 1 - pchisq(chi2_stat, df = df_chi)\np_value_chi2 &lt;- 2 * min(p_value_lower_chi, p_value_upper_chi)\n\n# Critical values\nchi2_lower &lt;- qchisq(alpha_chi / 2, df = df_chi)\nchi2_upper &lt;- qchisq(1 - alpha_chi / 2, df = df_chi)\n\n# Test results\nchi2_test_results &lt;- data.table(\n  Test_Type = \"Chi-square test (variance)\",\n  n = n_diameter,\n  Sample_Variance = round(s2_diameter, 6),\n  Sample_SD = round(sqrt(s2_diameter), 4),\n  Null_Variance = sigma2_0,\n  Chi2_Statistic = round(chi2_stat, 4),\n  Degrees_Freedom = df_chi,\n  P_Value = round(p_value_chi2, 4),\n  Alpha = alpha_chi,\n  Chi2_Lower = round(chi2_lower, 4),\n  Chi2_Upper = round(chi2_upper, 4),\n  Decision = ifelse(chi2_stat &lt; chi2_lower | chi2_stat &gt; chi2_upper,\n    \"Reject H0\", \"Fail to reject H0\"\n  ),\n  Conclusion = ifelse(p_value_chi2 &lt; alpha_chi,\n    \"Variance significantly different from 0.01\",\n    \"Variance not significantly different from 0.01\"\n  )\n)\n\nprint(\"Chi-square Test Results for Variance:\")\nprint(chi2_test_results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for σ²\n\n\n\nA 100(1-α)% confidence interval for σ²:\n\\frac{(n-1)s^2}{\\chi_{\\alpha/2,n-1}^2} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2,n-1}^2}\nFor σ, take the square root of the endpoints.\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Variance and Standard Deviation:\"\n\n\n                Parameter Sample_Estimate Confidence_Level Lower_Bound\n                   &lt;char&gt;           &lt;num&gt;           &lt;char&gt;       &lt;num&gt;\n1:          Variance (σ²)        0.001078              95%    0.000623\n2: Standard Deviation (σ)        0.032800              95%    0.025000\n   Upper_Bound    Width\n         &lt;num&gt;    &lt;num&gt;\n1:    0.002299 0.001676\n2:    0.048000 0.023000\n\n\n\n\n\n# Confidence interval for variance and standard deviation\nci_var_lower &lt;- (df_chi * s2_diameter) / chi2_upper\nci_var_upper &lt;- (df_chi * s2_diameter) / chi2_lower\nci_sd_lower &lt;- sqrt(ci_var_lower)\nci_sd_upper &lt;- sqrt(ci_var_upper)\n\nvariance_ci_results &lt;- data.table(\n  Parameter = c(\"Variance (σ²)\", \"Standard Deviation (σ)\"),\n  Sample_Estimate = c(round(s2_diameter, 6), round(sqrt(s2_diameter), 4)),\n  Confidence_Level = c(\"95%\", \"95%\"),\n  Lower_Bound = c(round(ci_var_lower, 6), round(ci_sd_lower, 4)),\n  Upper_Bound = c(round(ci_var_upper, 6), round(ci_sd_upper, 4)),\n  Width = c(round(ci_var_upper - ci_var_lower, 6), round(ci_sd_upper - ci_sd_lower, 4))\n)\n\nprint(\"95% Confidence Intervals for Variance and Standard Deviation:\")\nprint(variance_ci_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#inference-on-a-population-proportion",
    "href": "book/Ch04.html#inference-on-a-population-proportion",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteLarge-Sample Test for Proportion\n\n\n\nFor large n, the test statistic for H₀: p = p₀ is:\nZ_0 = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\nwhere \\hat{p} = X/n is the sample proportion.\nRule of thumb: Use when np₀ ≥ 5 and n(1-p₀) ≥ 5\n\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nA supplier claims their defect rate is 3%. An inspector examines 400 items and finds 18 defective. Test if the defect rate exceeds 3%.\nManual Calculation:\nGiven: n = 400, x = 18, p₀ = 0.03, α = 0.05\n\nHypotheses:\n\nH₀: p = 0.03\nH₁: p &gt; 0.03\n\nSample Proportion:\n\np̂ = 18/400 = 0.045\n\nTest Statistic:\n\nZ₀ = (0.045 - 0.03)/√(0.03×0.97/400) = 0.015/0.0085 = 1.76\n\nCritical Value: z₀.₀₅ = 1.645\nDecision: Z₀ = 1.76 &gt; 1.645, so reject H₀\n\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Defect Rate Data Summary:\"\n\n\n       n     x Sample_Proportion Sample_Percent\n   &lt;num&gt; &lt;num&gt;             &lt;num&gt;          &lt;num&gt;\n1:   400    18             0.045            4.5\n\n\n\n\n\n# Defect rate testing example\ndefect_data &lt;- data.table(\n  inspector_id = 1,\n  items_examined = 400,\n  defective_found = 18,\n  p_hat = 18 / 400\n)\n\n# Summary\ndefect_summary &lt;- defect_data %&gt;%\n  fsummarise(\n    n = items_examined,\n    x = defective_found,\n    Sample_Proportion = round(fmean(p_hat), 4),\n    Sample_Percent = round(fmean(p_hat) * 100, 2)\n  )\n\nprint(\"Defect Rate Data Summary:\")\nprint(defect_summary)\n\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Proportion Test Results:\"\n\n\n             Test_Type     n     x Sample_Proportion Null_Proportion   np0\n                &lt;char&gt; &lt;num&gt; &lt;num&gt;             &lt;num&gt;           &lt;num&gt; &lt;num&gt;\n1: Z-test (proportion)   400    18             0.045            0.03    12\n   n_1_p0 Conditions_Met Z_Statistic P_Value Alpha Z_Critical  Decision\n    &lt;num&gt;         &lt;lgcl&gt;       &lt;num&gt;   &lt;num&gt; &lt;num&gt;      &lt;num&gt;    &lt;char&gt;\n1:    388           TRUE      1.7586  0.0393  0.05      1.645 Reject H0\n                             Conclusion\n                                 &lt;char&gt;\n1: Defect rate significantly exceeds 3%\n\n\n[1] \"R's prop.test verification:\"\n\n\n\n    1-sample proportions test with continuity correction\n\ndata:  x_prop out of n_prop, null probability p0_prop\nX-squared = 2.5988, df = 1, p-value = 0.05347\nalternative hypothesis: true p is greater than 0.03\n95 percent confidence interval:\n 0.02977218 1.00000000\nsample estimates:\n    p \n0.045 \n\n\n\n\n\n# Z-test for proportion\nn_prop &lt;- defect_data$items_examined\nx_prop &lt;- defect_data$defective_found\np_hat_prop &lt;- x_prop / n_prop\np0_prop &lt;- 0.03 # Null hypothesis: p = 0.03\nalpha_prop &lt;- 0.05\n\n# Check conditions for large-sample test\nnp0 &lt;- n_prop * p0_prop\nn_1_p0 &lt;- n_prop * (1 - p0_prop)\nconditions_met &lt;- (np0 &gt;= 5) & (n_1_p0 &gt;= 5)\n\n# Manual calculation\nz_stat_prop &lt;- (p_hat_prop - p0_prop) / sqrt(p0_prop * (1 - p0_prop) / n_prop)\np_value_prop &lt;- 1 - pnorm(z_stat_prop) # One-sided upper test\nz_critical_prop &lt;- qnorm(1 - alpha_prop)\n\n# Test results\nprop_test_results &lt;- data.table(\n  Test_Type = \"Z-test (proportion)\",\n  n = n_prop,\n  x = x_prop,\n  Sample_Proportion = round(p_hat_prop, 4),\n  Null_Proportion = p0_prop,\n  np0 = np0,\n  n_1_p0 = n_1_p0,\n  Conditions_Met = conditions_met,\n  Z_Statistic = round(z_stat_prop, 4),\n  P_Value = round(p_value_prop, 4),\n  Alpha = alpha_prop,\n  Z_Critical = round(z_critical_prop, 3),\n  Decision = ifelse(z_stat_prop &gt; z_critical_prop, \"Reject H0\", \"Fail to reject H0\"),\n  Conclusion = ifelse(p_value_prop &lt; alpha_prop,\n    \"Defect rate significantly exceeds 3%\",\n    \"No significant evidence that defect rate exceeds 3%\"\n  )\n)\n\nprint(\"Proportion Test Results:\")\nprint(prop_test_results)\n\n# Using R's prop.test for verification\nprop_test_r &lt;- prop.test(x = x_prop, n = n_prop, p = p0_prop, alternative = \"greater\")\nprint(\"R's prop.test verification:\")\nprint(prop_test_r)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSample Size for Proportion Test\n\n\n\nFor specified α, β, and difference δ = |p₁ - p₀|:\nn = \\frac{[z_\\alpha\\sqrt{p_0(1-p_0)} + z_\\beta\\sqrt{p_1(1-p_1)}]^2}{\\delta^2}\nFor a two-sided test, replace z_α with z_{α/2}.\n\n\n\n\n\n\n\n\n\n\n\nNoteConfidence Interval for p\n\n\n\nA large-sample 100(1-α)% confidence interval for p:\n\\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\nWilson Score Interval (better for small samples): \\frac{\\hat{p} + \\frac{z_{\\alpha/2}^2}{2n} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n} + \\frac{z_{\\alpha/2}^2}{4n^2}}}{1 + \\frac{z_{\\alpha/2}^2}{n}}\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"95% Confidence Intervals for Proportion:\"\n\n\n            Method Sample_Proportion Confidence_Level Lower_Bound Upper_Bound\n            &lt;char&gt;             &lt;num&gt;           &lt;char&gt;       &lt;num&gt;       &lt;num&gt;\n1:     Standard CI             0.045              95%      0.0247      0.0653\n2: Wilson Score CI             0.045              95%      0.0287      0.0700\n   Lower_Percent Upper_Percent  Width\n           &lt;num&gt;         &lt;num&gt;  &lt;num&gt;\n1:          2.47          6.53 0.0406\n2:          2.87          7.00 0.0414\n\n\n\n\n\n# Confidence intervals for proportion\nz_alpha2_prop &lt;- qnorm(1 - alpha_prop / 2)\n\n# Standard confidence interval\nmargin_error_prop &lt;- z_alpha2_prop * sqrt(p_hat_prop * (1 - p_hat_prop) / n_prop)\nci_lower_prop &lt;- p_hat_prop - margin_error_prop\nci_upper_prop &lt;- p_hat_prop + margin_error_prop\n\n# Wilson score interval (more accurate for small samples)\nwilson_adjustment &lt;- (z_alpha2_prop^2) / (2 * n_prop)\nwilson_denominator &lt;- 1 + (z_alpha2_prop^2) / n_prop\nwilson_center &lt;- (p_hat_prop + wilson_adjustment) / wilson_denominator\nwilson_margin &lt;- z_alpha2_prop * sqrt((p_hat_prop * (1 - p_hat_prop) / n_prop + (z_alpha2_prop^2) / (4 * n_prop^2)) / wilson_denominator^2)\nwilson_lower &lt;- wilson_center - wilson_margin\nwilson_upper &lt;- wilson_center + wilson_margin\n\nprop_ci_results &lt;- data.table(\n  Method = c(\"Standard CI\", \"Wilson Score CI\"),\n  Sample_Proportion = c(round(p_hat_prop, 4), round(p_hat_prop, 4)),\n  Confidence_Level = c(\"95%\", \"95%\"),\n  Lower_Bound = c(round(ci_lower_prop, 4), round(wilson_lower, 4)),\n  Upper_Bound = c(round(ci_upper_prop, 4), round(wilson_upper, 4)),\n  Lower_Percent = c(round(ci_lower_prop * 100, 2), round(wilson_lower * 100, 2)),\n  Upper_Percent = c(round(ci_upper_prop * 100, 2), round(wilson_upper * 100, 2)),\n  Width = c(round(ci_upper_prop - ci_lower_prop, 4), round(wilson_upper - wilson_lower, 4))\n)\n\nprint(\"95% Confidence Intervals for Proportion:\")\nprint(prop_ci_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#other-interval-estimates-for-a-single-sample",
    "href": "book/Ch04.html#other-interval-estimates-for-a-single-sample",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NotePrediction Interval\n\n\n\nA prediction interval provides bounds for a future observation from the same population.\nFor a normal population with unknown σ: \\overline{x} \\pm t_{\\alpha/2,n-1}s\\sqrt{1 + \\frac{1}{n}}\nNote: Prediction intervals are always wider than confidence intervals because they account for both sampling variability and the variability of individual observations.\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Comparison of Confidence and Prediction Intervals:\"\n\n\n             Interval_Type              Purpose Sample_Mean Margin_Error\n                    &lt;char&gt;               &lt;char&gt;       &lt;num&gt;        &lt;num&gt;\n1: 95% Confidence Interval  For population mean     3233.75        19.93\n2: 95% Prediction Interval For next observation     3233.75        82.16\n   Lower_Bound Upper_Bound  Width\n         &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     3213.82     3253.68  39.85\n2:     3151.59     3315.91 164.31\n\n\n\n\n\n# Prediction interval example\n# Using concrete strength data\nprediction_data &lt;- concrete_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    Sample_Mean = fmean(strength),\n    Sample_SD = fsd(strength)\n  )\n\n# Calculate prediction interval for next observation\nt_alpha2_pred &lt;- qt(1 - alpha_concrete / 2, df = prediction_data$n - 1)\nprediction_margin &lt;- t_alpha2_pred * prediction_data$Sample_SD * sqrt(1 + 1 / prediction_data$n)\npred_lower &lt;- prediction_data$Sample_Mean - prediction_margin\npred_upper &lt;- prediction_data$Sample_Mean + prediction_margin\n\n# Compare with confidence interval\nconf_margin &lt;- t_alpha2_pred * prediction_data$Sample_SD / sqrt(prediction_data$n)\nconf_lower &lt;- prediction_data$Sample_Mean - conf_margin\nconf_upper &lt;- prediction_data$Sample_Mean + conf_margin\n\ninterval_comparison &lt;- data.table(\n  Interval_Type = c(\"95% Confidence Interval\", \"95% Prediction Interval\"),\n  Purpose = c(\"For population mean\", \"For next observation\"),\n  Sample_Mean = c(round(prediction_data$Sample_Mean, 2), round(prediction_data$Sample_Mean, 2)),\n  Margin_Error = c(round(conf_margin, 2), round(prediction_margin, 2)),\n  Lower_Bound = c(round(conf_lower, 2), round(pred_lower, 2)),\n  Upper_Bound = c(round(conf_upper, 2), round(pred_upper, 2)),\n  Width = c(round(conf_upper - conf_lower, 2), round(pred_upper - pred_lower, 2))\n)\n\nprint(\"Comparison of Confidence and Prediction Intervals:\")\nprint(interval_comparison)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteTolerance Intervals\n\n\n\nA tolerance interval is an interval that contains at least a specified proportion P of the population with confidence level 100(1-α)%.\nTwo-sided tolerance interval: \\overline{x} \\pm ks\nwhere k is a tolerance interval factor that depends on n, P, and 1-α.\nCommon applications:\n\nQuality control specifications\nProcess capability studies\nEngineering design limits\n\n\n\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Tolerance Interval Results:\"\n\n\n                Interval_Type                                        Purpose\n                       &lt;char&gt;                                         &lt;char&gt;\n1: 95%/95% Tolerance Interval Contains 95% of population with 95% confidence\n   Sample_Mean Sample_SD k_Factor Lower_Bound Upper_Bound  Width\n         &lt;num&gt;     &lt;num&gt;    &lt;num&gt;       &lt;num&gt;       &lt;num&gt;  &lt;num&gt;\n1:     3233.75     37.39    2.903     3125.19     3342.31 217.11\n                                                                 Interpretation\n                                                                         &lt;char&gt;\n1: 95% confident that 95% of concrete strengths are between 3125.19 and 3342.31\n\n\n\n\n\n# Tolerance interval example\n# For 95% of population with 95% confidence\n# Using tolerance interval factors (simplified version)\n\n# Approximate tolerance interval factors for n=16, 95% content, 95% confidence\nk_factor &lt;- 2.903 # This would typically come from statistical tables\n\ntolerance_lower &lt;- prediction_data$Sample_Mean - k_factor * prediction_data$Sample_SD\ntolerance_upper &lt;- prediction_data$Sample_Mean + k_factor * prediction_data$Sample_SD\n\ntolerance_results &lt;- data.table(\n  Interval_Type = \"95%/95% Tolerance Interval\",\n  Purpose = \"Contains 95% of population with 95% confidence\",\n  Sample_Mean = round(prediction_data$Sample_Mean, 2),\n  Sample_SD = round(prediction_data$Sample_SD, 2),\n  k_Factor = k_factor,\n  Lower_Bound = round(tolerance_lower, 2),\n  Upper_Bound = round(tolerance_upper, 2),\n  Width = round(tolerance_upper - tolerance_lower, 2),\n  Interpretation = paste(\n    \"95% confident that 95% of concrete strengths are between\",\n    round(tolerance_lower, 2), \"and\", round(tolerance_upper, 2)\n  )\n)\n\nprint(\"Tolerance Interval Results:\")\nprint(tolerance_results)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#summary-tables-of-inference-procedures-for-a-single-sample",
    "href": "book/Ch04.html#summary-tables-of-inference-procedures-for-a-single-sample",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "R OutputR Code\n\n\n\n\n\n\nTable 1: Summary of inference procedures for a single sample\n\n\n\n\nSummary of Inference Procedures for a Single Sample\n\n\nParameter\nTest_Statistic\nDistribution\nConfidence_Interval\nAssumptions\n\n\n\n\nMean (σ known)\nZ = (x̄ - μ₀)/(σ/√n)\nN(0,1)\nx̄ ± z_{α/2}σ/√n\nNormal population or large n\n\n\nMean (σ unknown)\nt = (x̄ - μ₀)/(s/√n)\nt_{n-1}\nx̄ ± t_{α/2,n-1}s/√n\nNormal population\n\n\nVariance\nχ² = (n-1)s²/σ₀²\nχ²_{n-1}\n((n-1)s²/χ²_{α/2}, (n-1)s²/χ²_{1-α/2})\nNormal population\n\n\nProportion\nZ = (p̂ - p₀)/√(p₀(1-p₀)/n)\nN(0,1)\np̂ ± z_{α/2}√(p̂(1-p̂)/n)\nLarge n, np≥5, n(1-p)≥5\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2\n\n\n# Summary table of inference procedures\ninference_summary &lt;- data.table(\n  Parameter = c(\"Mean (σ known)\", \"Mean (σ unknown)\", \"Variance\", \"Proportion\"),\n  Test_Statistic = c(\n    \"Z = (x̄ - μ₀)/(σ/√n)\", \"t = (x̄ - μ₀)/(s/√n)\",\n    \"χ² = (n-1)s²/σ₀²\", \"Z = (p̂ - p₀)/√(p₀(1-p₀)/n)\"\n  ),\n  Distribution = c(\"N(0,1)\", \"t_{n-1}\", \"χ²_{n-1}\", \"N(0,1)\"),\n  Confidence_Interval = c(\n    \"x̄ ± z_{α/2}σ/√n\", \"x̄ ± t_{α/2,n-1}s/√n\",\n    \"((n-1)s²/χ²_{α/2}, (n-1)s²/χ²_{1-α/2})\",\n    \"p̂ ± z_{α/2}√(p̂(1-p̂)/n)\"\n  ),\n  Assumptions = c(\n    \"Normal population or large n\", \"Normal population\",\n    \"Normal population\", \"Large n, np≥5, n(1-p)≥5\"\n  )\n)\n\nkbl(inference_summary,\n  caption = \"Summary of Inference Procedures for a Single Sample\"\n) %&gt;%\n  kable_styling() %&gt;%\n  column_spec(2, width = \"3cm\") %&gt;%\n  column_spec(4, width = \"3cm\") %&gt;%\n  column_spec(5, width = \"2.5cm\")",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#testing-for-goodness-of-fit",
    "href": "book/Ch04.html#testing-for-goodness-of-fit",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteChi-Square Goodness-of-Fit Test\n\n\n\nTests whether sample data fits a specified distribution.\nTest Statistic: \\chi_0^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\nwhere:\n\nO_i = observed frequency in cell i\nE_i = expected frequency in cell i\nk = number of cells\n\nUnder H₀, χ₀² ~ χ²_{k-1-p} where p = number of estimated parameters.\n\n\n\n\n\n\n\n\n\n\nNoteExample\n\n\n\nTest whether the following 50 observations follow a normal distribution:\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Data Summary for Normality Test:\"\n\n\n       n      Min       Q1   Median     Mean       Q3      Max       SD\n   &lt;int&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:    50 77.39232 93.06884 98.11524 100.3605 106.7775 124.0762 10.42499\n    Skewness   Kurtosis\n       &lt;num&gt;      &lt;num&gt;\n1: 0.4022879 -0.4221122\n\n\n\n\n\n# Goodness of fit test example\nset.seed(789)\nnormality_test_data &lt;- data.table(\n  observation_id = 1:50,\n  value = c(rnorm(40, mean = 100, sd = 10), rnorm(10, mean = 110, sd = 8)) # Mixed to test normality\n)\n\n# Summary statistics\nnormality_summary &lt;- normality_test_data %&gt;%\n  fsummarise(\n    n = fnobs(value),\n    Min = fmin(value),\n    Q1 = fquantile(value, 0.25),\n    Median = fmedian(value),\n    Mean = fmean(value),\n    Q3 = fquantile(value, 0.75),\n    Max = fmax(value),\n    SD = fsd(value),\n    Skewness = fsum((value - fmean(value))^3) / (fnobs(value) * fsd(value)^3),\n    Kurtosis = fsum((value - fmean(value))^4) / (fnobs(value) * fsd(value)^4) - 3\n  )\n\nprint(\"Data Summary for Normality Test:\")\nprint(normality_summary)\n\n\n\n\n\n\n\nSteps: 1. Estimate μ and σ from the sample 2. Define class intervals 3. Calculate expected frequencies under normality 4. Compute chi-square statistic 5. Compare to critical value\n\nR OutputR Code\n\n\n\n\n[1] \"Goodness of Fit Test Results:\"\n\n\n                         Test                  Null_Hypothesis     n Classes\n                       &lt;char&gt;                           &lt;char&gt; &lt;int&gt;   &lt;int&gt;\n1: Chi-square goodness of fit Data follows normal distribution    50       7\n   Chi2_Statistic Degrees_Freedom P_Value Alpha                   Decision\n            &lt;num&gt;           &lt;num&gt;   &lt;num&gt; &lt;num&gt;                     &lt;char&gt;\n1:          3.331               4   0.504  0.05 Fail to reject H0 (normal)\n                                 Conclusion\n                                     &lt;char&gt;\n1: Data consistent with normal distribution\n\n\n[1] \"Observed vs Expected Frequencies:\"\n\n\nKey: &lt;class_interval&gt;\n   class_interval     N  expected chi2_component\n           &lt;fctr&gt; &lt;int&gt;     &lt;num&gt;          &lt;num&gt;\n1:    [76.4,83.3]     1  2.029651    0.522346378\n2:    (83.3,90.3]     7  5.798476    0.248972473\n3:    (90.3,97.3]    14 10.782517    0.960091219\n4:     (97.3,104]    10 13.056560    0.715545084\n5:      (104,111]    10 10.296736    0.008551451\n6:      (111,118]     5  5.287598    0.015642709\n7:      (118,125]     3  1.767273    0.859864587\n\n\n[1] \"Shapiro-Wilk Test for comparison:\"\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  normality_test_data$value\nW = 0.971, p-value = 0.2541\n\n\n\n\n\n# Chi-square goodness of fit test for normality\nn_norm &lt;- fnobs(normality_test_data$value)\nsample_mean &lt;- fmean(normality_test_data$value)\nsample_sd &lt;- fsd(normality_test_data$value)\n\n# Create class intervals (using Sturges' rule for number of classes)\nk_classes &lt;- ceiling(1 + log2(n_norm)) # Sturges' rule\nclass_breaks &lt;- seq(\n  from = fmin(normality_test_data$value) - 1,\n  to = fmax(normality_test_data$value) + 1,\n  length.out = k_classes + 1\n)\n\n# Calculate observed frequencies\nnormality_test_data[, class_interval := cut(value, breaks = class_breaks, include.lowest = TRUE)]\nobserved_freq &lt;- normality_test_data[, .N, by = class_interval][order(class_interval)]\n\n# Calculate expected frequencies under normality\nexpected_freq &lt;- data.table()\nfor (i in 1:(length(class_breaks) - 1)) {\n  prob_class &lt;- pnorm(class_breaks[i + 1], mean = sample_mean, sd = sample_sd) -\n    pnorm(class_breaks[i], mean = sample_mean, sd = sample_sd)\n  expected_count &lt;- n_norm * prob_class\n  expected_freq &lt;- rbind(\n    expected_freq,\n    data.table(\n      class_interval = observed_freq$class_interval[i],\n      expected = expected_count\n    )\n  )\n}\n\n# Combine observed and expected\ngoodness_fit_table &lt;- merge(observed_freq, expected_freq, by = \"class_interval\")\ngoodness_fit_table[, chi2_component := (N - expected)^2 / expected]\n\n# Calculate chi-square statistic\nchi2_gof &lt;- sum(goodness_fit_table$chi2_component)\ndf_gof &lt;- nrow(goodness_fit_table) - 1 - 2 # -2 for estimated μ and σ\np_value_gof &lt;- 1 - pchisq(chi2_gof, df = df_gof)\n\ngoodness_fit_results &lt;- data.table(\n  Test = \"Chi-square goodness of fit\",\n  Null_Hypothesis = \"Data follows normal distribution\",\n  n = n_norm,\n  Classes = nrow(goodness_fit_table),\n  Chi2_Statistic = round(chi2_gof, 4),\n  Degrees_Freedom = df_gof,\n  P_Value = round(p_value_gof, 4),\n  Alpha = 0.05,\n  Decision = ifelse(p_value_gof &lt; 0.05, \"Reject H0 (not normal)\", \"Fail to reject H0 (normal)\"),\n  Conclusion = ifelse(p_value_gof &lt; 0.05,\n    \"Data does not follow normal distribution\",\n    \"Data consistent with normal distribution\"\n  )\n)\n\nprint(\"Goodness of Fit Test Results:\")\nprint(goodness_fit_results)\nprint(\"Observed vs Expected Frequencies:\")\nprint(goodness_fit_table)\n\n# Alternative tests for normality\nshapiro_test &lt;- shapiro.test(normality_test_data$value)\nprint(\"Shapiro-Wilk Test for comparison:\")\nprint(shapiro_test)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteComprehensive Example\n\n\n\nA manufacturing company wants to perform a complete statistical analysis of their production process. They collect data on 100 products measuring diameter, strength, and defect status.\n\nR OutputR CodeDownload Data\n\n\n\n\n[1] \"Comprehensive Quality Control Data Summary:\"\n\n\n       n Mean_Diameter SD_Diameter Min_Diameter Max_Diameter Mean_Strength\n   &lt;int&gt;         &lt;num&gt;       &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;\n1:   100      10.02267   0.1838186     9.477965     10.97406      246.6658\n   SD_Strength Min_Strength Max_Strength Total_Defects Defect_Rate\n         &lt;num&gt;        &lt;num&gt;        &lt;num&gt;         &lt;num&gt;       &lt;num&gt;\n1:    20.84695     191.4787     285.4509             8        0.08\n   Defect_Percent\n            &lt;num&gt;\n1:              8\n\n\nError in eval(e, .data, pe): object 'p_value' not found\n\n\nError in pchisq(chi2_stat, df = df): Non-numeric argument to mathematical function\n\n\nError in eval(e, .data, pe): object 'z_stat' not found\n\n\nError: object 'diameter_test' not found\n\n\n[1] \"Comprehensive Hypothesis Test Results:\"\n\n\nError: object 'comprehensive_test_results' not found\n\n\n\n\n\n# Comprehensive quality control analysis\nset.seed(456)\ncomprehensive_qc_data &lt;- data.table(\n  product_id = 1:100,\n  diameter = rnorm(100, mean = 10.0, sd = 0.15),\n  strength = rnorm(100, mean = 250, sd = 20),\n  defective = sample(c(0, 1), 100, replace = TRUE, prob = c(0.92, 0.08))\n)\n\n# Add some outliers to make it realistic\ncomprehensive_qc_data[sample(.N, 3), diameter := diameter + rnorm(3, 0, 0.5)]\ncomprehensive_qc_data[sample(.N, 2), strength := strength - 50]\n\n# Comprehensive summary\nqc_summary &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    # Diameter statistics\n    Mean_Diameter = fmean(diameter),\n    SD_Diameter = fsd(diameter),\n    Min_Diameter = fmin(diameter),\n    Max_Diameter = fmax(diameter),\n    # Strength statistics\n    Mean_Strength = fmean(strength),\n    SD_Strength = fsd(strength),\n    Min_Strength = fmin(strength),\n    Max_Strength = fmax(strength),\n    # Defect statistics\n    Total_Defects = fsum(defective),\n    Defect_Rate = fmean(defective),\n    Defect_Percent = fmean(defective) * 100\n  )\n\nprint(\"Comprehensive Quality Control Data Summary:\")\nprint(qc_summary)\n\n# Multiple hypothesis tests\nalpha_qc &lt;- 0.05\n\n# 1. Test diameter mean vs specification (10.0)\ndiameter_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(diameter),\n    xbar = fmean(diameter),\n    s = fsd(diameter),\n    mu0 = 10.0,\n    t_stat = (fmean(diameter) - 10.0) / (fsd(diameter) / sqrt(fnobs(diameter))),\n    df = fnobs(diameter) - 1,\n    p_value = 2 * (1 - pt(abs((fmean(diameter) - 10.0) / (fsd(diameter) / sqrt(fnobs(diameter)))),\n      df = fnobs(diameter) - 1\n    )),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\n# 2. Test strength variance\nstrength_var_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(strength),\n    s2 = fvar(strength),\n    sigma2_0 = 400, # Target variance\n    chi2_stat = (fnobs(strength) - 1) * fvar(strength) / 400,\n    df = fnobs(strength) - 1,\n    p_value = 2 * min(pchisq(chi2_stat, df = df), 1 - pchisq(chi2_stat, df = df)),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\n# 3. Test defect proportion\ndefect_prop_test &lt;- comprehensive_qc_data %&gt;%\n  fsummarise(\n    n = fnobs(defective),\n    x = fsum(defective),\n    p_hat = fmean(defective),\n    p0 = 0.05, # Target defect rate\n    z_stat = (fmean(defective) - 0.05) / sqrt(0.05 * 0.95 / fnobs(defective)),\n    p_value = 2 * (1 - pnorm(abs(z_stat))),\n    decision = ifelse(p_value &lt; alpha_qc, \"Reject H0\", \"Fail to reject H0\")\n  )\n\ncomprehensive_test_results &lt;- data.table(\n  Test = c(\"Diameter Mean\", \"Strength Variance\", \"Defect Proportion\"),\n  Parameter = c(\"μ = 10.0\", \"σ² = 400\", \"p = 0.05\"),\n  Test_Statistic = c(\n    round(diameter_test$t_stat, 3),\n    round(strength_var_test$chi2_stat, 3),\n    round(defect_prop_test$z_stat, 3)\n  ),\n  P_Value = c(\n    round(diameter_test$p_value, 4),\n    round(strength_var_test$p_value, 4),\n    round(defect_prop_test$p_value, 4)\n  ),\n  Decision = c(diameter_test$decision, strength_var_test$decision, defect_prop_test$decision),\n  Distribution = c(\n    paste0(\"t(\", diameter_test$df, \")\"),\n    paste0(\"χ²(\", strength_var_test$df, \")\"),\n    \"N(0,1)\"\n  )\n)\n\nprint(\"Comprehensive Hypothesis Test Results:\")\nprint(comprehensive_test_results)\n\n\n\n\n\n\n\nComplete Analysis: 1. Test mean diameter vs. specification 2. Test variance in strength 3. Test defect proportion 4. Construct confidence intervals 5. Create prediction intervals 6. Establish tolerance intervals 7. Test for normality\n\nR OutputR Code\n\n\n\n\n\n\n\n\n\n\nFigure 3: Comprehensive quality control analysis dashboard\n\n\n\n\n\n\n\n\n# Comprehensive quality control dashboard\nlibrary(gridExtra)\n\n# Diameter histogram with normal overlay\np1 &lt;- ggplot(data = comprehensive_qc_data, mapping = aes(x = diameter)) +\n  geom_histogram(\n    mapping = aes(y = after_stat(density)), bins = 20,\n    fill = \"lightblue\", color = \"black\", alpha = 0.7\n  ) +\n  stat_function(\n    fun = dnorm,\n    args = list(\n      mean = mean(comprehensive_qc_data$diameter),\n      sd = sd(comprehensive_qc_data$diameter)\n    ),\n    color = \"red\", linewidth = 1\n  ) +\n  geom_vline(xintercept = 10.0, linetype = \"dashed\", color = \"green\", linewidth = 1) +\n  labs(x = \"Diameter\", y = \"Density\", title = \"Diameter Distribution\") +\n  theme_classic()\n\n# Strength boxplot\np2 &lt;- ggplot(data = comprehensive_qc_data, mapping = aes(x = \"\", y = strength)) +\n  geom_boxplot(fill = \"lightgreen\", alpha = 0.7) +\n  geom_hline(yintercept = 250, linetype = \"dashed\", color = \"red\") +\n  labs(x = \"\", y = \"Strength\", title = \"Strength Distribution\") +\n  theme_classic() +\n  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())\n\n# Defect rate bar chart\ndefect_summary_plot &lt;- comprehensive_qc_data[, .(Count = .N), by = defective]\ndefect_summary_plot[, Status := ifelse(defective == 0, \"Good\", \"Defective\")]\n\np3 &lt;- ggplot(data = defect_summary_plot, mapping = aes(x = Status, y = Count, fill = Status)) +\n  geom_col(alpha = 0.7) +\n  scale_fill_manual(values = c(\"Good\" = \"green\", \"Defective\" = \"red\")) +\n  labs(x = \"Product Status\", y = \"Count\", title = \"Defect Analysis\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n# Control chart simulation\nset.seed(123)\ncontrol_data &lt;- data.table(\n  sample_num = 1:30,\n  sample_mean = rnorm(30, mean = 10.0, sd = 0.05)\n)\ncontrol_data[c(25, 28), sample_mean := c(10.3, 9.7)] # Add out-of-control points\n\nucl &lt;- 10.0 + 3 * 0.05\nlcl &lt;- 10.0 - 3 * 0.05\n\np4 &lt;- ggplot(data = control_data, mapping = aes(x = sample_num, y = sample_mean)) +\n  geom_line(color = \"blue\", linewidth = 0.8) +\n  geom_point(size = 2, color = \"red\") +\n  geom_hline(yintercept = 10.0, color = \"green\", linewidth = 1) +\n  geom_hline(yintercept = ucl, color = \"red\", linetype = \"dashed\") +\n  geom_hline(yintercept = lcl, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Sample Number\", y = \"Sample Mean\", title = \"Control Chart\") +\n  theme_classic()\n\n# Combine plots\ngrid.arrange(p1, p2, p3, p4, nrow = 2, ncol = 2)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch04.html#key-formulas-and-critical-values",
    "href": "book/Ch04.html#key-formulas-and-critical-values",
    "title": "1 Decision Making for a Single Sample",
    "section": "",
    "text": "NoteEssential Formulas Summary\n\n\n\nTest Statistics:\n\nZ-test (σ known): Z_0 = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}}\nt-test (σ unknown): T_0 = \\frac{\\overline{X} - \\mu_0}{S/\\sqrt{n}}\nChi-square (variance): \\chi_0^2 = \\frac{(n-1)S^2}{\\sigma_0^2}\nZ-test (proportion): Z_0 = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}}\n\nConfidence Intervals:\n\nMean (σ known): \\overline{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\nMean (σ unknown): \\overline{x} \\pm t_{\\alpha/2,n-1} \\frac{s}{\\sqrt{n}}\nVariance: \\frac{(n-1)s^2}{\\chi_{\\alpha/2,n-1}^2} \\leq \\sigma^2 \\leq \\frac{(n-1)s^2}{\\chi_{1-\\alpha/2,n-1}^2}\nProportion: \\hat{p} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\n\nOther Intervals:\n\nPrediction: \\overline{x} \\pm t_{\\alpha/2,n-1}s\\sqrt{1 + \\frac{1}{n}}\nTolerance: \\overline{x} \\pm ks (k from tables)\n\n\n\n\nR OutputR Code\n\n\n\n\n\n\nTable 3: Critical values for common significance levels\n\n\n\n\nCritical Values for Common Significance Levels\n\n\n\n\n\n\n\n\n\n\n\n\nNormal\n\n\nt-distribution\n\n\nChi-square\n\n\n\nα\ndf\nz_{α/2}\nt_{α/2,df}\nχ²_{α/2,df}\nχ²_{1-α/2,df}\n\n\n\n\nα = 0.05\n\n\n0.05\n5\n1.960\n2.571\n0.831\n12.833\n\n\n0.05\n10\n1.960\n2.228\n3.247\n20.483\n\n\n0.05\n20\n1.960\n2.086\n9.591\n34.170\n\n\n0.05\n30\n1.960\n2.042\n16.791\n46.979\n\n\n0.05\n100\n1.960\n1.984\n74.222\n129.561\n\n\nα = 0.01\n\n\n0.01\n5\n2.576\n4.032\n0.412\n16.750\n\n\n0.01\n10\n2.576\n3.169\n2.156\n25.188\n\n\n0.01\n20\n2.576\n2.845\n7.434\n39.997\n\n\n0.01\n30\n2.576\n2.750\n13.787\n53.672\n\n\n0.01\n100\n2.576\n2.626\n67.328\n140.169\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 4\n\n\n# Critical values table\nalpha_levels &lt;- c(0.10, 0.05, 0.01)\ndf_values &lt;- c(1, 2, 5, 10, 15, 20, 30, 50, 100, 1000)\n\ncritical_values_table &lt;- data.table()\nfor (alpha_val in alpha_levels) {\n  for (df_val in df_values) {\n    z_crit &lt;- qnorm(1 - alpha_val / 2)\n    t_crit &lt;- qt(1 - alpha_val / 2, df = df_val)\n    chi2_lower &lt;- qchisq(alpha_val / 2, df = df_val)\n    chi2_upper &lt;- qchisq(1 - alpha_val / 2, df = df_val)\n\n    critical_values_table &lt;- rbind(\n      critical_values_table,\n      data.table(\n        alpha = alpha_val,\n        df = df_val,\n        z_alpha2 = round(z_crit, 3),\n        t_alpha2 = round(t_crit, 3),\n        chi2_alpha2 = round(chi2_lower, 3),\n        chi2_1_alpha2 = round(chi2_upper, 3)\n      )\n    )\n  }\n}\n\n# Create formatted table for display\ncritical_display &lt;- critical_values_table[alpha %in% c(0.05, 0.01) &\n  df %in% c(5, 10, 20, 30, 100)]\n\nkbl(critical_display,\n  col.names = c(\"α\", \"df\", \"z_{α/2}\", \"t_{α/2,df}\", \"χ²_{α/2,df}\", \"χ²_{1-α/2,df}\"),\n  caption = \"Critical Values for Common Significance Levels\"\n) %&gt;%\n  kable_styling() %&gt;%\n  add_header_above(c(\" \" = 2, \"Normal\" = 1, \"t-distribution\" = 1, \"Chi-square\" = 2)) %&gt;%\n  pack_rows(\"α = 0.05\", 1, 5) %&gt;%\n  pack_rows(\"α = 0.01\", 6, 10)\n\n# Sample size determination examples",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Decision Making for a Single Sample"
    ]
  },
  {
    "objectID": "book/Ch03.html",
    "href": "book/Ch03.html",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteMajor Themes of Chapter 3\n\n\n\n\nRandom Variables: Understand discrete and continuous random variables and their probability distributions\nProbability Distributions: Master key continuous distributions (normal, exponential, Weibull, gamma, beta, lognormal) and discrete distributions (binomial, Poisson)\nDistribution Properties: Calculate means, variances, and probabilities using probability density/mass functions and cumulative distribution functions\nNormal Distribution: Apply standardization techniques and use normal approximations to other distributions\nEngineering Applications: Model real-world engineering phenomena using appropriate probability distributions\nMultiple Variables: Analyze joint distributions, independence, and functions of random variables including error propagation\n\n\n\n\n\n\n\n\n\nImportantLearning Objectives\n\n\n\nAfter careful study of this chapter, you should be able to do the following:\n\nDetermine probabilities for discrete random variables from probability mass functions and for continuous random variables from probability density functions, and use cumulative distribution functions in both cases.\nCalculate means and variances for discrete and continuous random variables.\nUnderstand the assumptions for each of the probability distributions presented.\nSelect an appropriate probability distribution to calculate probabilities in specific applications.\nUse the table (or software) for the cumulative distribution function of a standard normal distribution to calculate probabilities.\nApproximate probabilities for binomial and Poisson distributions.\nInterpret and calculate covariances and correlations between random variables.\nCalculate means and variances for linear combinations of random variables.\nApproximate means and variances for general functions of several random variables using error propagation.\nUnderstand statistics and the central limit theorem.\n\n\n\n\n\n\n\n\n\n\n\nNoteIntroduction to Random Experiments\n\n\n\nA random experiment is an experiment that can result in different outcomes, even though it is repeated in the same manner every time. The collection of all possible outcomes of a random experiment is called the sample space of the experiment.\nKey Concepts:\n\nExperiment: A process that generates observations\nRandom: The outcome cannot be predicted with certainty\nRandom experiment: An experiment where the outcome varies unpredictably\n\nFigure 1 illustrates the relationship between a mathematical model and the physical system it represents. While models like Newton’s laws are not perfect abstractions, they are useful for analyzing and approximating system performance. Once validated with measurements, models can help understand, describe, and predict a system’s response to inputs.\n\n\n\n\n\n\nFigure 1: Continuous iteration between model and physical system.\n\n\n\nFigure 2 illustrates a model where uncontrollable variables (noise) interact with controllable variables to produce system outputs. Due to the noise, identical controllable settings yield varying outputs upon measurement.\n\n\n\n\n\n\nFigure 2: Noise variables affect the transformation of inputs to outputs.\n\n\n\nFor measuring current in a copper wire, Ohm’s law may suffice as a model. However, if variations are significant, the model may need to account for them (see Figure 3).\n\n\n\n\n\n\nFigure 3: A closer examination of the system identifies deviations from the model.\n\n\n\nIn designing a voice communication system, a model is required for call frequency and duration. Even if calls occur and last precisely 5 minutes on average, variations in timing or duration can lead to call blocking, necessitating multiple lines (see Figure 4).\n\n\n\n\n\n\nFigure 4: Variation causes disruptions in the system.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Variables\n\n\n\nA random variable is a function that assigns a real number to each outcome in the sample space of a random experiment. It provides a numerical description of the outcome of a random experiment.\nDefinition: A random variable is a numerical variable whose measured value can change from one replicate of the experiment to another.\nTypes of Random Variables:\n\nDiscrete Random Variable: A random variable with a finite (or countably infinite) set of real numbers for its range.\n\nExamples: number of scratches on a surface, proportion of defective parts among 1000 tested, number of transmitted bits received in error\n\nContinuous Random Variable: A random variable with an interval (either finite or infinite) of real numbers for its range.\n\nExamples: electrical current, length, pressure, temperature, time, voltage, weight\n\n\nThe distinction between discrete and continuous random variables is important because different methods are used to work with each type.\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Fundamentals\n\n\n\nProbability is used to quantify likelihood or chance and represents risk or uncertainty in engineering applications. It can be interpreted as our degree of belief or relative frequency.\nKey Properties:\n\nProbability statements describe the likelihood that particular values occur\nThe likelihood is quantified by assigning a number from the interval [0, 1] to the set of values (or a percentage from 0 to 100%)\nHigher numbers indicate that the set of values is more likely\nA probability is usually expressed in terms of a random variable\n\nFor example, if X denotes part length, the probability statement can be written as: P(X \\in [10.8, 11.2]) = 0.25 \\quad \\text{or} \\quad P(10.8 \\leq X \\leq 11.2) = 0.25\nBoth equations state that the probability that the random variable X assumes a value in [10.8, 11.2] is 0.25.\n\n\n\n\n\n\n\n\n\n\nNoteComplement of an Event\n\n\n\nGiven a set E, the complement of E is the set of elements that are not in E. The complement is denoted as E' or E^c.\nProperty: P(E^c) = 1 - P(E)\n\n\n\n\n\n\n\n\n\n\n\nNoteMutually Exclusive Events\n\n\n\nThe sets E_1, E_2, \\ldots, E_k are mutually exclusive if the intersection of any pair is empty. That is, each element is in one and only one of the sets E_1, E_2, \\ldots, E_k.\nProperty: If events are mutually exclusive, their probabilities add.\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Properties\n\n\n\n\nP(X \\in \\mathbb{R}) = 1, where \\mathbb{R} is the set of real numbers\n0 \\leq P(X \\in E) \\leq 1 for any set E\nIf E_1, E_2, \\ldots, E_k are mutually exclusive sets:\n\n\n\\small\nP(X \\in E_1 \\cup E_2 \\cup \\ldots \\cup E_k) = P(X \\in E_1) + P(X \\in E_2) + \\cdots + P(X \\in E_k)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEvents\n\n\n\nAn event is a subset of the sample space. Sometimes, experimental results are classified into categories rather than measured numerically.\nExamples:\n\nCurrent measurement recorded as low, medium, or high\nElectronic component classified as defective or not defective\nMessage transmission success or failure\n\nEvents provide a framework for probability calculations in both discrete and continuous settings.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Density Function\n\n\n\nThe probability distribution or simply distribution of a random variable X is a description of the set of probabilities associated with the possible values for X.\nThe probability density function (PDF) f(x) of a continuous random variable X is used to determine probabilities as follows:\nP(a &lt; X &lt; b) = \\int_{a}^{b} f(x)\\,dx\nProperties of the PDF:\n\nf(x) \\geq 0 for all x\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\nImportant Note: If X is a continuous random variable, for any x_1 and x_2:\n\n\\small\nP(x_1 \\leq X \\leq x_2) = P(x_1 &lt; X \\leq x_2) = P(x_1 \\leq X &lt; x_2) = P(x_1 &lt; X &lt; x_2)\n\nThis is because P(X = c) = 0 for any specific value c when X is continuous.\n\n\n\n\n\n\n\n\nTipExample 3-1: Exponential Distribution for Disk Flaw Distance\n\n\n\nLet the continuous random variable X denote the distance in micrometers from the start of a track on a magnetic disk until the first flaw. Historical data show that the distribution of X can be modeled by the probability density function:\nf(x) = \\frac{1}{2000} e^{-x/2000}, \\quad x \\geq 0\nProblem 1: For what proportion of disks is the distance to the first flaw greater than 1000 micrometers?\nSolution: \\begin{aligned}\nP(X &gt; 1000) &= \\int_{1000}^{\\infty} f(x)\\,dx \\\\\n&= \\int_{1000}^{\\infty} \\frac{1}{2000} e^{-x/2000} \\,dx \\\\\n&= \\left[ -e^{-x/2000} \\right]_{1000}^{\\infty} \\\\\n&= 0 - (-e^{-1000/2000}) \\\\\n&= e^{-1/2} \\\\\n&\\approx 0.6065\n\\end{aligned}\nProblem 2: What proportion of disks has the flaw distance between 1000 and 2000 micrometers?\nSolution: \\begin{aligned}\nP(1000 \\leq X \\leq 2000) &= \\int_{1000}^{2000} f(x)\\,dx \\\\\n&= \\left[ -e^{-x/2000} \\right]_{1000}^{2000} \\\\\n&= e^{-1/2} - e^{-1} \\\\\n&= 0.6065 - 0.3679 \\\\\n&= 0.2386\n\\end{aligned}\n\n\n\n\n\n\nFigure 5: Probability density function for disk flaw example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Disk Flaw Distance Analysis:\"\n\n\n   lambda mean_param prob_greater_1000 prob_between_1000_2000\n    &lt;num&gt;      &lt;num&gt;             &lt;num&gt;                  &lt;num&gt;\n1:  5e-04       2000         0.6065307              0.2386512\n   prob_greater_1000_manual prob_between_manual\n                      &lt;num&gt;               &lt;num&gt;\n1:                0.6065307           0.2386512\n\n\n\n\n\n# Example 3-1: Exponential Distribution for Disk Flaw Distance\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(fitdistrplus)\nlibrary(scales)\n\n# Define the exponential PDF for disk flaw example\ndisk_data &lt;- data.table(\n  lambda = 1 / 2000,\n  mean_param = 2000\n) %&gt;%\n  fmutate(\n    # Calculate probabilities\n    prob_greater_1000 = pexp(q = 1000, rate = lambda, lower.tail = FALSE),\n    prob_between_1000_2000 = pexp(q = 2000, rate = lambda, lower.tail = TRUE) -\n      pexp(q = 1000, rate = lambda, lower.tail = TRUE),\n    # Verify using manual integration\n    prob_greater_1000_manual = exp(-1000 / mean_param),\n    prob_between_manual = exp(-1000 / mean_param) - exp(-2000 / mean_param)\n  )\n\nprint(\"Disk Flaw Distance Analysis:\")\nprint(disk_data)\n\n# Create visualization\nx_vals &lt;- seq(0, 8000, by = 50)\npdf_vals &lt;- dexp(x_vals, rate = 1 / 2000)\n\nfwrite(data.table(x = x_vals, pdf = pdf_vals), \"data/exponential_pdf.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function\n\n\n\nThe cumulative distribution function (CDF) of a continuous random variable X with probability density function f(x) is:\nF(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f(u)\\,du\nfor -\\infty &lt; x &lt; \\infty.\nKey Relationships:\n\nP(a &lt; X &lt; b) = F(b) - F(a)\n\\frac{d}{dx}F(x) = f(x) (fundamental theorem of calculus)\nF(x) is non-decreasing\n\\lim_{x \\to -\\infty} F(x) = 0 and \\lim_{x \\to \\infty} F(x) = 1\n\n\n\n\n\n\n\n\n\nTipExample 3-2: CDF for Disk Flaw Distance\n\n\n\nContinuing with the disk flaw example, the CDF is:\n\\begin{aligned}\nF(x) &= P(X \\leq x) = \\int_{0}^{x} \\frac{1}{2000} e^{-u/2000}\\,du \\\\\n&= \\left[ -e^{-u/2000} \\right]_{0}^{x} \\\\\n&= 1 - e^{-x/2000}\n\\end{aligned}\nfor x \\geq 0, and F(x) = 0 for x &lt; 0.\nVerification: We can check that P(X &gt; 1000) = 1 - F(1000) = 1 - (1 - e^{-1/2}) = e^{-1/2} = 0.6065\n\n\n\n\n\n\nFigure 6: Cumulative distribution function for disk flaw example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"CDF Verification:\"\n\n\n       x cdf_value cdf_manual prob_greater_1000_cdf prob_between_1000_2000_cdf\n   &lt;num&gt;     &lt;num&gt;      &lt;num&gt;                 &lt;num&gt;                      &lt;num&gt;\n1:  1000 0.3934693  0.3934693             0.6065307                  0.2386512\n2:  2000 0.6321206  0.6321206             0.6065307                  0.2386512\n\n\n\n\n\n# Example 3-2: CDF for Disk Flaw Distance\ncdf_data &lt;- data.table(\n  x = x_vals\n) %&gt;%\n  fmutate(\n    cdf_value = pexp(x, rate = 1 / 2000),\n    cdf_manual = 1 - exp(-x / 2000),\n    # Verify probabilities using CDF\n    prob_greater_1000_cdf = 1 - pexp(1000, rate = 1 / 2000),\n    prob_between_1000_2000_cdf = pexp(2000, rate = 1 / 2000) - pexp(1000, rate = 1 / 2000)\n  )\n\nprint(\"CDF Verification:\")\nprint(cdf_data[x %in% c(1000, 2000)])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMean and Variance for Continuous Random Variables\n\n\n\nSuppose X is a continuous random variable with PDF f(x).\nMean (Expected Value): The mean or expected value of X, denoted as \\mu or E(X), is:\n\n\\mu = E(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\nVariance: The variance of X, denoted as V(X) or \\sigma^2, is:\n\n\\begin{aligned}\n\\sigma^2 = V(X) &= \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)\\,dx \\\\\n&= E(X^2) - [E(X)]^2\n\\end{aligned}\n\nStandard Deviation: The standard deviation of X is \\sigma = \\sqrt{V(X)}.\nInterpretation:\n\nThe mean represents the “center” or average value\nThe variance measures the spread or variability\nStandard deviation is in the same units as the original variable\n\n\n\n\n\n\n\n\n\nTipExample 3-3: Mean and Variance Calculation\n\n\n\nFor the disk flaw distance example with f(x) = \\frac{1}{2000} e^{-x/2000} for x \\geq 0:\nMean Calculation: \\begin{aligned}\nE(X) &= \\int_{0}^{\\infty} x \\cdot \\frac{1}{2000} e^{-x/2000}\\,dx\n\\end{aligned}\nUsing integration by parts with u = x and dv = \\frac{1}{2000} e^{-x/2000} dx:\n\\begin{aligned}\nE(X) &= \\left[ -x e^{-x/2000} \\right]_{0}^{\\infty} + \\int_{0}^{\\infty} e^{-x/2000}\\,dx \\\\\n&= 0 + 2000 \\left[ -e^{-x/2000} \\right]_{0}^{\\infty} \\\\\n&= 2000(0 - (-1)) = 2000\n\\end{aligned}\nVariance Calculation: For this exponential distribution, V(X) = (2000)^2 = 4,000,000.\nInterpretation: On average, the first flaw occurs at 2000 micrometers, with substantial variability (standard deviation = 2000 micrometers).\n\nR OutputR Code\n\n\n\n\n[1] \"Exponential Distribution Statistics:\"\n\n\n    rate scale theoretical_mean theoretical_variance theoretical_sd mean_manual\n   &lt;num&gt; &lt;num&gt;            &lt;num&gt;                &lt;num&gt;          &lt;num&gt;       &lt;num&gt;\n1: 5e-04  2000             2000                4e+06           2000        2000\n   variance_manual\n             &lt;num&gt;\n1:           4e+06\n\n\n\n\n\n# Example 3-3: Mean and Variance Calculation\nexponential_stats &lt;- data.table(\n  rate = 1 / 2000,\n  scale = 2000\n) %&gt;%\n  fmutate(\n    theoretical_mean = 1 / rate,\n    theoretical_variance = 1 / rate^2,\n    theoretical_sd = sqrt(theoretical_variance),\n    # Verify mean calculation manually\n    mean_manual = scale,\n    variance_manual = scale^2\n  )\n\nprint(\"Exponential Distribution Statistics:\")\nprint(exponential_stats)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNormal Distribution\n\n\n\nThe normal distribution is the most widely used model for continuous random variables. It’s also called the Gaussian distribution.\nProbability Density Function: A random variable X has a normal distribution with parameters \\mu and \\sigma if:\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right\\}\nfor -\\infty &lt; x &lt; \\infty, where -\\infty &lt; \\mu &lt; \\infty and \\sigma &gt; 0.\nParameters:\n\nE(X) = \\mu (location parameter)\nV(X) = \\sigma^2 (scale parameter)\n\nNotation: X \\sim N(\\mu, \\sigma^2)\nKey Properties:\n\nSymmetric about \\mu\nBell-shaped curve\nInflection points at \\mu \\pm \\sigma\nAbout 68% of values within \\mu \\pm \\sigma\nAbout 95% of values within \\mu \\pm 2\\sigma\nAbout 99.7% of values within \\mu \\pm 3\\sigma\n\n\n\n\n\n\n\nFigure 7: Normal probability density functions for selected values of \\mu and \\sigma^2\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-4: Current Measurement\n\n\n\nAssume that current measurements in a wire follow a normal distribution with mean \\mu = 10 milliamperes and variance \\sigma^2 = 4 (so \\sigma = 2) milliamperes^2. What is the probability that a measurement exceeds 13 milliamperes?\nSolution: We need P(X &gt; 13) where X \\sim N(10, 4).\nThis probability corresponds to the area under the normal curve to the right of 13, as shown in Figure 8.\n\n\n\n\n\n\nFigure 8: Probability that X &gt; 13 for normal distribution with \\mu = 10, \\sigma^2 = 4\n\n\n\nSince there’s no closed-form expression for the normal integral, we use standardization or statistical software.\n\n\n\n\n\n\n\n\nNoteEmpirical Rule for Normal Distribution\n\n\n\nFor any normal distribution:\n\\begin{aligned}\nP(\\mu - \\sigma &lt; X &lt; \\mu + \\sigma) &= 0.6827 \\\\\nP(\\mu - 2\\sigma &lt; X &lt; \\mu + 2\\sigma) &= 0.9545 \\\\\nP(\\mu - 3\\sigma &lt; X &lt; \\mu + 3\\sigma) &= 0.9973\n\\end{aligned}\n\n\n\n\n\n\nFigure 9: Probabilities associated with a normal distribution\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Distribution Empirical Rule:\"\n\n\n      mu sigma prob_1sigma prob_2sigma prob_3sigma theoretical_1sigma\n   &lt;num&gt; &lt;num&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;              &lt;num&gt;\n1:     0     1   0.6826895   0.9544997   0.9973002             0.6827\n   theoretical_2sigma theoretical_3sigma\n                &lt;num&gt;              &lt;num&gt;\n1:             0.9545             0.9973\n\n\n\n\n\n# Normal Distribution Properties and Empirical Rule\nnormal_props &lt;- data.table(\n  mu = 0,\n  sigma = 1\n) %&gt;%\n  fmutate(\n    # Empirical rule calculations\n    prob_1sigma = pnorm(mu + sigma) - pnorm(mu - sigma),\n    prob_2sigma = pnorm(mu + 2 * sigma) - pnorm(mu - 2 * sigma),\n    prob_3sigma = pnorm(mu + 3 * sigma) - pnorm(mu - 3 * sigma),\n    # Theoretical values\n    theoretical_1sigma = 0.6827,\n    theoretical_2sigma = 0.9545,\n    theoretical_3sigma = 0.9973\n  )\n\nprint(\"Normal Distribution Empirical Rule:\")\nprint(normal_props)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStandard Normal Random Variable\n\n\n\nA normal random variable with \\mu = 0 and \\sigma^2 = 1 is called a standard normal random variable, denoted as Z.\nStandard Normal CDF: \\Phi(z) = P(Z \\leq z)\nStandardizing: If X \\sim N(\\mu, \\sigma^2), then:\nZ = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\nKey Property: P(X \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\n\n\n\n\n\n\nFigure 10: Standard normal probability density function\n\n\n\n\n\n\n\n\n\n\nFigure 11: Cumulative Standard Normal Distribution Table\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-5: Standard Normal Calculations\n\n\n\nProblem 1: P(Z &gt; 1.26)\nSolution: P(Z &gt; 1.26) = 1 - P(Z \\leq 1.26) = 1 - 0.89617 = 0.10383\nProblem 2: P(Z &lt; -0.86)\nSolution: P(Z &lt; -0.86) = 0.19489\nProblem 3: P(-1.25 &lt; Z &lt; 0.37)\nSolution: P(-1.25 &lt; Z &lt; 0.37) = P(Z &lt; 0.37) - P(Z &lt; -1.25) = 0.64431 - 0.10565 = 0.53866\nProblem 4: Find z such that P(Z &gt; z) = 0.05\nSolution: We need P(Z \\leq z) = 0.95. From the table, z = 1.645.\n\n\n\n\n\n\nFigure 12: Graphical displays for standard normal examples\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Standard Normal Calculations:\"\n\n\n                  calculation       result\n                       &lt;char&gt;        &lt;num&gt;\n1:                P(Z &gt; 1.26) 1.038347e-01\n2:               P(Z &lt; -0.86) 1.948945e-01\n3:               P(Z &gt; -1.37) 9.146565e-01\n4:        P(-1.25 &lt; Z &lt; 0.37) 5.386590e-01\n5:               P(Z &lt;= -4.6) 2.112455e-06\n6:      z for P(Z &gt; z) = 0.05 1.644854e+00\n7: z for P(-z &lt; Z &lt; z) = 0.99 2.575829e+00\n\n\n\n\n\n# Example 3-5: Standard Normal Calculations\nstandard_normal_calcs &lt;- data.table(\n  calculation = c(\n    \"P(Z &gt; 1.26)\", \"P(Z &lt; -0.86)\", \"P(Z &gt; -1.37)\",\n    \"P(-1.25 &lt; Z &lt; 0.37)\", \"P(Z &lt;= -4.6)\", \"z for P(Z &gt; z) = 0.05\",\n    \"z for P(-z &lt; Z &lt; z) = 0.99\"\n  )\n) %&gt;%\n  fmutate(\n    result = case_when(\n      calculation == \"P(Z &gt; 1.26)\" ~ pnorm(1.26, lower.tail = FALSE),\n      calculation == \"P(Z &lt; -0.86)\" ~ pnorm(-0.86),\n      calculation == \"P(Z &gt; -1.37)\" ~ pnorm(-1.37, lower.tail = FALSE),\n      calculation == \"P(-1.25 &lt; Z &lt; 0.37)\" ~ pnorm(0.37) - pnorm(-1.25),\n      calculation == \"P(Z &lt;= -4.6)\" ~ pnorm(-4.6),\n      calculation == \"z for P(Z &gt; z) = 0.05\" ~ qnorm(0.05, lower.tail = FALSE),\n      calculation == \"z for P(-z &lt; Z &lt; z) = 0.99\" ~ qnorm(0.995),\n      TRUE ~ NA_real_\n    )\n  )\n\nprint(\"Standard Normal Calculations:\")\nprint(standard_normal_calcs)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-6: Shaft Diameter Quality Control\n\n\n\nThe diameter of a shaft is normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. The specifications are 0.2500 \\pm 0.0015 inch. What proportion of shafts conforms to specifications?\nSolution: We need P(0.2485 &lt; X &lt; 0.2515) where X \\sim N(0.2508, 0.0005^2).\nStandardizing: \n\\small\n\\begin{aligned}\nP(0.2485 &lt; X &lt; 0.2515) &= P\\left(\\frac{0.2485 - 0.2508}{0.0005} &lt; Z &lt; \\frac{0.2515 - 0.2508}{0.0005}\\right) \\\\\n&= P(-4.6 &lt; Z &lt; 1.4) \\\\\n&= P(Z &lt; 1.4) - P(Z &lt; -4.6) \\\\\n&= 0.91924 - 0.00000 = 0.91924\n\\end{aligned}\n\nInterpretation: About 91.9% of shafts meet specifications.\n\n\n\n\n\n\nFigure 13: Normal distribution for shaft diameter example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Shaft Diameter Quality Control:\"\n\n\n       mu sigma lower_spec upper_spec z_lower z_upper prob_meets_specs\n    &lt;num&gt; &lt;num&gt;      &lt;num&gt;      &lt;num&gt;   &lt;num&gt;   &lt;num&gt;            &lt;num&gt;\n1: 0.2508 5e-04     0.2485     0.2515    -4.6     1.4        0.9192412\n   prob_direct\n         &lt;num&gt;\n1:   0.9192412\n\n\n\n\n\n# Example 3-6: Shaft Diameter Quality Control\nshaft_data &lt;- data.table(\n  mu = 0.2508,\n  sigma = 0.0005,\n  lower_spec = 0.2485,\n  upper_spec = 0.2515\n) %&gt;%\n  fmutate(\n    # Standardize the specification limits\n    z_lower = (lower_spec - mu) / sigma,\n    z_upper = (upper_spec - mu) / sigma,\n    # Calculate probability\n    prob_meets_specs = pnorm(z_upper) - pnorm(z_lower),\n    # Direct calculation\n    prob_direct = pnorm(upper_spec, mean = mu, sd = sigma) -\n      pnorm(lower_spec, mean = mu, sd = sigma)\n  )\n\nprint(\"Shaft Diameter Quality Control:\")\nprint(shaft_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLognormal Distribution\n\n\n\nIf W \\sim N(\\theta, \\omega^2), then X = \\exp(W) has a lognormal distribution.\nProbability Density Function: f(x) = \\frac{1}{x\\sqrt{2\\pi\\omega^2}} \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\ln(x) - \\theta}{\\omega}\\right)^2\\right\\}\nfor 0 &lt; x &lt; \\infty.\nParameters:\n\n\\theta: location parameter of underlying normal\n\\omega &gt; 0: scale parameter of underlying normal\n\nMean and Variance:\n\\begin{aligned}\nE(X) &= \\exp\\left(\\theta + \\frac{\\omega^2}{2}\\right) \\\\\nV(X) &= \\exp(2\\theta + \\omega^2)\\left[\\exp(\\omega^2) - 1\\right]\n\\end{aligned}\nApplications: Lifetimes, financial data, environmental measurements\n\n\n\n\n\n\nFigure 14: Lognormal probability density functions with \\theta = 0 for selected values of \\omega^2\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-7: Semiconductor Laser Lifetime\n\n\n\nThe lifetime of a semiconductor laser has a lognormal distribution with \\theta = 10 and \\omega = 1.5 hours.\nProblem 1: What is the probability the lifetime exceeds 10,000 hours?\nSolution: Let W \\sim N(10, 1.5^2), so X = \\exp(W).\n\\begin{aligned}\nP(X &gt; 10,000) &= P(\\exp(W) &gt; 10,000) \\\\\n&= P(W &gt; \\ln(10,000)) \\\\\n&= P\\left(\\frac{W - 10}{1.5} &gt; \\frac{\\ln(10,000) - 10}{1.5}\\right) \\\\\n&= P\\left(Z &gt; \\frac{9.2103 - 10}{1.5}\\right) \\\\\n&= P(Z &gt; -0.5264) = 0.7009\n\\end{aligned}\nProblem 2: What lifetime is exceeded by 99% of lasers?\nSolution: We need x such that P(X &gt; x) = 0.99, or P(X \\leq x) = 0.01.\nFrom P(\\ln(X) \\leq \\ln(x)) = 0.01 and \\ln(X) \\sim N(10, 1.5^2):\nP\\left(\\frac{\\ln(X) - 10}{1.5} \\leq \\frac{\\ln(x) - 10}{1.5}\\right) = 0.01\nSince P(Z \\leq -2.326) = 0.01: \\frac{\\ln(x) - 10}{1.5} = -2.326 x = \\exp(10 - 1.5 \\times 2.326) = \\exp(6.511) = 673.6 \\text{ hours}\n\nR OutputR Code\n\n\n\n\n[1] \"Lognormal Distribution Analysis:\"\n\n\n   theta omega test_value prob_exceeds_10000 percentile_99 theoretical_mean\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;              &lt;num&gt;         &lt;num&gt;            &lt;num&gt;\n1:    10   1.5      10000          0.7007086      672.1478         67846.29\n   theoretical_variance theoretical_sd\n                  &lt;num&gt;          &lt;num&gt;\n1:          39070059887       197661.5\n\n\n\n\n\n# Example 3-7: Semiconductor Laser Lifetime (Lognormal)\nlognormal_data &lt;- data.table(\n  theta = 10,\n  omega = 1.5,\n  test_value = 10000\n) %&gt;%\n  fmutate(\n    # Probability calculations\n    prob_exceeds_10000 = plnorm(test_value, meanlog = theta, sdlog = omega, lower.tail = FALSE),\n    # Find 99th percentile (exceeded by 99%)\n    percentile_99 = qlnorm(0.01, meanlog = theta, sdlog = omega),\n    # Mean and variance\n    theoretical_mean = exp(theta + omega^2 / 2),\n    theoretical_variance = exp(2 * theta + omega^2) * (exp(omega^2) - 1),\n    theoretical_sd = sqrt(theoretical_variance)\n  )\n\nprint(\"Lognormal Distribution Analysis:\")\nprint(lognormal_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteGamma Distribution\n\n\n\nThe random variable X has a gamma distribution with shape parameter r &gt; 0 and rate parameter \\lambda &gt; 0 if:\nProbability Density Function:\nf(x) = \\frac{\\lambda^r x^{r-1} \\exp(-\\lambda x)}{\\Gamma(r)}, \\quad x &gt; 0\nwhere \\Gamma(r) = \\int_0^{\\infty} t^{r-1} e^{-t} dt is the gamma function.\nProperties of Gamma Function:\n\n\\Gamma(r) = (r-1)! for positive integers r\n\\Gamma(r) = (r-1)\\Gamma(r-1) for r &gt; 1\n\\Gamma(1/2) = \\sqrt{\\pi}\n\nMean and Variance: \\begin{aligned}\nE(X) &= \\frac{r}{\\lambda} \\\\\nV(X) &= \\frac{r}{\\lambda^2}\n\\end{aligned}\nSpecial Cases:\n\nr = 1: Exponential distribution\nr = \\nu/2, \\lambda = 1/2: Chi-square distribution with \\nu degrees of freedom\n\n\n\n\n\n\n\nFigure 15: Gamma probability density functions for selected values of \\lambda and r\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWeibull Distribution\n\n\n\nThe Weibull distribution is widely used in reliability engineering and survival analysis.\nProbability Density Function: f(x) = \\frac{\\beta}{\\delta} \\left(\\frac{x}{\\delta}\\right)^{\\beta-1} \\exp\\left\\{-\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\}, \\quad x &gt; 0\nCumulative Distribution Function: F(x) = 1 - \\exp\\left\\{-\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\}\nParameters:\n\n\\delta &gt; 0: scale parameter\n\\beta &gt; 0: shape parameter\n\nMean and Variance: \\begin{aligned}\nE(X) &= \\delta \\Gamma\\left(1 + \\frac{1}{\\beta}\\right) \\\\\nV(X) &= \\delta^2 \\left[\\Gamma\\left(1 + \\frac{2}{\\beta}\\right) - \\left\\{\\Gamma\\left(1 + \\frac{1}{\\beta}\\right)\\right\\}^2\\right]\n\\end{aligned}\nSpecial Cases:\n\n\\beta = 1: Exponential distribution\n\\beta = 2: Rayleigh distribution\n\n\n\n\n\n\n\nFigure 16: Weibull probability density functions for selected values of \\delta and \\beta\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-8: Bearing Failure Time\n\n\n\nThe time to failure (in hours) of a bearing follows a Weibull distribution with \\beta = 0.5 and \\delta = 5000 hours.\nProblem 1: Determine the mean time until failure.\nSolution: \\begin{aligned}\nE(X) &= \\delta \\Gamma\\left(1 + \\frac{1}{\\beta}\\right) \\\\\n&= 5000 \\times \\Gamma(1 + 2) \\\\\n&= 5000 \\times \\Gamma(3) \\\\\n&= 5000 \\times 2! = 10,000 \\text{ hours}\n\\end{aligned}\nProblem 2: Determine the probability that a bearing lasts at least 6000 hours.\nSolution: \\begin{aligned}\nP(X &gt; 6000) &= 1 - F(6000) \\\\\n&= \\exp\\left\\{-\\left(\\frac{6000}{5000}\\right)^{0.5}\\right\\} \\\\\n&= \\exp(-1.2^{0.5}) \\\\\n&= \\exp(-1.095) = 0.334\n\\end{aligned}\nInterpretation: Only 33.4% of bearings last at least 6000 hours.\n\nR OutputR Code\n\n\n\n\n[1] \"Weibull Distribution Analysis:\"\n\n\n   shape scale test_time mean_failure_time prob_lasts_6000 variance       sd\n   &lt;num&gt; &lt;num&gt;     &lt;num&gt;             &lt;num&gt;           &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:   0.5  5000      6000             10000       0.3343907    5e+08 22360.68\n\n\n\n\n\n# Example 3-8: Bearing Failure Time (Weibull)\nweibull_data &lt;- data.table(\n  shape = 0.5,\n  scale = 5000,\n  test_time = 6000\n) %&gt;%\n  fmutate(\n    # Mean time until failure\n    mean_failure_time = scale * gamma(1 + 1 / shape),\n    # Probability of lasting at least 6000 hours\n    prob_lasts_6000 = pweibull(test_time, shape = shape, scale = scale, lower.tail = FALSE),\n    # Variance calculation\n    variance = scale^2 * (gamma(1 + 2 / shape) - (gamma(1 + 1 / shape))^2),\n    sd = sqrt(variance)\n  )\n\nprint(\"Weibull Distribution Analysis:\")\nprint(weibull_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBeta Distribution\n\n\n\nThe beta distribution is defined on the interval [0, 1] and is useful for modeling proportions and percentages.\nProbability Density Function:\nf(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1\nParameters:\n\n\\alpha &gt; 0: shape parameter\n\\beta &gt; 0: shape parameter\n\nMean and Variance:\n\\begin{aligned}\nE(X) &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\nV(X) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{aligned}\nSpecial Cases:\n\n\\alpha = \\beta = 1: Uniform distribution on [0, 1]\n\\alpha = 1, \\beta &gt; 1: Decreasing function\n\\alpha &gt; 1, \\beta = 1: Increasing function\n\\alpha = \\beta &gt; 1: Symmetric and unimodal\n\nApplications: Quality control, project completion times, proportions\n\n\n\n\n\n\nFigure 17: Beta probability density functions for selected values of \\alpha and \\beta\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-9: Project Completion Time\n\n\n\nThe proportion of maximum allowed time to complete a task follows a beta distribution with \\alpha = 2.5 and \\beta = 1.\nProblem 1: What is the probability that the proportion exceeds 0.7?\nSolution: \\begin{aligned}\nP(X &gt; 0.7) &= \\int_{0.7}^{1} \\frac{\\Gamma(2.5 + 1)}{\\Gamma(2.5)\\Gamma(1)} x^{2.5-1} (1-x)^{1-1} dx \\\\\n&= \\int_{0.7}^{1} \\frac{\\Gamma(3.5)}{\\Gamma(2.5)} x^{1.5} dx \\\\\n&= \\frac{2.5}{1} \\int_{0.7}^{1} x^{1.5} dx \\\\\n&= 2.5 \\left[\\frac{x^{2.5}}{2.5}\\right]_{0.7}^{1} \\\\\n&= (1^{2.5} - 0.7^{2.5}) = 1 - 0.7^{2.5} = 0.59\n\\end{aligned}\nProblem 2: Calculate the mean and variance.\nSolution: \\begin{aligned}\nE(X) &= \\frac{\\alpha}{\\alpha + \\beta} = \\frac{2.5}{2.5 + 1} = \\frac{2.5}{3.5} = 0.714 \\\\\nV(X) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\\\\n&= \\frac{2.5 \\times 1}{(3.5)^2 \\times 4.5} = \\frac{2.5}{55.125} = 0.045\n\\end{aligned}\n\nR OutputR Code\n\n\n\n\n[1] \"Beta Distribution Analysis:\"\n\n\n   alpha beta_param test_value prob_exceeds_07 mean_beta variance_beta\n   &lt;num&gt;      &lt;num&gt;      &lt;num&gt;           &lt;num&gt;     &lt;num&gt;         &lt;num&gt;\n1:   2.5          1        0.7       0.5900366 0.7142857    0.04535147\n     sd_beta\n       &lt;num&gt;\n1: 0.2129589\n\n\n\n\n\n# Example 3-9: Project Completion Time (Beta)\nbeta_data &lt;- data.table(\n  alpha = 2.5,\n  beta_param = 1,\n  test_value = 0.7\n) %&gt;%\n  fmutate(\n    # Probability exceeds 0.7\n    prob_exceeds_07 = pbeta(test_value, shape1 = alpha, shape2 = beta_param, lower.tail = FALSE),\n    # Mean and variance\n    mean_beta = alpha / (alpha + beta_param),\n    variance_beta = (alpha * beta_param) / ((alpha + beta_param)^2 * (alpha + beta_param + 1)),\n    sd_beta = sqrt(variance_beta)\n  )\n\nprint(\"Beta Distribution Analysis:\")\nprint(beta_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNormal Probability Plots\n\n\n\nA normal probability plot is a graphical method for determining whether sample data conform to a hypothesized normal distribution. The procedure is:\n\nOrder the sample data from smallest to largest: x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\nPlot the pairs (x_{(i)}, \\Phi^{-1}((i-0.5)/n)) for i = 1, 2, \\ldots, n\nIf the data are from a normal distribution, the plotted points will approximately follow a straight line\n\nInterpretation:\n\nStraight line pattern: Data consistent with normal distribution\nCurved pattern: Data may follow a different distribution\nOutliers: Points that deviate significantly from the line\n\nAdvantages:\n\nVisual assessment of normality\nIdentifies outliers\nSuggests alternative distributions\n\n\n\n\n\n\n\n\n\nTipExample 3-10: Battery Life Normal Probability Plot\n\n\n\nConsider battery life data (in hours): 176, 191, 214, 220, 205, 192, 201, 190, 183, 185.\nThe normal probability plot helps assess whether these data could reasonably come from a normal distribution.\n\nR OutputR Code\n\n\n\n\n[1] \"Battery Life Data for Normal Probability Plot:\"\n\n\n     life sample_mean sample_sd sample_size  rank theoretical_quantile\n    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;       &lt;int&gt; &lt;num&gt;                &lt;num&gt;\n 1:   176       195.7  14.03211          10     1           -1.6448536\n 2:   183       195.7  14.03211          10     2           -1.0364334\n 3:   185       195.7  14.03211          10     3           -0.6744898\n 4:   190       195.7  14.03211          10     4           -0.3853205\n 5:   191       195.7  14.03211          10     5           -0.1256613\n 6:   192       195.7  14.03211          10     6            0.1256613\n 7:   201       195.7  14.03211          10     7            0.3853205\n 8:   205       195.7  14.03211          10     8            0.6744898\n 9:   214       195.7  14.03211          10     9            1.0364334\n10:   220       195.7  14.03211          10    10            1.6448536\n    fitted_value\n           &lt;num&gt;\n 1:     172.6192\n 2:     181.1567\n 3:     186.2355\n 4:     190.2931\n 5:     193.9367\n 6:     197.4633\n 7:     201.1069\n 8:     205.1645\n 9:     210.2433\n10:     218.7808\n\n\n[1] \"Shapiro-Wilk p-value: 0.7173\"\n\n\n\n\n\n# Example 3-10: Normal Probability Plot\nbattery_life &lt;- data.table(\n  life = c(176, 191, 214, 220, 205, 192, 201, 190, 183, 185)\n) %&gt;%\n  fmutate(\n    sample_mean = fmean(life),\n    sample_sd = fsd(life),\n    sample_size = fnobs(life)\n  )\n\n# Create normal probability plot data\nbattery_sorted &lt;- battery_life %&gt;%\n  fmutate(rank = frank(life, ties.method = \"average\")) %&gt;%\n  fmutate(\n    theoretical_quantile = qnorm((rank - 0.5) / sample_size),\n    fitted_value = sample_mean + sample_sd * theoretical_quantile\n  ) %&gt;%\n  roworder(life)\n\nprint(\"Battery Life Data for Normal Probability Plot:\")\nprint(battery_sorted)\n\n# Test for normality using Shapiro-Wilk\nshapiro_test &lt;- shapiro.test(battery_life$life)\nprint(paste(\"Shapiro-Wilk p-value:\", round(shapiro_test$p.value, 4)))\n\nfwrite(battery_sorted, \"data/battery_normal_plot.csv\")\n\n\n\n\nInterpretation: If the points roughly follow a straight line, the normal distribution is a reasonable model for the data.\n\n\n\n\n\n\n\n\n\n\n\nNoteOther Probability Plots\n\n\n\nThe same probability plotting technique can be used with any distribution:\nLognormal Probability Plot:\n\nPlot (x_{(i)}, \\Phi^{-1}((i-0.5)/n)) where the x-axis uses a log scale\nOr plot (\\ln(x_{(i)}), \\Phi^{-1}((i-0.5)/n)) on normal scales\n\nWeibull Probability Plot:\n\nPlot (\\ln(x_{(i)}), \\ln(-\\ln(1-(i-0.5)/n)))\nStraight line indicates Weibull distribution\n\nExponential Probability Plot:\n\nPlot (x_{(i)}, -\\ln(1-(i-0.5)/n))\nStraight line indicates exponential distribution\n\nGeneral Approach:\n\nTransform data using the inverse CDF of the hypothesized distribution\nPlot against order statistics\nAssess linearity\n\n\n\n\n\n\n\n\n\nTipExample 3-11: Lognormal Probability Plot\n\n\n\nConsider failure time data that might follow a lognormal distribution. The lognormal probability plot helps determine if this distribution is appropriate.\n\nR OutputR Code\n\n\n\n\n[1] \"Lognormal Distribution Fit:\"\n\n\n    time  ln_time sample_size meanlog_est sdlog_est\n   &lt;num&gt;    &lt;num&gt;       &lt;int&gt;       &lt;num&gt;     &lt;num&gt;\n1:    81 4.394449          50    4.966551 0.4756993\n\n\n[1] \"KS test p-value for lognormal: 0.8849\"\n\n\n\n\n\n# Example 3-11: Lognormal Probability Plot\n# Simulate some failure time data that might be lognormal\nset.seed(42)\nfailure_times &lt;- data.table(\n  time = c(\n    81, 249, 117, 227, 134, 98, 135, 149, 225, 59,\n    291, 223, 127, 185, 181, 101, 205, 115, 240, 151,\n    98, 80, 198, 161, 240, 118, 177, 342, 197, 146,\n    158, 82, 83, 98, 104, 197, 64, 34, 65, 100,\n    139, 137, 342, 144, 215, 249, 149, 185, 151, 200\n  )\n) %&gt;%\n  fmutate(\n    ln_time = log(time),\n    sample_size = fnobs(time)\n  )\n\n# Fit lognormal distribution\nlognormal_fit &lt;- failure_times %&gt;%\n  fmutate(\n    meanlog_est = fmean(ln_time),\n    sdlog_est = fsd(ln_time)\n  )\n\nprint(\"Lognormal Distribution Fit:\")\nprint(lognormal_fit[1])\n\n# Test goodness of fit\nks_test_lognormal &lt;- ks.test(failure_times$time, \"plnorm\",\n  meanlog = lognormal_fit$meanlog_est[1],\n  sdlog = lognormal_fit$sdlog_est[1]\n)\nprint(paste(\"KS test p-value for lognormal:\", round(ks_test_lognormal$p.value, 4)))\n\nfwrite(failure_times, \"data/failure_times.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteDiscrete Random Variables\n\n\n\nA discrete random variable can take on only a finite or countably infinite number of distinct values. Common examples include:\n\nNumber of defective items in a batch\nNumber of customers arriving per hour\nNumber of successful trials in a sequence\n\nKey Characteristics:\n\nPossible values are usually integers\nProbability is concentrated at specific points\nBetween any two possible values, the probability is zero\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Mass Function (PMF)\n\n\n\nFor a discrete random variable X with possible values x_1, x_2, \\ldots, x_n, the probability mass function is:\nf(x_i) = P(X = x_i)\nProperties:\n\nf(x_i) \\geq 0 for all x_i\n\\sum_{i=1}^{n} f(x_i) = 1\nf(x) = 0 if x is not a possible value\n\nNotation: Sometimes written as p(x) or P(X = x)\n\n\n\n\n\n\n\n\nTipExample 3-12: Digital Transmission Errors\n\n\n\nLet X equal the number of bits in error in the next 4 bits transmitted. Suppose the probabilities are:\n\nP(X = 0) = 0.6561\nP(X = 1) = 0.2916\n\nP(X = 2) = 0.0486\nP(X = 3) = 0.0036\nP(X = 4) = 0.0001\n\nThis completely specifies the probability distribution of X.\n\n\n\n\n\n\nFigure 18: Probability mass function for transmission errors\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Transmission Error Distribution:\"\n\n\n   errors probability total_prob cumulative_prob\n    &lt;int&gt;       &lt;num&gt;      &lt;num&gt;           &lt;num&gt;\n1:      0      0.6561          1          0.6561\n2:      1      0.2916          1          0.9477\n3:      2      0.0486          1          0.9963\n4:      3      0.0036          1          0.9999\n5:      4      0.0001          1          1.0000\n\n\n\n\n\n# Example 3-12: Digital Transmission Errors (PMF)\ntransmission_data &lt;- data.table(\n  errors = 0:4,\n  probability = c(0.6561, 0.2916, 0.0486, 0.0036, 0.0001)\n) %&gt;%\n  fmutate(\n    # Verify probabilities sum to 1\n    total_prob = fsum(probability),\n    # Calculate cumulative probabilities\n    cumulative_prob = cumsum(probability)\n  )\n\nprint(\"Transmission Error Distribution:\")\nprint(transmission_data)\n\nfwrite(transmission_data, \"data/transmission_errors.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function for Discrete Variables\n\n\n\nThe cumulative distribution function of a discrete random variable X is:\nF(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f(x_i)\nProperties:\n\nF(x) is a step function\nJumps occur at possible values of X\nJump size equals P(X = x_i)\nF(x) is non-decreasing\n\\lim_{x \\to -\\infty} F(x) = 0 and \\lim_{x \\to \\infty} F(x) = 1\n\nKey Relationship: P(a &lt; X \\leq b) = F(b) - F(a)\n\n\n\n\n\n\n\n\nTipExample 3-13: CDF for Transmission Errors\n\n\n\nContinuing the previous example:\n\nF(0) = P(X \\leq 0) = 0.6561\nF(1) = P(X \\leq 1) = 0.6561 + 0.2916 = 0.9477\nF(2) = P(X \\leq 2) = 0.9477 + 0.0486 = 0.9963\nF(3) = P(X \\leq 3) = 0.9963 + 0.0036 = 0.9999\nF(4) = P(X \\leq 4) = 0.9999 + 0.0001 = 1.0000\n\nNote: F(1.5) = P(X \\leq 1.5) = P(X \\leq 1) = 0.9477 since X cannot equal 1.5.\n\n\n\n\n\n\nFigure 19: Cumulative distribution function for transmission errors\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"CDF for Transmission Errors:\"\n\n\n   errors probability cumulative_prob     prob_statement\n    &lt;int&gt;       &lt;num&gt;           &lt;num&gt;             &lt;char&gt;\n1:      0      0.6561          0.6561 P(X &lt;= 0) = 0.6561\n2:      1      0.2916          0.9477 P(X &lt;= 1) = 0.9477\n3:      2      0.0486          0.9963 P(X &lt;= 2) = 0.9963\n4:      3      0.0036          0.9999 P(X &lt;= 3) = 0.9999\n5:      4      0.0001          1.0000      P(X &lt;= 4) = 1\n\n\n[1] \"CDF at Non-Integer Values:\"\n\n\n       x cdf_value  interpretation\n   &lt;num&gt;     &lt;num&gt;          &lt;char&gt;\n1:   1.5    0.9477 F(1.5) = 0.9477\n2:   2.3    0.9963 F(2.3) = 0.9963\n3:   3.7    0.9999 F(3.7) = 0.9999\n\n\n\n\n\n# Example 3-13: CDF for Transmission Errors\ncdf_transmission &lt;- transmission_data %&gt;%\n  fselect(errors, probability, cumulative_prob) %&gt;%\n  fmutate(\n    prob_statement = paste0(\"P(X &lt;= \", errors, \") = \", round(cumulative_prob, 4))\n  )\n\nprint(\"CDF for Transmission Errors:\")\nprint(cdf_transmission)\n\n# Example of intermediate values\nintermediate_vals &lt;- data.table(\n  x = c(1.5, 2.3, 3.7),\n  cdf_value = c(0.9477, 0.9963, 0.9999)\n) %&gt;%\n  fmutate(\n    interpretation = paste0(\"F(\", x, \") = \", cdf_value)\n  )\n\nprint(\"CDF at Non-Integer Values:\")\nprint(intermediate_vals)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMean and Variance for Discrete Random Variables\n\n\n\nLet X have possible values x_1, x_2, \\ldots, x_n with PMF f(x).\nMean (Expected Value):\n\n\\mu = E(X) = \\sum_{i=1}^{n} x_i f(x_i)\n\nVariance:\n\n\\begin{aligned}\n\\sigma^2 = V(X) &= E[(X - \\mu)^2] \\\\\n&= \\sum_{i=1}^{n} (x_i - \\mu)^2 f(x_i) \\\\\n&= \\sum_{i=1}^{n} x_i^2 f(x_i) - \\mu^2\n\\end{aligned}\n\nStandard Deviation: \\sigma = \\sqrt{V(X)}\nInterpretation:\n\nMean is the weighted average of possible values\nVariance measures spread around the mean\nStandard deviation is in original units\n\n\n\n\n\n\n\n\n\nTipExample 3-14: Mean and Variance Calculation\n\n\n\nFor the transmission error example:\nMean:\n\\begin{aligned}\n\\mu &= E(X) = 0(0.6561) + 1(0.2916) + 2(0.0486) + 3(0.0036) + 4(0.0001) \\\\\n&= 0 + 0.2916 + 0.0972 + 0.0108 + 0.0004 = 0.4\n\\end{aligned}\nVariance:\nFirst, calculate E(X^2):\nE(X^2) = 0^2(0.6561) + 1^2(0.2916) + 2^2(0.0486) + 3^2(0.0036) + 4^2(0.0001) = 0.52\nThen: V(X) = E(X^2) - [E(X)]^2 = 0.52 - (0.4)^2 = 0.52 - 0.16 = 0.36\nStandard Deviation: \\sigma = \\sqrt{0.36} = 0.6\n\nR OutputR Code\n\n\n\n\nError in eval(e, .data, pe): object 'e_x_squared' not found\n\n\n[1] \"Mean and Variance for Transmission Errors:\"\n\n\nError: object 'discrete_stats' not found\n\n\nError in eval(ei, .data, pe): object 'discrete_stats' not found\n\n\n[1] \"Detailed Variance Calculation:\"\n\n\nError: object 'calc_table' not found\n\n\n\n\n\n# Example 3-14: Mean and Variance Calculation for Discrete Distribution\ndiscrete_stats &lt;- transmission_data %&gt;%\n  fmutate(\n    x_times_p = errors * probability,\n    x_squared_times_p = errors^2 * probability\n  ) %&gt;%\n  fsummarise(\n    mean_x = fsum(x_times_p),\n    e_x_squared = fsum(x_squared_times_p),\n    variance_x = e_x_squared - mean_x^2,\n    sd_x = sqrt(variance_x)\n  )\n\nprint(\"Mean and Variance for Transmission Errors:\")\nprint(discrete_stats)\n\n# Create detailed calculation table\ncalc_table &lt;- transmission_data %&gt;%\n  fmutate(\n    x_times_p = errors * probability,\n    x_minus_mu = errors - discrete_stats$mean_x[1],\n    x_minus_mu_squared = x_minus_mu^2,\n    variance_term = x_minus_mu_squared * probability\n  )\n\nprint(\"Detailed Variance Calculation:\")\nprint(calc_table)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-15: Design Choice Problem\n\n\n\nTwo product designs are compared based on revenue potential:\n\nDesign A: Revenue of $3 million with probability 1 (certain)\nDesign B: Revenue of $7 million with probability 0.3, or $2 million with probability 0.7\n\nAnalysis:\nDesign A: E(X_A) = 3 million dollars, V(X_A) = 0\nDesign B: \n\\begin{aligned}\nE(X_B) &= 7(0.3) + 2(0.7) = 2.1 + 1.4 = 3.5 \\text{ million} \\\\\nV(X_B) &= (7-3.5)^2(0.3) + (2-3.5)^2(0.7) \\\\\n&= (3.5)^2(0.3) + (-1.5)^2(0.7) \\\\\n&= 3.675 + 1.575 = 5.25 \\text{ million}^2\n\\end{aligned}\n\nDecision: Design B has higher expected revenue but much higher risk (variance).\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBinomial Distribution\n\n\n\nA binomial experiment consists of n independent trials, each with:\n\nExactly two possible outcomes (success/failure)\nConstant probability p of success on each trial\nTrials are independent\n\nDefinition: The random variable X that counts the number of successes in n binomial trials has a binomial distribution with parameters n and p.\nProbability Mass Function:\n\nf(x) = P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}\n\nfor x = 0, 1, 2, \\ldots, n, where \\binom{n}{x} = \\frac{n!}{x!(n-x)!}.\nParameters:\n\nn: number of trials\np: probability of success on each trial\n\nMean and Variance:\n\n\\begin{aligned}\nE(X) &= np \\\\\nV(X) &= np(1-p)\n\\end{aligned}\n\nNotation: X \\sim \\text{Binomial}(n, p) or X \\sim B(n, p)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 20: Binomial distributions for selected values of n and p\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-16: Water Sample Analysis\n\n\n\nEach water sample has a 10% chance of containing high levels of organic solids. Assume samples are independent. For the next 18 samples:\nProblem 1: Probability that exactly 2 contain high solids?\nSolution:\nX \\sim B(18, 0.1)\nP(X = 2) = \\binom{18}{2} (0.1)^2 (0.9)^{16} = 153 \\times 0.01 \\times 0.1853 = 0.284\nProblem 2: Probability that at least 4 samples contain high solids?\nSolution:\n\n\\begin{aligned}\nP(X \\geq 4) &= 1 - P(X \\leq 3) \\\\\n&= 1 - \\sum_{x=0}^{3} \\binom{18}{x} (0.1)^x (0.9)^{18-x} \\\\\n&= 1 - [0.150 + 0.300 + 0.284 + 0.168] = 0.098\n\\end{aligned}\n\nProblem 3: Mean and variance?\nSolution:\n\n\\begin{aligned}\nE(X) &= np = 18 \\times 0.1 = 1.8 \\\\\nV(X) &= np(1-p) = 18 \\times 0.1 \\times 0.9 = 1.62\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Water Sample Binomial Analysis:\"\n\n\n       n     p test_values prob_exactly_2 prob_at_least_4 prob_at_least_4_alt\n   &lt;num&gt; &lt;num&gt;      &lt;list&gt;          &lt;num&gt;           &lt;num&gt;               &lt;num&gt;\n1:    18   0.1         2,4      0.2835121      0.09819684          0.09819684\n   prob_3_to_7 mean_binomial variance_binomial sd_binomial\n         &lt;num&gt;         &lt;num&gt;             &lt;num&gt;       &lt;num&gt;\n1:   0.2660305           1.8              1.62    1.272792\n\n\n[1] \"Binomial Probability Table (first 9 values):\"\n\n\n       x  probability cumulative\n   &lt;int&gt;        &lt;num&gt;      &lt;num&gt;\n1:     0 0.1500946353  0.1500946\n2:     1 0.3001892706  0.4502839\n3:     2 0.2835120889  0.7337960\n4:     3 0.1680071638  0.9018032\n5:     4 0.0700029849  0.9718061\n6:     5 0.0217787064  0.9935848\n7:     6 0.0052430219  0.9988279\n8:     7 0.0009986708  0.9998265\n9:     8 0.0001525747  0.9999791\n\n\n\n\n\n# Example 3-16: Water Sample Analysis (Binomial)\nwater_sample_data &lt;- data.table(\n  n = 18,\n  p = 0.1,\n  test_values = list(c(2, 4))\n) %&gt;%\n  fmutate(\n    # Problem 1: Exactly 2 samples\n    prob_exactly_2 = dbinom(2, size = n, prob = p),\n    # Problem 2: At least 4 samples\n    prob_at_least_4 = 1 - pbinom(3, size = n, prob = p),\n    prob_at_least_4_alt = pbinom(3, size = n, prob = p, lower.tail = FALSE),\n    # Problem 3: Between 3 and 7 (inclusive)\n    prob_3_to_7 = pbinom(7, size = n, prob = p) - pbinom(2, size = n, prob = p),\n    # Mean and variance\n    mean_binomial = n * p,\n    variance_binomial = n * p * (1 - p),\n    sd_binomial = sqrt(variance_binomial)\n  )\n\nprint(\"Water Sample Binomial Analysis:\")\nprint(water_sample_data)\n\n# Create binomial probability table\nbinomial_table &lt;- data.table(\n  x = 0:8\n) %&gt;%\n  fmutate(\n    probability = dbinom(x, size = 18, prob = 0.1),\n    cumulative = pbinom(x, size = 18, prob = 0.1)\n  )\n\nprint(\"Binomial Probability Table (first 9 values):\")\nprint(binomial_table)\n\nfwrite(binomial_table, \"data/binomial_probabilities.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePoisson Process\n\n\n\nA Poisson process models events occurring randomly over time or space. Examples include:\n\nEmail arrivals at a server\nRadioactive decay\nManufacturing defects\nCustomer arrivals\n\nDefinition: A Poisson process with rate \\lambda has these properties:\n\nIndependence: Events in disjoint intervals are independent\nStationarity: Rate is constant over time\nOrdinarity: At most one event occurs in any infinitesimal interval\n\nApplications:\n\nReliability engineering\nQueuing theory\n\nQuality control\nNetwork traffic modeling\n\n\n\n\n\n\n\nFigure 21: Events occurring randomly in a Poisson process\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePoisson Distribution\n\n\n\nIf events occur according to a Poisson process with rate \\lambda, then the number of events X in a unit interval has a Poisson distribution.\nProbability Mass Function:\n\nf(x) = P(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\nfor x = 0, 1, 2, \\ldots\nParameter:\n\n\\lambda &gt; 0: average number of events per unit interval\n\nMean and Variance:\n\nE(X) = V(X) = \\lambda\n\nKey Property: Mean equals variance (equidispersion)\nNotation: X \\sim \\text{Poisson}(\\lambda)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 22: Poisson distributions for selected values of \\lambda\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-17: Copper Wire Flaws\n\n\n\nFlaws in copper wire follow a Poisson distribution with mean 2.3 flaws per millimeter.\nProblem 1: Probability of exactly 2 flaws in 1 millimeter?\nSolution: X \\sim \\text{Poisson}(2.3), P(X = 2) = \\frac{e^{-2.3} \\times 2.3^2}{2!} = \\frac{e^{-2.3} \\times 5.29}{2} = 0.265\nProblem 2: Probability of 10 flaws in 5 millimeters?\nSolution: For 5 mm, \\lambda = 5 \\times 2.3 = 11.5 P(X = 10) = \\frac{e^{-11.5} \\times 11.5^{10}}{10!} = 0.113\n\nR OutputR Code\n\n\n\n\n[1] \"Poisson Distribution Analysis:\"\n\n\n   lambda_per_mm test_scenarios prob_2_in_1mm lambda_5mm prob_10_in_5mm\n           &lt;num&gt;         &lt;list&gt;         &lt;num&gt;      &lt;num&gt;          &lt;num&gt;\n1:           2.3            1,5     0.2651846       11.5      0.1129351\n   mean_1mm variance_1mm mean_5mm variance_5mm\n      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1:      2.3          2.3     11.5         11.5\n\n\n[1] \"Poisson Table for 1mm (lambda = 2.3):\"\n\n\n       x probability cumulative\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;\n1:     0  0.10025884  0.1002588\n2:     1  0.23059534  0.3308542\n3:     2  0.26518464  0.5960388\n4:     3  0.20330823  0.7993471\n5:     4  0.11690223  0.9162493\n6:     5  0.05377503  0.9700243\n\n\n\n\n\n# Example 3-17: Copper Wire Flaws (Poisson)\npoisson_data &lt;- data.table(\n  lambda_per_mm = 2.3,\n  test_scenarios = list(c(1, 5)) # 1 mm and 5 mm\n) %&gt;%\n  fmutate(\n    # Problem 1: Exactly 2 flaws in 1 mm\n    prob_2_in_1mm = dpois(2, lambda = lambda_per_mm),\n    # Problem 2: 10 flaws in 5 mm\n    lambda_5mm = lambda_per_mm * 5,\n    prob_10_in_5mm = dpois(10, lambda = lambda_5mm),\n    # Additional calculations\n    mean_1mm = lambda_per_mm,\n    variance_1mm = lambda_per_mm,\n    mean_5mm = lambda_5mm,\n    variance_5mm = lambda_5mm\n  )\n\nprint(\"Poisson Distribution Analysis:\")\nprint(poisson_data)\n\n# Create Poisson probability tables\npoisson_1mm &lt;- data.table(x = 0:10) %&gt;%\n  fmutate(\n    probability = dpois(x, lambda = 2.3),\n    cumulative = ppois(x, lambda = 2.3)\n  )\n\npoisson_5mm &lt;- data.table(x = 0:20) %&gt;%\n  fmutate(\n    probability = dpois(x, lambda = 11.5),\n    cumulative = ppois(x, lambda = 11.5)\n  )\n\nprint(\"Poisson Table for 1mm (lambda = 2.3):\")\nprint(head(poisson_1mm))\n\nfwrite(poisson_1mm, \"data/poisson_1mm.csv\")\nfwrite(poisson_5mm, \"data/poisson_5mm.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExponential Distribution\n\n\n\nThe exponential distribution models the time between events in a Poisson process.\nProbability Density Function:\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\nCumulative Distribution Function:\nF(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0\nParameter:\n\n\\lambda &gt; 0: rate parameter (events per unit time)\n\nMean and Variance:\n\n\\begin{aligned}\nE(X) &= \\frac{1}{\\lambda} \\\\\nV(X) &= \\frac{1}{\\lambda^2}\n\\end{aligned}\n\nMemoryless Property: P(X &gt; s + t | X &gt; s) = P(X &gt; t)\nApplications: Reliability analysis, queuing systems, survival analysis\n\n\n\n\n\n\nFigure 23: Exponential probability density functions for selected values of \\lambda\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-18: Computer Network Log-ons\n\n\n\nUser log-ons to a system follow a Poisson process with mean 25 log-ons per hour.\nProblem: Probability of no log-ons in a 6-minute interval?\nSolution: Let X = time until first log-on (in hours). Then X \\sim \\text{Exponential}(25).\nWe want P(X &gt; 0.1) since 6 minutes = 0.1 hour.\nP(X &gt; 0.1) = e^{-25 \\times 0.1} = e^{-2.5} = 0.082\nAlternative approach: Number of events in 0.1 hour ~ Poisson(2.5) P(\\text{0 events}) = \\frac{e^{-2.5} \\times 2.5^0}{0!} = e^{-2.5} = 0.082\n\nR OutputR Code\n\n\n\n\n[1] \"Network Log-on Analysis:\"\n\n\n   lambda_per_hour time_interval_minutes time_interval_hours prob_no_logons\n             &lt;num&gt;                 &lt;num&gt;               &lt;num&gt;          &lt;num&gt;\n1:              25                     6                 0.1       0.082085\n   prob_no_logons_alt expected_events prob_0_events_poisson mean_time_between\n                &lt;num&gt;           &lt;num&gt;                 &lt;num&gt;             &lt;num&gt;\n1:           0.082085             2.5              0.082085              0.04\n   mean_time_minutes\n               &lt;num&gt;\n1:               2.4\n\n\n[1] \"Exponential Probabilities for Various Times:\"\n\n\n   time_hours  prob_exceed prob_within time_minutes\n        &lt;num&gt;        &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:        0.1 8.208500e-02   0.9179150            6\n2:        0.2 6.737947e-03   0.9932621           12\n3:        0.5 3.726653e-06   0.9999963           30\n4:        1.0 1.388794e-11   1.0000000           60\n\n\n\n\n\n# Example 3-18: Computer Network Log-ons (Exponential)\nnetwork_data &lt;- data.table(\n  lambda_per_hour = 25,\n  time_interval_minutes = 6\n) %&gt;%\n  fmutate(\n    time_interval_hours = time_interval_minutes / 60,\n    # Probability of no log-ons in 6 minutes\n    prob_no_logons = pexp(time_interval_hours, rate = lambda_per_hour, lower.tail = FALSE),\n    prob_no_logons_alt = exp(-lambda_per_hour * time_interval_hours),\n    # Verification using Poisson (events in interval)\n    expected_events = lambda_per_hour * time_interval_hours,\n    prob_0_events_poisson = dpois(0, lambda = expected_events),\n    # Mean time between log-ons\n    mean_time_between = 1 / lambda_per_hour,\n    # Convert to minutes\n    mean_time_minutes = mean_time_between * 60\n  )\n\nprint(\"Network Log-on Analysis:\")\nprint(network_data)\n\n# Additional exponential calculations\nexp_calculations &lt;- data.table(\n  time_hours = c(0.1, 0.2, 0.5, 1.0)\n) %&gt;%\n  fmutate(\n    prob_exceed = pexp(time_hours, rate = 25, lower.tail = FALSE),\n    prob_within = pexp(time_hours, rate = 25),\n    time_minutes = time_hours * 60\n  )\n\nprint(\"Exponential Probabilities for Various Times:\")\nprint(exp_calculations)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNormal Approximation to Binomial\n\n\n\nWhen n is large, the binomial distribution approaches normality due to the Central Limit Theorem.\nApproximation: If X \\sim B(n, p), then for large n: Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\approx N(0, 1)\nRule of Thumb: Use when np &gt; 5 and n(1-p) &gt; 5\nContinuity Correction: Since we’re approximating discrete with continuous:\n\nP(X = k) \\approx P(k - 0.5 &lt; Y &lt; k + 0.5)\nP(X \\leq k) \\approx P(Y \\leq k + 0.5)\nP(X \\geq k) \\approx P(Y \\geq k - 0.5)\n\nwhere Y \\sim N(np, np(1-p)).\n\n\n\n\n\n\nFigure 24: Normal approximation to binomial distribution\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-19: Normal Approximation to Binomial\n\n\n\nFor n = 50 bits transmitted with error probability p = 0.1:\nExact calculation: P(X \\leq 2) = \\sum_{x=0}^{2} \\binom{50}{x} (0.1)^x (0.9)^{50-x} = 0.1117\nNormal approximation: X \\sim B(50, 0.1) approximately N(5, 4.5)\nWith continuity correction:\n\n\\begin{aligned}\nP(X \\leq 2) &\\approx P\\left(Z \\leq \\frac{2.5 - 5}{\\sqrt{4.5}}\\right) \\\\\n&= P(Z \\leq -1.18) = 0.119\n\\end{aligned}\n\nComparison: Exact = 0.1117, Approximation = 0.119 (very close!)\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Approximation to Binomial:\"\n\n\n       n     p test_value    np    nq conditions_met exact_prob mu_approx\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt; &lt;num&gt; &lt;num&gt;         &lt;lgcl&gt;      &lt;num&gt;     &lt;num&gt;\n1:    50   0.1          2     5    45          FALSE  0.1117288         5\n   sigma_approx z_score_no_cc approx_no_cc z_score_with_cc approx_with_cc\n          &lt;num&gt;         &lt;num&gt;        &lt;num&gt;           &lt;num&gt;          &lt;num&gt;\n1:      2.12132     -1.414214    0.0786496       -1.178511      0.1192964\n   error_no_cc error_with_cc\n         &lt;num&gt;         &lt;num&gt;\n1:  0.03307915   0.007567658\n\n\n[1] \"Comparison Table (first 9 values):\"\n\n\n       x       exact  approx_cc       error\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     0 0.005153775 0.01694743 0.011793652\n2:     1 0.033785860 0.04948008 0.015694217\n3:     2 0.111728756 0.11929641 0.007567658\n4:     3 0.250293906 0.23975006 0.010543845\n5:     4 0.431198407 0.40683186 0.024366549\n6:     5 0.616123008 0.59316814 0.022954866\n7:     6 0.770226842 0.76024994 0.009976903\n8:     7 0.877854916 0.88070359 0.002848669\n9:     8 0.942132794 0.95051992 0.008387129\n\n\n\n\n\n# Example 3-19: Normal Approximation to Binomial\nnormal_approx_data &lt;- data.table(\n  n = 50,\n  p = 0.1,\n  test_value = 2\n) %&gt;%\n  fmutate(\n    # Check conditions for normal approximation\n    np = n * p,\n    nq = n * (1 - p),\n    conditions_met = (np &gt; 5) & (nq &gt; 5),\n    # Exact binomial calculation\n    exact_prob = pbinom(test_value, size = n, prob = p),\n    # Normal approximation parameters\n    mu_approx = np,\n    sigma_approx = sqrt(np * (1 - p)),\n    # Normal approximation without continuity correction\n    z_score_no_cc = (test_value - mu_approx) / sigma_approx,\n    approx_no_cc = pnorm(z_score_no_cc),\n    # Normal approximation with continuity correction\n    z_score_with_cc = (test_value + 0.5 - mu_approx) / sigma_approx,\n    approx_with_cc = pnorm(z_score_with_cc),\n    # Error calculations\n    error_no_cc = abs(exact_prob - approx_no_cc),\n    error_with_cc = abs(exact_prob - approx_with_cc)\n  )\n\nprint(\"Normal Approximation to Binomial:\")\nprint(normal_approx_data)\n\n# Compare multiple values\ncomparison_table &lt;- data.table(x = 0:8) %&gt;%\n  fmutate(\n    exact = pbinom(x, size = 50, prob = 0.1),\n    approx_cc = pnorm((x + 0.5 - 5) / sqrt(4.5)),\n    error = abs(exact - approx_cc)\n  )\n\nprint(\"Comparison Table (first 9 values):\")\nprint(comparison_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNormal Approximation to Poisson\n\n\n\nFor large \\lambda, the Poisson distribution approaches normality.\nApproximation: If X \\sim \\text{Poisson}(\\lambda), then for large \\lambda: Z = \\frac{X - \\lambda}{\\sqrt{\\lambda}} \\approx N(0, 1)\nRule of Thumb: Use when \\lambda &gt; 5\nContinuity Correction: Apply the same corrections as for binomial approximation.\n\n\n\n\n\n\n\n\nTipExample 3-20: Normal Approximation to Poisson\n\n\n\nContamination particles in water follow Poisson(1000). Find P(X &lt; 950).\nExact: P(X &lt; 950) = \\sum_{x=0}^{949} \\frac{e^{-1000} \\times 1000^x}{x!} (computationally intensive)\nNormal approximation: X \\sim \\text{Poisson}(1000) approximately N(1000, 1000)\nWith continuity correction:\n\n\\begin{aligned}\nP(X &lt; 950) &= P(X \\leq 949) \\\\\n&\\approx P\\left(Z \\leq \\frac{949.5 - 1000}{\\sqrt{1000}}\\right) \\\\\n&= P(Z \\leq -1.60) = 0.055\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Approximation to Poisson:\"\n\n\n   lambda test_value condition_met exact_prob_approx mu_approx sigma_approx\n    &lt;num&gt;      &lt;num&gt;        &lt;lgcl&gt;             &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:   1000        950          TRUE        0.05420667      1000     31.62278\n    z_score normal_approx approx_error\n      &lt;num&gt;         &lt;num&gt;        &lt;num&gt;\n1: -1.59695     0.0551384 0.0009317283\n\n\n[1] \"Poisson Approximation Examples:\"\n\n\n   lambda test_x     exact   z_score    approx       error\n    &lt;num&gt;  &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;       &lt;num&gt;\n1:     10     15 0.9512596 1.7392527 0.9590048 0.007745243\n2:     25     30 0.8633089 1.1000000 0.8643339 0.001025070\n3:     50     55 0.7844704 0.7778175 0.7816617 0.002808718\n4:    100    105 0.7128079 0.5500000 0.7088403 0.003967569\n\n\n\n\n\n# Example 3-20: Normal Approximation to Poisson\npoisson_approx_data &lt;- data.table(\n  lambda = 1000,\n  test_value = 950\n) %&gt;%\n  fmutate(\n    # Check condition for normal approximation\n    condition_met = lambda &gt; 5,\n    # Exact Poisson (approximated due to computational limits)\n    exact_prob_approx = ppois(test_value - 1, lambda = lambda), # P(X &lt; 950) = P(X &lt;= 949)\n    # Normal approximation parameters\n    mu_approx = lambda,\n    sigma_approx = sqrt(lambda),\n    # Normal approximation with continuity correction\n    z_score = (test_value - 0.5 - mu_approx) / sigma_approx,\n    normal_approx = pnorm(z_score),\n    # Error (approximate since exact is computationally intensive)\n    approx_error = abs(exact_prob_approx - normal_approx)\n  )\n\nprint(\"Normal Approximation to Poisson:\")\nprint(poisson_approx_data)\n\n# Additional Poisson approximation examples\npoisson_examples &lt;- data.table(\n  lambda = c(10, 25, 50, 100),\n  test_x = c(15, 30, 55, 105)\n) %&gt;%\n  fmutate(\n    exact = ppois(test_x, lambda = lambda),\n    z_score = (test_x + 0.5 - lambda) / sqrt(lambda),\n    approx = pnorm(z_score),\n    error = abs(exact - approx)\n  )\n\nprint(\"Poisson Approximation Examples:\")\nprint(poisson_examples)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteJoint Probability Distributions\n\n\n\nWhen multiple random variables are measured simultaneously, we need joint probability distributions.\nJoint PDF (Continuous): For continuous random variables X and Y:\n\nP(a &lt; X &lt; b, c &lt; Y &lt; d) = \\int_a^b \\int_c^d f(x,y) \\, dy \\, dx\n\nProperties:\n\nf(x,y) \\geq 0 for all (x,y)\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) \\, dy \\, dx = 1\n\nJoint PMF (Discrete): For discrete random variables: f(x,y) = P(X = x, Y = y)\nMarginal Distributions:\n\nf_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy (continuous)\nf_X(x) = \\sum_y f(x,y) (discrete)\n\n\n\n\n\n\n\nFigure 25: Scatter diagram showing joint behavior of two variables\n\n\n\n\n\n\n\n\n\nFigure 26: Joint probability density function surface\n\n\n\n\n\n\n\n\n\nFigure 27: Probability as volume under joint PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIndependence of Random Variables\n\n\n\nRandom variables X_1, X_2, \\ldots, X_n are independent if:\nP(X_1 \\in E_1, X_2 \\in E_2, \\ldots, X_n \\in E_n) = P(X_1 \\in E_1) \\times P(X_2 \\in E_2) \\times \\cdots \\times P(X_n \\in E_n)\nfor any sets E_1, E_2, \\ldots, E_n.\nEquivalent Conditions:\n\nContinuous: f(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\times f_{X_2}(x_2) \\times \\cdots \\times f_{X_n}(x_n)\nDiscrete: f(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\times f_{X_2}(x_2) \\times \\cdots \\times f_{X_n}(x_n)\n\nPractical Meaning: Knowledge about one variable provides no information about others.\n\n\n\n\n\n\n\n\nTipExample 3-21: Shaft Diameter Independence\n\n\n\nShaft diameters are normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. Each shaft has probability 0.919 of meeting specifications.\nProblem: If diameters are independent, what’s the probability that all 10 shafts meet specifications?\nSolution: \\begin{aligned}\nP(\\text{all 10 meet specs}) &= P(X_1 \\text{ meets}) \\times P(X_2 \\text{ meets}) \\times \\cdots \\times P(X_{10} \\text{ meets}) \\\\\n&= (0.919)^{10} = 0.430\n\\end{aligned}\nInterpretation: Only 43% chance that all 10 shafts will be acceptable.\n\n\n\n\n\n\n\n\nTipExample 3-22: System Reliability\n\n\n\nA system operates if there’s a functional path from left to right. Components function independently with given probabilities.\n\n\n\n\n\n\nFigure 28: System reliability diagram\n\n\n\nSolution: For a series system: P(\\text{system works}) = P(C_1 \\text{ and } C_2) = P(C_1) \\times P(C_2) = 0.9 \\times 0.95 = 0.855\nFor parallel systems, calculate probability that at least one component works.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLinear Combinations of Independent Random Variables\n\n\n\nFor independent random variables X_1, X_2, \\ldots, X_n with means \\mu_i and variances \\sigma_i^2, consider the linear function:\nY = c_0 + c_1X_1 + c_2X_2 + \\cdots + c_nX_n\nMean and Variance:\n\n\\begin{aligned}\nE(Y) &= c_0 + c_1\\mu_1 + c_2\\mu_2 + \\cdots + c_n\\mu_n \\\\\nV(Y) &= c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2 + \\cdots + c_n^2\\sigma_n^2\n\\end{aligned}\n\nKey Properties:\n\nExpected value of a sum equals sum of expected values\nFor independent variables, variance of a sum equals sum of variances\nConstants factor out of variance as squares\n\nSpecial Case: If X_1, X_2, \\ldots, X_n are independent and normally distributed, then Y is also normally distributed.\n\n\n\n\n\n\n\n\nTipExample 3-23: Rectangular Part Perimeter\n\n\n\nA rectangular part has length X_1 and width X_2 that are independent and normally distributed: - X_1 \\sim N(2, 0.1^2) cm - X_2 \\sim N(5, 0.2^2) cm\nThe perimeter is Y = 2X_1 + 2X_2.\nSolution:\n\n\\begin{aligned}\nE(Y) &= 2E(X_1) + 2E(X_2) = 2(2) + 2(5) = 14 \\text{ cm} \\\\\nV(Y) &= 2^2V(X_1) + 2^2V(X_2) = 4(0.1^2) + 4(0.2^2) \\\\\n&= 4(0.01) + 4(0.04) = 0.04 + 0.16 = 0.20 \\text{ cm}^2 \\\\\n\\sigma_Y &= \\sqrt{0.20} = 0.447 \\text{ cm}\n\\end{aligned}\n\nSince X_1 and X_2 are normal and independent, Y \\sim N(14, 0.20).\nProbability calculation: P(Y &gt; 14.5) = P\\left(Z &gt; \\frac{14.5 - 14}{0.447}\\right) = P(Z &gt; 1.12) = 0.131\n\n\n\n\n\n\n\n\n\n\n\nNoteCovariance and Correlation\n\n\n\nWhen random variables are not independent, we need to account for their relationship.\nCovariance: \n\\text{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = E(XY) - \\mu_X\\mu_Y\n\nCorrelation Coefficient: \n\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\nProperties:\n\n-1 \\leq \\rho_{XY} \\leq 1\n\\rho_{XY} = 0 indicates no linear relationship\n|\\rho_{XY}| = 1 indicates perfect linear relationship\nIf X and Y are independent, then \\rho_{XY} = 0 (but not vice versa)\n\nGeneral Variance Formula: For Y = c_0 + c_1X_1 + c_2X_2 + \\cdots + c_nX_n:\n\nV(Y) = \\sum_{i=1}^n c_i^2\\sigma_i^2 + 2\\sum_{i&lt;j} c_ic_j\\text{Cov}(X_i, X_j)\n\n\n\n\n\n\n\n\n\nTipExample 3-24: Portfolio Risk\n\n\n\nAn investment portfolio consists of two stocks with returns X_1 and X_2:\n\nE(X_1) = 0.12, V(X_1) = 0.04\nE(X_2) = 0.08, V(X_2) = 0.02\n\n\\text{Cov}(X_1, X_2) = 0.01\n\nPortfolio return: Y = 0.6X_1 + 0.4X_2\nSolution:\n\n\\begin{aligned}\nE(Y) &= 0.6(0.12) + 0.4(0.08) = 0.072 + 0.032 = 0.104 \\\\\nV(Y) &= (0.6)^2(0.04) + (0.4)^2(0.02) + 2(0.6)(0.4)(0.01) \\\\\n&= 0.0144 + 0.0032 + 0.0048 = 0.0224\n\\end{aligned}\n\nInterpretation: Positive covariance increases portfolio risk compared to independent case.\n\n\n\n\n\n\n\n\n\n\n\nNotePropagation of Error Formula\n\n\n\nFor nonlinear functions, we use Taylor series approximation around the means.\nSingle Variable: If Y = h(X) where X has mean \\mu_X and variance \\sigma_X^2:\n\n\\begin{aligned}\nE(Y) &\\approx h(\\mu_X) \\\\\nV(Y) &\\approx \\left(\\frac{dh}{dx}\\bigg|_{x=\\mu_X}\\right)^2 \\sigma_X^2\n\\end{aligned}\n\nMultiple Variables: If Y = h(X_1, X_2, \\ldots, X_n) with independent X_i:\n\n\\begin{aligned}\nE(Y) &\\approx h(\\mu_1, \\mu_2, \\ldots, \\mu_n) \\\\\nV(Y) &\\approx \\sum_{i=1}^n \\left(\\frac{\\partial h}{\\partial x_i}\\bigg|_{\\boldsymbol{\\mu}}\\right)^2 \\sigma_i^2\n\\end{aligned}\n\nApplications: Engineering tolerance analysis, measurement uncertainty\n\n\n\n\n\n\n\n\nTipExample 3-25: Electrical Power Dissipation\n\n\n\nPower dissipated by a resistor: P = I^2R where I is current and R is resistance.\nGiven: I \\sim N(20, 0.1^2) amperes, R = 80 ohms (constant)\nSolution: h(I) = I^2R = 80I^2, so \\frac{dh}{dI} = 160I\n\n\\begin{aligned}\nE(P) &\\approx h(\\mu_I) = 80(20)^2 = 32,000 \\text{ watts} \\\\\nV(P) &\\approx \\left(\\frac{dh}{dI}\\bigg|_{I=20}\\right)^2 \\sigma_I^2 \\\\\n&= (160 \\times 20)^2 (0.1)^2 = (3200)^2(0.01) = 102,400 \\text{ watts}^2 \\\\\n\\sigma_P &= \\sqrt{102,400} = 320 \\text{ watts}\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Power Dissipation Analysis:\"\n\n\n   current_mean current_sd resistance derivative_at_mean power_mean_approx\n          &lt;num&gt;      &lt;num&gt;      &lt;num&gt;              &lt;num&gt;             &lt;num&gt;\n1:           20        0.1         80               3200             32000\n   power_variance_approx power_sd_approx exact_mean exact_variance exact_sd\n                   &lt;num&gt;           &lt;num&gt;      &lt;num&gt;          &lt;num&gt;    &lt;num&gt;\n1:                102400             320    32000.8       102401.3  320.002\n\n\n[1] \"Multiple Variable Function Analysis:\"\n\n\n     mu1 sigma1_sq   mu2 sigma2_sq   mu3 sigma3_sq dY_dX1 dY_dX2 dY_dX3\n   &lt;num&gt;     &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;num&gt;     &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;\n1:    10         1     5      0.25     2      0.04    2.5      5  -12.5\n   Y_mean_approx Y_variance_approx Y_sd_approx\n           &lt;num&gt;             &lt;num&gt;       &lt;num&gt;\n1:            25             18.75    4.330127\n\n\n\n\n\n# Example 3-25: Electrical Power Dissipation (Propagation of Error)\npower_analysis &lt;- data.table(\n  current_mean = 20,\n  current_sd = 0.1,\n  resistance = 80\n) %&gt;%\n  fmutate(\n    # Function: P = I^2 * R\n    # Derivative: dP/dI = 2*I*R\n    derivative_at_mean = 2 * current_mean * resistance,\n    # Mean of P\n    power_mean_approx = current_mean^2 * resistance,\n    # Variance of P using propagation of error\n    power_variance_approx = (derivative_at_mean)^2 * current_sd^2,\n    power_sd_approx = sqrt(power_variance_approx),\n    # Exact calculation for comparison (since I is normal, I^2 follows chi-square scaled)\n    # For normal I, E[I^2] = mu^2 + sigma^2, Var[I^2] = 2*sigma^4 + 4*mu^2*sigma^2\n    exact_mean = (current_mean^2 + current_sd^2) * resistance,\n    exact_variance = (2 * current_sd^4 + 4 * current_mean^2 * current_sd^2) * resistance^2,\n    exact_sd = sqrt(exact_variance)\n  )\n\nprint(\"Power Dissipation Analysis:\")\nprint(power_analysis)\n\n# Multiple variable example: Y = X1*X2/X3\nmulti_var_data &lt;- data.table(\n  mu1 = 10, sigma1_sq = 1,\n  mu2 = 5, sigma2_sq = 0.25,\n  mu3 = 2, sigma3_sq = 0.04\n) %&gt;%\n  fmutate(\n    # Function: Y = X1*X2/X3\n    # Partial derivatives at means\n    dY_dX1 = mu2 / mu3,\n    dY_dX2 = mu1 / mu3,\n    dY_dX3 = -(mu1 * mu2) / (mu3^2),\n    # Approximate mean\n    Y_mean_approx = (mu1 * mu2) / mu3,\n    # Approximate variance\n    Y_variance_approx = (dY_dX1)^2 * sigma1_sq + (dY_dX2)^2 * sigma2_sq + (dY_dX3)^2 * sigma3_sq,\n    Y_sd_approx = sqrt(Y_variance_approx)\n  )\n\nprint(\"Multiple Variable Function Analysis:\")\nprint(multi_var_data)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-26: Multiple Variable Function\n\n\n\nConsider Y = \\frac{X_1X_2}{X_3} where X_1, X_2, X_3 are independent with:\n\n\\mu_1 = 10, \\sigma_1^2 = 1\n\\mu_2 = 5, \\sigma_2^2 = 0.25\n\n\\mu_3 = 2, \\sigma_3^2 = 0.04\n\nSolution:\n\n\\frac{\\partial h}{\\partial x_1} = \\frac{x_2}{x_3}, \\quad \\frac{\\partial h}{\\partial x_2} = \\frac{x_1}{x_3}, \\quad \\frac{\\partial h}{\\partial x_3} = -\\frac{x_1x_2}{x_3^2}\n\nAt (\\mu_1, \\mu_2, \\mu_3) = (10, 5, 2):\n\n\\frac{\\partial h}{\\partial x_1}\\bigg|_{\\boldsymbol{\\mu}} = \\frac{5}{2} = 2.5, \\quad \\frac{\\partial h}{\\partial x_2}\\bigg|_{\\boldsymbol{\\mu}} = \\frac{10}{2} = 5, \\quad \\frac{\\partial h}{\\partial x_3}\\bigg|_{\\boldsymbol{\\mu}} = -\\frac{50}{4} = -12.5\n\n\n\\begin{aligned}\nE(Y) &\\approx \\frac{10 \\times 5}{2} = 25 \\\\\nV(Y) &\\approx (2.5)^2(1) + (5)^2(0.25) + (-12.5)^2(0.04) \\\\\n&= 6.25 + 6.25 + 6.25 = 18.75\n\\end{aligned}\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteRandom Sample and Statistics\n\n\n\nRandom Sample: Independent random variables X_1, X_2, \\ldots, X_n with the same distribution are called a random sample of size n.\nStatistic: A statistic is any function of the random variables in a random sample. Examples include:\n\nSample mean: \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\nSample variance: S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X})^2\nSample range: R = X_{(n)} - X_{(1)}\n\nSampling Distribution: The probability distribution of a statistic is called its sampling distribution.\nKey Properties of Sample Mean: If X_1, X_2, \\ldots, X_n is a random sample from a population with mean \\mu and variance \\sigma^2:\n\n\\begin{aligned}\nE(\\overline{X}) &= \\mu \\\\\nV(\\overline{X}) &= \\frac{\\sigma^2}{n} \\\\\n\\sigma_{\\overline{X}} &= \\frac{\\sigma}{\\sqrt{n}}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\nNoteCentral Limit Theorem\n\n\n\nCentral Limit Theorem: If X_1, X_2, \\ldots, X_n is a random sample of size n from a population with mean \\mu and variance \\sigma^2, then as n \\to \\infty:\nZ = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0, 1)\nKey Points:\n\nApplies regardless of the population distribution shape\nApproximation improves as n increases\nRule of thumb: n \\geq 30 for reasonable approximation\nIf population is normal, then \\overline{X} is exactly normal for any n\n\nPractical Importance: - Justifies normal approximations - Foundation for confidence intervals - Basis for hypothesis testing - Explains why sample means are approximately normal\n\n\n\n\n\n\n\n\n\n\n\n(a) One die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Two dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Three dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Five dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Ten dice\n\n\n\n\n\n\n\nFigure 29: Distributions of average scores from throwing dice (demonstration of CLT)\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-27: Resistor Manufacturing\n\n\n\nAn electronics company manufactures resistors with mean resistance 100 Ω and standard deviation 10 Ω. Find the probability that a random sample of n = 25 resistors has average resistance less than 95 Ω.\nSolution: Let \\overline{X} be the sample mean resistance.\nBy the Central Limit Theorem: \n\\overline{X} \\sim N\\left(100, \\frac{10^2}{25}\\right) = N(100, 4)\n\n\n\\begin{aligned}\nP(\\overline{X} &lt; 95) &= P\\left(\\frac{\\overline{X} - 100}{10/\\sqrt{25}} &lt; \\frac{95 - 100}{2}\\right) \\\\\n&= P(Z &lt; -2.5) = 0.0062\n\\end{aligned}\n\nInterpretation: Only 0.62% chance that the sample mean is below 95 Ω.\n\nR OutputR Code\n\n\n\n\n[1] \"Central Limit Theorem - Resistor Example:\"\n\n\n   population_mean population_sd sample_size test_value sample_mean_mean\n             &lt;num&gt;         &lt;num&gt;       &lt;num&gt;      &lt;num&gt;            &lt;num&gt;\n1:             100            10          25         95              100\n   sample_mean_sd sample_mean_variance z_score probability percentile_5\n            &lt;num&gt;                &lt;num&gt;   &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:              2                    4    -2.5 0.006209665     96.71029\n   percentile_95\n           &lt;num&gt;\n1:      103.2897\n\n\n[1] \"CLT Demonstration - Effect of Sample Size:\"\n\n\n   sample_size sample_mean_sd prob_below_95 prob_above_105\n         &lt;num&gt;          &lt;num&gt;         &lt;num&gt;          &lt;num&gt;\n1:           1      10.000000  3.085375e-01   3.085375e-01\n2:           4       5.000000  1.586553e-01   1.586553e-01\n3:           9       3.333333  6.680720e-02   6.680720e-02\n4:          16       2.500000  2.275013e-02   2.275013e-02\n5:          25       2.000000  6.209665e-03   6.209665e-03\n6:          36       1.666667  1.349898e-03   1.349898e-03\n7:          49       1.428571  2.326291e-04   2.326291e-04\n8:          64       1.250000  3.167124e-05   3.167124e-05\n\n\n\n\n\n# Example 3-27: Resistor Manufacturing (Central Limit Theorem)\nresistor_clt &lt;- data.table(\n  population_mean = 100,\n  population_sd = 10,\n  sample_size = 25,\n  test_value = 95\n) %&gt;%\n  fmutate(\n    # Sample mean distribution\n    sample_mean_mean = population_mean,\n    sample_mean_sd = population_sd / sqrt(sample_size),\n    sample_mean_variance = sample_mean_sd^2,\n    # Probability calculation\n    z_score = (test_value - sample_mean_mean) / sample_mean_sd,\n    probability = pnorm(z_score),\n    # Additional percentiles\n    percentile_5 = qnorm(0.05, mean = sample_mean_mean, sd = sample_mean_sd),\n    percentile_95 = qnorm(0.95, mean = sample_mean_mean, sd = sample_mean_sd)\n  )\n\nprint(\"Central Limit Theorem - Resistor Example:\")\nprint(resistor_clt)\n\n# CLT demonstration with different sample sizes\nclt_demo &lt;- data.table(\n  sample_size = c(1, 4, 9, 16, 25, 36, 49, 64)\n) %&gt;%\n  fmutate(\n    sample_mean_sd = 10 / sqrt(sample_size),\n    prob_below_95 = pnorm(95, mean = 100, sd = sample_mean_sd),\n    prob_above_105 = pnorm(105, mean = 100, sd = sample_mean_sd, lower.tail = FALSE)\n  )\n\nprint(\"CLT Demonstration - Effect of Sample Size:\")\nprint(clt_demo)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-28: Quality Control Application\n\n\n\nA production process has mean output 500 units/hour with standard deviation 50 units/hour. If we observe production for 36 hours, what’s the probability that average hourly production exceeds 510 units?\nSolution: n = 36, \\mu = 500, \\sigma = 50\n\n\\begin{aligned}\nP(\\overline{X} &gt; 510) &= P\\left(Z &gt; \\frac{510 - 500}{50/\\sqrt{36}}\\right) \\\\\n&= P\\left(Z &gt; \\frac{10}{50/6}\\right) \\\\\n&= P(Z &gt; 1.2) = 1 - 0.8849 = 0.1151\n\\end{aligned}\n\nInterpretation: About 11.5% chance of observing such high average production.\n\n\n\n\n\n\n\n\n\n\n\nImportantChapter 3 Summary\n\n\n\nContinuous Random Variables:\n\nProbability density function (PDF): P(a &lt; X &lt; b) = \\int_a^b f(x)dx\nCumulative distribution function (CDF): F(x) = P(X \\leq x)\nMean: \\mu = \\int_{-\\infty}^{\\infty} x f(x)dx\nVariance: \\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)dx\n\nImportant Continuous Distributions:\n\nNormal: Bell-shaped, symmetric, completely specified by \\mu and \\sigma^2\nLognormal: For positive variables, related to normal via logarithm\nExponential: Memoryless, models time between events\nGamma: Generalizes exponential, models waiting times\nWeibull: Flexible shape, widely used in reliability\nBeta: Defined on [0,1], models proportions\n\nDiscrete Random Variables:\n\nProbability mass function (PMF): f(x) = P(X = x)\nMean: \\mu = \\sum x f(x)\nVariance: \\sigma^2 = \\sum (x-\\mu)^2 f(x)\n\nImportant Discrete Distributions:\n\nBinomial: Fixed number of independent trials\nPoisson: Counts of rare events\nGeometric: Number of trials until first success\nHypergeometric: Sampling without replacement\n\nMultiple Random Variables:\n\nIndependence: Joint distribution factors\nLinear combinations: E(aX + bY) = aE(X) + bE(Y)\nFor independent variables: V(aX + bY) = a^2V(X) + b^2V(Y)\nCovariance and correlation measure dependence\n\nCentral Limit Theorem:\n\nSample means approach normal distribution\nFoundation for statistical inference\nExplains robustness of normal-based methods\n\nApplications:\n\nQuality control and process monitoring\nReliability and survival analysis\nRisk assessment and decision making\nExperimental design and data analysis\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Distribution Summary Statistics:\"\n\n\n   distribution sample_mean sample_sd  sample_min sample_max sample_range\n         &lt;char&gt;       &lt;num&gt;     &lt;num&gt;       &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:       Normal    50.90406  9.128159 26.90831124   71.87333     44.96502\n2:  Exponential    10.43276  9.353413  0.04367371   43.65911     43.61544\n3:      Weibull    88.00317 42.605619  5.82109597  189.09828    183.27719\n4:    Lognormal    60.70836 31.776841 10.42973266  186.75246    176.32273\n          cv\n       &lt;num&gt;\n1: 0.1793208\n2: 0.8965424\n3: 0.4841373\n4: 0.5234344\n\n\n[1] \"Parameter Estimates:\"\n\n\n   distribution param1_name param1_estimate param2_name param2_estimate\n         &lt;char&gt;      &lt;char&gt;           &lt;num&gt;      &lt;char&gt;           &lt;num&gt;\n1:       Normal        mean      50.9040591          sd       9.0824033\n2:  Exponential        rate       0.0958519        &lt;NA&gt;              NA\n3:      Weibull       shape       2.1616282       scale      98.9731545\n4:    Lognormal     meanlog       3.9722223       sdlog       0.5320079\n\n\n[1] \"Goodness of Fit Tests:\"\n\n\n   distribution ks_statistic ks_p_value good_fit\n         &lt;char&gt;        &lt;num&gt;      &lt;num&gt;   &lt;lgcl&gt;\n1:       Normal   0.05871913  0.8807748     TRUE\n2:  Exponential   0.07511794  0.6251819     TRUE\n3:      Weibull   0.09167937  0.3699697     TRUE\n4:    Lognormal   0.09690997  0.3046243     TRUE\n\n\n\n\n\n# Comprehensive Summary and Distribution Fitting Examples\nlibrary(fitdistrplus)\n\n# Generate sample data for distribution fitting\nset.seed(123)\n\n# Example datasets\nnormal_sample &lt;- rnorm(100, mean = 50, sd = 10)\nexponential_sample &lt;- rexp(100, rate = 0.1)\nweibull_sample &lt;- rweibull(100, shape = 2, scale = 100)\nlognormal_sample &lt;- rlnorm(100, meanlog = 4, sdlog = 0.5)\n\n# Create comprehensive dataset\nsample_data &lt;- data.table(\n  normal_data = normal_sample,\n  exponential_data = exponential_sample,\n  weibull_data = weibull_sample,\n  lognormal_data = lognormal_sample\n) %&gt;%\n  fmutate(\n    observation = 1:fnobs(normal_data)\n  )\n\n# Fit distributions using fitdistrplus\nfit_normal &lt;- fitdist(sample_data$normal_data, \"norm\")\nfit_exponential &lt;- fitdist(sample_data$exponential_data, \"exp\")\nfit_weibull &lt;- fitdist(sample_data$weibull_data, \"weibull\")\nfit_lognormal &lt;- fitdist(sample_data$lognormal_data, \"lnorm\")\n\n# Summary statistics for each distribution\ndistribution_summary &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  sample_mean = c(\n    fmean(sample_data$normal_data),\n    fmean(sample_data$exponential_data),\n    fmean(sample_data$weibull_data),\n    fmean(sample_data$lognormal_data)\n  ),\n  sample_sd = c(\n    fsd(sample_data$normal_data),\n    fsd(sample_data$exponential_data),\n    fsd(sample_data$weibull_data),\n    fsd(sample_data$lognormal_data)\n  ),\n  sample_min = c(\n    fmin(sample_data$normal_data),\n    fmin(sample_data$exponential_data),\n    fmin(sample_data$weibull_data),\n    fmin(sample_data$lognormal_data)\n  ),\n  sample_max = c(\n    fmax(sample_data$normal_data),\n    fmax(sample_data$exponential_data),\n    fmax(sample_data$weibull_data),\n    fmax(sample_data$lognormal_data)\n  )\n) %&gt;%\n  fmutate(\n    sample_range = sample_max - sample_min,\n    cv = sample_sd / sample_mean\n  )\n\nprint(\"Distribution Summary Statistics:\")\nprint(distribution_summary)\n\n# Parameter estimates\nparam_estimates &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  param1_name = c(\"mean\", \"rate\", \"shape\", \"meanlog\"),\n  param1_estimate = c(\n    fit_normal$estimate[1],\n    fit_exponential$estimate[1],\n    fit_weibull$estimate[1],\n    fit_lognormal$estimate[1]\n  ),\n  param2_name = c(\"sd\", NA, \"scale\", \"sdlog\"),\n  param2_estimate = c(\n    fit_normal$estimate[2],\n    NA,\n    fit_weibull$estimate[2],\n    fit_lognormal$estimate[2]\n  )\n)\n\nprint(\"Parameter Estimates:\")\nprint(param_estimates)\n\n# Goodness of fit tests\ngof_tests &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  ks_statistic = c(\n    ks.test(sample_data$normal_data, \"pnorm\",\n      mean = fit_normal$estimate[1], sd = fit_normal$estimate[2]\n    )$statistic,\n    ks.test(sample_data$exponential_data, \"pexp\",\n      rate = fit_exponential$estimate[1]\n    )$statistic,\n    ks.test(sample_data$weibull_data, \"pweibull\",\n      shape = fit_weibull$estimate[1], scale = fit_weibull$estimate[2]\n    )$statistic,\n    ks.test(sample_data$lognormal_data, \"plnorm\",\n      meanlog = fit_lognormal$estimate[1], sdlog = fit_lognormal$estimate[2]\n    )$statistic\n  ),\n  ks_p_value = c(\n    ks.test(sample_data$normal_data, \"pnorm\",\n      mean = fit_normal$estimate[1], sd = fit_normal$estimate[2]\n    )$p.value,\n    ks.test(sample_data$exponential_data, \"pexp\",\n      rate = fit_exponential$estimate[1]\n    )$p.value,\n    ks.test(sample_data$weibull_data, \"pweibull\",\n      shape = fit_weibull$estimate[1], scale = fit_weibull$estimate[2]\n    )$p.value,\n    ks.test(sample_data$lognormal_data, \"plnorm\",\n      meanlog = fit_lognormal$estimate[1], sdlog = fit_lognormal$estimate[2]\n    )$p.value\n  )\n) %&gt;%\n  fmutate(\n    good_fit = ks_p_value &gt; 0.05\n  )\n\nprint(\"Goodness of Fit Tests:\")\nprint(gof_tests)\n\n# Save all datasets\nfwrite(sample_data, \"data/distribution_samples.csv\")\nfwrite(distribution_summary, \"data/distribution_summary.csv\")\nfwrite(param_estimates, \"data/parameter_estimates.csv\")\nfwrite(gof_tests, \"data/goodness_of_fit.csv\")\n\n\n\n\n:::",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#introduction",
    "href": "book/Ch03.html#introduction",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteIntroduction to Random Experiments\n\n\n\nA random experiment is an experiment that can result in different outcomes, even though it is repeated in the same manner every time. The collection of all possible outcomes of a random experiment is called the sample space of the experiment.\nKey Concepts:\n\nExperiment: A process that generates observations\nRandom: The outcome cannot be predicted with certainty\nRandom experiment: An experiment where the outcome varies unpredictably\n\nFigure 1 illustrates the relationship between a mathematical model and the physical system it represents. While models like Newton’s laws are not perfect abstractions, they are useful for analyzing and approximating system performance. Once validated with measurements, models can help understand, describe, and predict a system’s response to inputs.\n\n\n\n\n\n\nFigure 1: Continuous iteration between model and physical system.\n\n\n\nFigure 2 illustrates a model where uncontrollable variables (noise) interact with controllable variables to produce system outputs. Due to the noise, identical controllable settings yield varying outputs upon measurement.\n\n\n\n\n\n\nFigure 2: Noise variables affect the transformation of inputs to outputs.\n\n\n\nFor measuring current in a copper wire, Ohm’s law may suffice as a model. However, if variations are significant, the model may need to account for them (see Figure 3).\n\n\n\n\n\n\nFigure 3: A closer examination of the system identifies deviations from the model.\n\n\n\nIn designing a voice communication system, a model is required for call frequency and duration. Even if calls occur and last precisely 5 minutes on average, variations in timing or duration can lead to call blocking, necessitating multiple lines (see Figure 4).\n\n\n\n\n\n\nFigure 4: Variation causes disruptions in the system.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#random-variables",
    "href": "book/Ch03.html#random-variables",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteRandom Variables\n\n\n\nA random variable is a function that assigns a real number to each outcome in the sample space of a random experiment. It provides a numerical description of the outcome of a random experiment.\nDefinition: A random variable is a numerical variable whose measured value can change from one replicate of the experiment to another.\nTypes of Random Variables:\n\nDiscrete Random Variable: A random variable with a finite (or countably infinite) set of real numbers for its range.\n\nExamples: number of scratches on a surface, proportion of defective parts among 1000 tested, number of transmitted bits received in error\n\nContinuous Random Variable: A random variable with an interval (either finite or infinite) of real numbers for its range.\n\nExamples: electrical current, length, pressure, temperature, time, voltage, weight\n\n\nThe distinction between discrete and continuous random variables is important because different methods are used to work with each type.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#probability",
    "href": "book/Ch03.html#probability",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteProbability Fundamentals\n\n\n\nProbability is used to quantify likelihood or chance and represents risk or uncertainty in engineering applications. It can be interpreted as our degree of belief or relative frequency.\nKey Properties:\n\nProbability statements describe the likelihood that particular values occur\nThe likelihood is quantified by assigning a number from the interval [0, 1] to the set of values (or a percentage from 0 to 100%)\nHigher numbers indicate that the set of values is more likely\nA probability is usually expressed in terms of a random variable\n\nFor example, if X denotes part length, the probability statement can be written as: P(X \\in [10.8, 11.2]) = 0.25 \\quad \\text{or} \\quad P(10.8 \\leq X \\leq 11.2) = 0.25\nBoth equations state that the probability that the random variable X assumes a value in [10.8, 11.2] is 0.25.\n\n\n\n\n\n\n\n\n\n\nNoteComplement of an Event\n\n\n\nGiven a set E, the complement of E is the set of elements that are not in E. The complement is denoted as E' or E^c.\nProperty: P(E^c) = 1 - P(E)\n\n\n\n\n\n\n\n\n\n\n\nNoteMutually Exclusive Events\n\n\n\nThe sets E_1, E_2, \\ldots, E_k are mutually exclusive if the intersection of any pair is empty. That is, each element is in one and only one of the sets E_1, E_2, \\ldots, E_k.\nProperty: If events are mutually exclusive, their probabilities add.\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Properties\n\n\n\n\nP(X \\in \\mathbb{R}) = 1, where \\mathbb{R} is the set of real numbers\n0 \\leq P(X \\in E) \\leq 1 for any set E\nIf E_1, E_2, \\ldots, E_k are mutually exclusive sets:\n\n\n\\small\nP(X \\in E_1 \\cup E_2 \\cup \\ldots \\cup E_k) = P(X \\in E_1) + P(X \\in E_2) + \\cdots + P(X \\in E_k)\n\n\n\n\n\n\n\n\n\n\n\n\nNoteEvents\n\n\n\nAn event is a subset of the sample space. Sometimes, experimental results are classified into categories rather than measured numerically.\nExamples:\n\nCurrent measurement recorded as low, medium, or high\nElectronic component classified as defective or not defective\nMessage transmission success or failure\n\nEvents provide a framework for probability calculations in both discrete and continuous settings.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#continuous-random-variables",
    "href": "book/Ch03.html#continuous-random-variables",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteProbability Density Function\n\n\n\nThe probability distribution or simply distribution of a random variable X is a description of the set of probabilities associated with the possible values for X.\nThe probability density function (PDF) f(x) of a continuous random variable X is used to determine probabilities as follows:\nP(a &lt; X &lt; b) = \\int_{a}^{b} f(x)\\,dx\nProperties of the PDF:\n\nf(x) \\geq 0 for all x\n\\int_{-\\infty}^{\\infty} f(x)\\,dx = 1\n\nImportant Note: If X is a continuous random variable, for any x_1 and x_2:\n\n\\small\nP(x_1 \\leq X \\leq x_2) = P(x_1 &lt; X \\leq x_2) = P(x_1 \\leq X &lt; x_2) = P(x_1 &lt; X &lt; x_2)\n\nThis is because P(X = c) = 0 for any specific value c when X is continuous.\n\n\n\n\n\n\n\n\nTipExample 3-1: Exponential Distribution for Disk Flaw Distance\n\n\n\nLet the continuous random variable X denote the distance in micrometers from the start of a track on a magnetic disk until the first flaw. Historical data show that the distribution of X can be modeled by the probability density function:\nf(x) = \\frac{1}{2000} e^{-x/2000}, \\quad x \\geq 0\nProblem 1: For what proportion of disks is the distance to the first flaw greater than 1000 micrometers?\nSolution: \\begin{aligned}\nP(X &gt; 1000) &= \\int_{1000}^{\\infty} f(x)\\,dx \\\\\n&= \\int_{1000}^{\\infty} \\frac{1}{2000} e^{-x/2000} \\,dx \\\\\n&= \\left[ -e^{-x/2000} \\right]_{1000}^{\\infty} \\\\\n&= 0 - (-e^{-1000/2000}) \\\\\n&= e^{-1/2} \\\\\n&\\approx 0.6065\n\\end{aligned}\nProblem 2: What proportion of disks has the flaw distance between 1000 and 2000 micrometers?\nSolution: \\begin{aligned}\nP(1000 \\leq X \\leq 2000) &= \\int_{1000}^{2000} f(x)\\,dx \\\\\n&= \\left[ -e^{-x/2000} \\right]_{1000}^{2000} \\\\\n&= e^{-1/2} - e^{-1} \\\\\n&= 0.6065 - 0.3679 \\\\\n&= 0.2386\n\\end{aligned}\n\n\n\n\n\n\nFigure 5: Probability density function for disk flaw example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Disk Flaw Distance Analysis:\"\n\n\n   lambda mean_param prob_greater_1000 prob_between_1000_2000\n    &lt;num&gt;      &lt;num&gt;             &lt;num&gt;                  &lt;num&gt;\n1:  5e-04       2000         0.6065307              0.2386512\n   prob_greater_1000_manual prob_between_manual\n                      &lt;num&gt;               &lt;num&gt;\n1:                0.6065307           0.2386512\n\n\n\n\n\n# Example 3-1: Exponential Distribution for Disk Flaw Distance\nlibrary(fastverse)\nlibrary(tidyverse)\nlibrary(fitdistrplus)\nlibrary(scales)\n\n# Define the exponential PDF for disk flaw example\ndisk_data &lt;- data.table(\n  lambda = 1 / 2000,\n  mean_param = 2000\n) %&gt;%\n  fmutate(\n    # Calculate probabilities\n    prob_greater_1000 = pexp(q = 1000, rate = lambda, lower.tail = FALSE),\n    prob_between_1000_2000 = pexp(q = 2000, rate = lambda, lower.tail = TRUE) -\n      pexp(q = 1000, rate = lambda, lower.tail = TRUE),\n    # Verify using manual integration\n    prob_greater_1000_manual = exp(-1000 / mean_param),\n    prob_between_manual = exp(-1000 / mean_param) - exp(-2000 / mean_param)\n  )\n\nprint(\"Disk Flaw Distance Analysis:\")\nprint(disk_data)\n\n# Create visualization\nx_vals &lt;- seq(0, 8000, by = 50)\npdf_vals &lt;- dexp(x_vals, rate = 1 / 2000)\n\nfwrite(data.table(x = x_vals, pdf = pdf_vals), \"data/exponential_pdf.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function\n\n\n\nThe cumulative distribution function (CDF) of a continuous random variable X with probability density function f(x) is:\nF(x) = P(X \\leq x) = \\int_{-\\infty}^{x} f(u)\\,du\nfor -\\infty &lt; x &lt; \\infty.\nKey Relationships:\n\nP(a &lt; X &lt; b) = F(b) - F(a)\n\\frac{d}{dx}F(x) = f(x) (fundamental theorem of calculus)\nF(x) is non-decreasing\n\\lim_{x \\to -\\infty} F(x) = 0 and \\lim_{x \\to \\infty} F(x) = 1\n\n\n\n\n\n\n\n\n\nTipExample 3-2: CDF for Disk Flaw Distance\n\n\n\nContinuing with the disk flaw example, the CDF is:\n\\begin{aligned}\nF(x) &= P(X \\leq x) = \\int_{0}^{x} \\frac{1}{2000} e^{-u/2000}\\,du \\\\\n&= \\left[ -e^{-u/2000} \\right]_{0}^{x} \\\\\n&= 1 - e^{-x/2000}\n\\end{aligned}\nfor x \\geq 0, and F(x) = 0 for x &lt; 0.\nVerification: We can check that P(X &gt; 1000) = 1 - F(1000) = 1 - (1 - e^{-1/2}) = e^{-1/2} = 0.6065\n\n\n\n\n\n\nFigure 6: Cumulative distribution function for disk flaw example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"CDF Verification:\"\n\n\n       x cdf_value cdf_manual prob_greater_1000_cdf prob_between_1000_2000_cdf\n   &lt;num&gt;     &lt;num&gt;      &lt;num&gt;                 &lt;num&gt;                      &lt;num&gt;\n1:  1000 0.3934693  0.3934693             0.6065307                  0.2386512\n2:  2000 0.6321206  0.6321206             0.6065307                  0.2386512\n\n\n\n\n\n# Example 3-2: CDF for Disk Flaw Distance\ncdf_data &lt;- data.table(\n  x = x_vals\n) %&gt;%\n  fmutate(\n    cdf_value = pexp(x, rate = 1 / 2000),\n    cdf_manual = 1 - exp(-x / 2000),\n    # Verify probabilities using CDF\n    prob_greater_1000_cdf = 1 - pexp(1000, rate = 1 / 2000),\n    prob_between_1000_2000_cdf = pexp(2000, rate = 1 / 2000) - pexp(1000, rate = 1 / 2000)\n  )\n\nprint(\"CDF Verification:\")\nprint(cdf_data[x %in% c(1000, 2000)])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMean and Variance for Continuous Random Variables\n\n\n\nSuppose X is a continuous random variable with PDF f(x).\nMean (Expected Value): The mean or expected value of X, denoted as \\mu or E(X), is:\n\n\\mu = E(X) = \\int_{-\\infty}^{\\infty} x f(x)\\,dx\n\nVariance: The variance of X, denoted as V(X) or \\sigma^2, is:\n\n\\begin{aligned}\n\\sigma^2 = V(X) &= \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)\\,dx \\\\\n&= E(X^2) - [E(X)]^2\n\\end{aligned}\n\nStandard Deviation: The standard deviation of X is \\sigma = \\sqrt{V(X)}.\nInterpretation:\n\nThe mean represents the “center” or average value\nThe variance measures the spread or variability\nStandard deviation is in the same units as the original variable\n\n\n\n\n\n\n\n\n\nTipExample 3-3: Mean and Variance Calculation\n\n\n\nFor the disk flaw distance example with f(x) = \\frac{1}{2000} e^{-x/2000} for x \\geq 0:\nMean Calculation: \\begin{aligned}\nE(X) &= \\int_{0}^{\\infty} x \\cdot \\frac{1}{2000} e^{-x/2000}\\,dx\n\\end{aligned}\nUsing integration by parts with u = x and dv = \\frac{1}{2000} e^{-x/2000} dx:\n\\begin{aligned}\nE(X) &= \\left[ -x e^{-x/2000} \\right]_{0}^{\\infty} + \\int_{0}^{\\infty} e^{-x/2000}\\,dx \\\\\n&= 0 + 2000 \\left[ -e^{-x/2000} \\right]_{0}^{\\infty} \\\\\n&= 2000(0 - (-1)) = 2000\n\\end{aligned}\nVariance Calculation: For this exponential distribution, V(X) = (2000)^2 = 4,000,000.\nInterpretation: On average, the first flaw occurs at 2000 micrometers, with substantial variability (standard deviation = 2000 micrometers).\n\nR OutputR Code\n\n\n\n\n[1] \"Exponential Distribution Statistics:\"\n\n\n    rate scale theoretical_mean theoretical_variance theoretical_sd mean_manual\n   &lt;num&gt; &lt;num&gt;            &lt;num&gt;                &lt;num&gt;          &lt;num&gt;       &lt;num&gt;\n1: 5e-04  2000             2000                4e+06           2000        2000\n   variance_manual\n             &lt;num&gt;\n1:           4e+06\n\n\n\n\n\n# Example 3-3: Mean and Variance Calculation\nexponential_stats &lt;- data.table(\n  rate = 1 / 2000,\n  scale = 2000\n) %&gt;%\n  fmutate(\n    theoretical_mean = 1 / rate,\n    theoretical_variance = 1 / rate^2,\n    theoretical_sd = sqrt(theoretical_variance),\n    # Verify mean calculation manually\n    mean_manual = scale,\n    variance_manual = scale^2\n  )\n\nprint(\"Exponential Distribution Statistics:\")\nprint(exponential_stats)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#important-continuous-distributions",
    "href": "book/Ch03.html#important-continuous-distributions",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteNormal Distribution\n\n\n\nThe normal distribution is the most widely used model for continuous random variables. It’s also called the Gaussian distribution.\nProbability Density Function: A random variable X has a normal distribution with parameters \\mu and \\sigma if:\nf(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right\\}\nfor -\\infty &lt; x &lt; \\infty, where -\\infty &lt; \\mu &lt; \\infty and \\sigma &gt; 0.\nParameters:\n\nE(X) = \\mu (location parameter)\nV(X) = \\sigma^2 (scale parameter)\n\nNotation: X \\sim N(\\mu, \\sigma^2)\nKey Properties:\n\nSymmetric about \\mu\nBell-shaped curve\nInflection points at \\mu \\pm \\sigma\nAbout 68% of values within \\mu \\pm \\sigma\nAbout 95% of values within \\mu \\pm 2\\sigma\nAbout 99.7% of values within \\mu \\pm 3\\sigma\n\n\n\n\n\n\n\nFigure 7: Normal probability density functions for selected values of \\mu and \\sigma^2\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-4: Current Measurement\n\n\n\nAssume that current measurements in a wire follow a normal distribution with mean \\mu = 10 milliamperes and variance \\sigma^2 = 4 (so \\sigma = 2) milliamperes^2. What is the probability that a measurement exceeds 13 milliamperes?\nSolution: We need P(X &gt; 13) where X \\sim N(10, 4).\nThis probability corresponds to the area under the normal curve to the right of 13, as shown in Figure 8.\n\n\n\n\n\n\nFigure 8: Probability that X &gt; 13 for normal distribution with \\mu = 10, \\sigma^2 = 4\n\n\n\nSince there’s no closed-form expression for the normal integral, we use standardization or statistical software.\n\n\n\n\n\n\n\n\nNoteEmpirical Rule for Normal Distribution\n\n\n\nFor any normal distribution:\n\\begin{aligned}\nP(\\mu - \\sigma &lt; X &lt; \\mu + \\sigma) &= 0.6827 \\\\\nP(\\mu - 2\\sigma &lt; X &lt; \\mu + 2\\sigma) &= 0.9545 \\\\\nP(\\mu - 3\\sigma &lt; X &lt; \\mu + 3\\sigma) &= 0.9973\n\\end{aligned}\n\n\n\n\n\n\nFigure 9: Probabilities associated with a normal distribution\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Distribution Empirical Rule:\"\n\n\n      mu sigma prob_1sigma prob_2sigma prob_3sigma theoretical_1sigma\n   &lt;num&gt; &lt;num&gt;       &lt;num&gt;       &lt;num&gt;       &lt;num&gt;              &lt;num&gt;\n1:     0     1   0.6826895   0.9544997   0.9973002             0.6827\n   theoretical_2sigma theoretical_3sigma\n                &lt;num&gt;              &lt;num&gt;\n1:             0.9545             0.9973\n\n\n\n\n\n# Normal Distribution Properties and Empirical Rule\nnormal_props &lt;- data.table(\n  mu = 0,\n  sigma = 1\n) %&gt;%\n  fmutate(\n    # Empirical rule calculations\n    prob_1sigma = pnorm(mu + sigma) - pnorm(mu - sigma),\n    prob_2sigma = pnorm(mu + 2 * sigma) - pnorm(mu - 2 * sigma),\n    prob_3sigma = pnorm(mu + 3 * sigma) - pnorm(mu - 3 * sigma),\n    # Theoretical values\n    theoretical_1sigma = 0.6827,\n    theoretical_2sigma = 0.9545,\n    theoretical_3sigma = 0.9973\n  )\n\nprint(\"Normal Distribution Empirical Rule:\")\nprint(normal_props)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteStandard Normal Random Variable\n\n\n\nA normal random variable with \\mu = 0 and \\sigma^2 = 1 is called a standard normal random variable, denoted as Z.\nStandard Normal CDF: \\Phi(z) = P(Z \\leq z)\nStandardizing: If X \\sim N(\\mu, \\sigma^2), then:\nZ = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\nKey Property: P(X \\leq x) = P\\left(Z \\leq \\frac{x - \\mu}{\\sigma}\\right) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\n\n\n\n\n\n\nFigure 10: Standard normal probability density function\n\n\n\n\n\n\n\n\n\n\nFigure 11: Cumulative Standard Normal Distribution Table\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-5: Standard Normal Calculations\n\n\n\nProblem 1: P(Z &gt; 1.26)\nSolution: P(Z &gt; 1.26) = 1 - P(Z \\leq 1.26) = 1 - 0.89617 = 0.10383\nProblem 2: P(Z &lt; -0.86)\nSolution: P(Z &lt; -0.86) = 0.19489\nProblem 3: P(-1.25 &lt; Z &lt; 0.37)\nSolution: P(-1.25 &lt; Z &lt; 0.37) = P(Z &lt; 0.37) - P(Z &lt; -1.25) = 0.64431 - 0.10565 = 0.53866\nProblem 4: Find z such that P(Z &gt; z) = 0.05\nSolution: We need P(Z \\leq z) = 0.95. From the table, z = 1.645.\n\n\n\n\n\n\nFigure 12: Graphical displays for standard normal examples\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Standard Normal Calculations:\"\n\n\n                  calculation       result\n                       &lt;char&gt;        &lt;num&gt;\n1:                P(Z &gt; 1.26) 1.038347e-01\n2:               P(Z &lt; -0.86) 1.948945e-01\n3:               P(Z &gt; -1.37) 9.146565e-01\n4:        P(-1.25 &lt; Z &lt; 0.37) 5.386590e-01\n5:               P(Z &lt;= -4.6) 2.112455e-06\n6:      z for P(Z &gt; z) = 0.05 1.644854e+00\n7: z for P(-z &lt; Z &lt; z) = 0.99 2.575829e+00\n\n\n\n\n\n# Example 3-5: Standard Normal Calculations\nstandard_normal_calcs &lt;- data.table(\n  calculation = c(\n    \"P(Z &gt; 1.26)\", \"P(Z &lt; -0.86)\", \"P(Z &gt; -1.37)\",\n    \"P(-1.25 &lt; Z &lt; 0.37)\", \"P(Z &lt;= -4.6)\", \"z for P(Z &gt; z) = 0.05\",\n    \"z for P(-z &lt; Z &lt; z) = 0.99\"\n  )\n) %&gt;%\n  fmutate(\n    result = case_when(\n      calculation == \"P(Z &gt; 1.26)\" ~ pnorm(1.26, lower.tail = FALSE),\n      calculation == \"P(Z &lt; -0.86)\" ~ pnorm(-0.86),\n      calculation == \"P(Z &gt; -1.37)\" ~ pnorm(-1.37, lower.tail = FALSE),\n      calculation == \"P(-1.25 &lt; Z &lt; 0.37)\" ~ pnorm(0.37) - pnorm(-1.25),\n      calculation == \"P(Z &lt;= -4.6)\" ~ pnorm(-4.6),\n      calculation == \"z for P(Z &gt; z) = 0.05\" ~ qnorm(0.05, lower.tail = FALSE),\n      calculation == \"z for P(-z &lt; Z &lt; z) = 0.99\" ~ qnorm(0.995),\n      TRUE ~ NA_real_\n    )\n  )\n\nprint(\"Standard Normal Calculations:\")\nprint(standard_normal_calcs)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-6: Shaft Diameter Quality Control\n\n\n\nThe diameter of a shaft is normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. The specifications are 0.2500 \\pm 0.0015 inch. What proportion of shafts conforms to specifications?\nSolution: We need P(0.2485 &lt; X &lt; 0.2515) where X \\sim N(0.2508, 0.0005^2).\nStandardizing: \n\\small\n\\begin{aligned}\nP(0.2485 &lt; X &lt; 0.2515) &= P\\left(\\frac{0.2485 - 0.2508}{0.0005} &lt; Z &lt; \\frac{0.2515 - 0.2508}{0.0005}\\right) \\\\\n&= P(-4.6 &lt; Z &lt; 1.4) \\\\\n&= P(Z &lt; 1.4) - P(Z &lt; -4.6) \\\\\n&= 0.91924 - 0.00000 = 0.91924\n\\end{aligned}\n\nInterpretation: About 91.9% of shafts meet specifications.\n\n\n\n\n\n\nFigure 13: Normal distribution for shaft diameter example\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Shaft Diameter Quality Control:\"\n\n\n       mu sigma lower_spec upper_spec z_lower z_upper prob_meets_specs\n    &lt;num&gt; &lt;num&gt;      &lt;num&gt;      &lt;num&gt;   &lt;num&gt;   &lt;num&gt;            &lt;num&gt;\n1: 0.2508 5e-04     0.2485     0.2515    -4.6     1.4        0.9192412\n   prob_direct\n         &lt;num&gt;\n1:   0.9192412\n\n\n\n\n\n# Example 3-6: Shaft Diameter Quality Control\nshaft_data &lt;- data.table(\n  mu = 0.2508,\n  sigma = 0.0005,\n  lower_spec = 0.2485,\n  upper_spec = 0.2515\n) %&gt;%\n  fmutate(\n    # Standardize the specification limits\n    z_lower = (lower_spec - mu) / sigma,\n    z_upper = (upper_spec - mu) / sigma,\n    # Calculate probability\n    prob_meets_specs = pnorm(z_upper) - pnorm(z_lower),\n    # Direct calculation\n    prob_direct = pnorm(upper_spec, mean = mu, sd = sigma) -\n      pnorm(lower_spec, mean = mu, sd = sigma)\n  )\n\nprint(\"Shaft Diameter Quality Control:\")\nprint(shaft_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLognormal Distribution\n\n\n\nIf W \\sim N(\\theta, \\omega^2), then X = \\exp(W) has a lognormal distribution.\nProbability Density Function: f(x) = \\frac{1}{x\\sqrt{2\\pi\\omega^2}} \\exp\\left\\{-\\frac{1}{2}\\left(\\frac{\\ln(x) - \\theta}{\\omega}\\right)^2\\right\\}\nfor 0 &lt; x &lt; \\infty.\nParameters:\n\n\\theta: location parameter of underlying normal\n\\omega &gt; 0: scale parameter of underlying normal\n\nMean and Variance:\n\\begin{aligned}\nE(X) &= \\exp\\left(\\theta + \\frac{\\omega^2}{2}\\right) \\\\\nV(X) &= \\exp(2\\theta + \\omega^2)\\left[\\exp(\\omega^2) - 1\\right]\n\\end{aligned}\nApplications: Lifetimes, financial data, environmental measurements\n\n\n\n\n\n\nFigure 14: Lognormal probability density functions with \\theta = 0 for selected values of \\omega^2\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-7: Semiconductor Laser Lifetime\n\n\n\nThe lifetime of a semiconductor laser has a lognormal distribution with \\theta = 10 and \\omega = 1.5 hours.\nProblem 1: What is the probability the lifetime exceeds 10,000 hours?\nSolution: Let W \\sim N(10, 1.5^2), so X = \\exp(W).\n\\begin{aligned}\nP(X &gt; 10,000) &= P(\\exp(W) &gt; 10,000) \\\\\n&= P(W &gt; \\ln(10,000)) \\\\\n&= P\\left(\\frac{W - 10}{1.5} &gt; \\frac{\\ln(10,000) - 10}{1.5}\\right) \\\\\n&= P\\left(Z &gt; \\frac{9.2103 - 10}{1.5}\\right) \\\\\n&= P(Z &gt; -0.5264) = 0.7009\n\\end{aligned}\nProblem 2: What lifetime is exceeded by 99% of lasers?\nSolution: We need x such that P(X &gt; x) = 0.99, or P(X \\leq x) = 0.01.\nFrom P(\\ln(X) \\leq \\ln(x)) = 0.01 and \\ln(X) \\sim N(10, 1.5^2):\nP\\left(\\frac{\\ln(X) - 10}{1.5} \\leq \\frac{\\ln(x) - 10}{1.5}\\right) = 0.01\nSince P(Z \\leq -2.326) = 0.01: \\frac{\\ln(x) - 10}{1.5} = -2.326 x = \\exp(10 - 1.5 \\times 2.326) = \\exp(6.511) = 673.6 \\text{ hours}\n\nR OutputR Code\n\n\n\n\n[1] \"Lognormal Distribution Analysis:\"\n\n\n   theta omega test_value prob_exceeds_10000 percentile_99 theoretical_mean\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt;              &lt;num&gt;         &lt;num&gt;            &lt;num&gt;\n1:    10   1.5      10000          0.7007086      672.1478         67846.29\n   theoretical_variance theoretical_sd\n                  &lt;num&gt;          &lt;num&gt;\n1:          39070059887       197661.5\n\n\n\n\n\n# Example 3-7: Semiconductor Laser Lifetime (Lognormal)\nlognormal_data &lt;- data.table(\n  theta = 10,\n  omega = 1.5,\n  test_value = 10000\n) %&gt;%\n  fmutate(\n    # Probability calculations\n    prob_exceeds_10000 = plnorm(test_value, meanlog = theta, sdlog = omega, lower.tail = FALSE),\n    # Find 99th percentile (exceeded by 99%)\n    percentile_99 = qlnorm(0.01, meanlog = theta, sdlog = omega),\n    # Mean and variance\n    theoretical_mean = exp(theta + omega^2 / 2),\n    theoretical_variance = exp(2 * theta + omega^2) * (exp(omega^2) - 1),\n    theoretical_sd = sqrt(theoretical_variance)\n  )\n\nprint(\"Lognormal Distribution Analysis:\")\nprint(lognormal_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteGamma Distribution\n\n\n\nThe random variable X has a gamma distribution with shape parameter r &gt; 0 and rate parameter \\lambda &gt; 0 if:\nProbability Density Function:\nf(x) = \\frac{\\lambda^r x^{r-1} \\exp(-\\lambda x)}{\\Gamma(r)}, \\quad x &gt; 0\nwhere \\Gamma(r) = \\int_0^{\\infty} t^{r-1} e^{-t} dt is the gamma function.\nProperties of Gamma Function:\n\n\\Gamma(r) = (r-1)! for positive integers r\n\\Gamma(r) = (r-1)\\Gamma(r-1) for r &gt; 1\n\\Gamma(1/2) = \\sqrt{\\pi}\n\nMean and Variance: \\begin{aligned}\nE(X) &= \\frac{r}{\\lambda} \\\\\nV(X) &= \\frac{r}{\\lambda^2}\n\\end{aligned}\nSpecial Cases:\n\nr = 1: Exponential distribution\nr = \\nu/2, \\lambda = 1/2: Chi-square distribution with \\nu degrees of freedom\n\n\n\n\n\n\n\nFigure 15: Gamma probability density functions for selected values of \\lambda and r\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteWeibull Distribution\n\n\n\nThe Weibull distribution is widely used in reliability engineering and survival analysis.\nProbability Density Function: f(x) = \\frac{\\beta}{\\delta} \\left(\\frac{x}{\\delta}\\right)^{\\beta-1} \\exp\\left\\{-\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\}, \\quad x &gt; 0\nCumulative Distribution Function: F(x) = 1 - \\exp\\left\\{-\\left(\\frac{x}{\\delta}\\right)^{\\beta}\\right\\}\nParameters:\n\n\\delta &gt; 0: scale parameter\n\\beta &gt; 0: shape parameter\n\nMean and Variance: \\begin{aligned}\nE(X) &= \\delta \\Gamma\\left(1 + \\frac{1}{\\beta}\\right) \\\\\nV(X) &= \\delta^2 \\left[\\Gamma\\left(1 + \\frac{2}{\\beta}\\right) - \\left\\{\\Gamma\\left(1 + \\frac{1}{\\beta}\\right)\\right\\}^2\\right]\n\\end{aligned}\nSpecial Cases:\n\n\\beta = 1: Exponential distribution\n\\beta = 2: Rayleigh distribution\n\n\n\n\n\n\n\nFigure 16: Weibull probability density functions for selected values of \\delta and \\beta\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-8: Bearing Failure Time\n\n\n\nThe time to failure (in hours) of a bearing follows a Weibull distribution with \\beta = 0.5 and \\delta = 5000 hours.\nProblem 1: Determine the mean time until failure.\nSolution: \\begin{aligned}\nE(X) &= \\delta \\Gamma\\left(1 + \\frac{1}{\\beta}\\right) \\\\\n&= 5000 \\times \\Gamma(1 + 2) \\\\\n&= 5000 \\times \\Gamma(3) \\\\\n&= 5000 \\times 2! = 10,000 \\text{ hours}\n\\end{aligned}\nProblem 2: Determine the probability that a bearing lasts at least 6000 hours.\nSolution: \\begin{aligned}\nP(X &gt; 6000) &= 1 - F(6000) \\\\\n&= \\exp\\left\\{-\\left(\\frac{6000}{5000}\\right)^{0.5}\\right\\} \\\\\n&= \\exp(-1.2^{0.5}) \\\\\n&= \\exp(-1.095) = 0.334\n\\end{aligned}\nInterpretation: Only 33.4% of bearings last at least 6000 hours.\n\nR OutputR Code\n\n\n\n\n[1] \"Weibull Distribution Analysis:\"\n\n\n   shape scale test_time mean_failure_time prob_lasts_6000 variance       sd\n   &lt;num&gt; &lt;num&gt;     &lt;num&gt;             &lt;num&gt;           &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:   0.5  5000      6000             10000       0.3343907    5e+08 22360.68\n\n\n\n\n\n# Example 3-8: Bearing Failure Time (Weibull)\nweibull_data &lt;- data.table(\n  shape = 0.5,\n  scale = 5000,\n  test_time = 6000\n) %&gt;%\n  fmutate(\n    # Mean time until failure\n    mean_failure_time = scale * gamma(1 + 1 / shape),\n    # Probability of lasting at least 6000 hours\n    prob_lasts_6000 = pweibull(test_time, shape = shape, scale = scale, lower.tail = FALSE),\n    # Variance calculation\n    variance = scale^2 * (gamma(1 + 2 / shape) - (gamma(1 + 1 / shape))^2),\n    sd = sqrt(variance)\n  )\n\nprint(\"Weibull Distribution Analysis:\")\nprint(weibull_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteBeta Distribution\n\n\n\nThe beta distribution is defined on the interval [0, 1] and is useful for modeling proportions and percentages.\nProbability Density Function:\nf(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha-1} (1-x)^{\\beta-1}, \\quad 0 \\leq x \\leq 1\nParameters:\n\n\\alpha &gt; 0: shape parameter\n\\beta &gt; 0: shape parameter\n\nMean and Variance:\n\\begin{aligned}\nE(X) &= \\frac{\\alpha}{\\alpha + \\beta} \\\\\nV(X) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\n\\end{aligned}\nSpecial Cases:\n\n\\alpha = \\beta = 1: Uniform distribution on [0, 1]\n\\alpha = 1, \\beta &gt; 1: Decreasing function\n\\alpha &gt; 1, \\beta = 1: Increasing function\n\\alpha = \\beta &gt; 1: Symmetric and unimodal\n\nApplications: Quality control, project completion times, proportions\n\n\n\n\n\n\nFigure 17: Beta probability density functions for selected values of \\alpha and \\beta\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-9: Project Completion Time\n\n\n\nThe proportion of maximum allowed time to complete a task follows a beta distribution with \\alpha = 2.5 and \\beta = 1.\nProblem 1: What is the probability that the proportion exceeds 0.7?\nSolution: \\begin{aligned}\nP(X &gt; 0.7) &= \\int_{0.7}^{1} \\frac{\\Gamma(2.5 + 1)}{\\Gamma(2.5)\\Gamma(1)} x^{2.5-1} (1-x)^{1-1} dx \\\\\n&= \\int_{0.7}^{1} \\frac{\\Gamma(3.5)}{\\Gamma(2.5)} x^{1.5} dx \\\\\n&= \\frac{2.5}{1} \\int_{0.7}^{1} x^{1.5} dx \\\\\n&= 2.5 \\left[\\frac{x^{2.5}}{2.5}\\right]_{0.7}^{1} \\\\\n&= (1^{2.5} - 0.7^{2.5}) = 1 - 0.7^{2.5} = 0.59\n\\end{aligned}\nProblem 2: Calculate the mean and variance.\nSolution: \\begin{aligned}\nE(X) &= \\frac{\\alpha}{\\alpha + \\beta} = \\frac{2.5}{2.5 + 1} = \\frac{2.5}{3.5} = 0.714 \\\\\nV(X) &= \\frac{\\alpha\\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)} \\\\\n&= \\frac{2.5 \\times 1}{(3.5)^2 \\times 4.5} = \\frac{2.5}{55.125} = 0.045\n\\end{aligned}\n\nR OutputR Code\n\n\n\n\n[1] \"Beta Distribution Analysis:\"\n\n\n   alpha beta_param test_value prob_exceeds_07 mean_beta variance_beta\n   &lt;num&gt;      &lt;num&gt;      &lt;num&gt;           &lt;num&gt;     &lt;num&gt;         &lt;num&gt;\n1:   2.5          1        0.7       0.5900366 0.7142857    0.04535147\n     sd_beta\n       &lt;num&gt;\n1: 0.2129589\n\n\n\n\n\n# Example 3-9: Project Completion Time (Beta)\nbeta_data &lt;- data.table(\n  alpha = 2.5,\n  beta_param = 1,\n  test_value = 0.7\n) %&gt;%\n  fmutate(\n    # Probability exceeds 0.7\n    prob_exceeds_07 = pbeta(test_value, shape1 = alpha, shape2 = beta_param, lower.tail = FALSE),\n    # Mean and variance\n    mean_beta = alpha / (alpha + beta_param),\n    variance_beta = (alpha * beta_param) / ((alpha + beta_param)^2 * (alpha + beta_param + 1)),\n    sd_beta = sqrt(variance_beta)\n  )\n\nprint(\"Beta Distribution Analysis:\")\nprint(beta_data)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#probability-plots",
    "href": "book/Ch03.html#probability-plots",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteNormal Probability Plots\n\n\n\nA normal probability plot is a graphical method for determining whether sample data conform to a hypothesized normal distribution. The procedure is:\n\nOrder the sample data from smallest to largest: x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\nPlot the pairs (x_{(i)}, \\Phi^{-1}((i-0.5)/n)) for i = 1, 2, \\ldots, n\nIf the data are from a normal distribution, the plotted points will approximately follow a straight line\n\nInterpretation:\n\nStraight line pattern: Data consistent with normal distribution\nCurved pattern: Data may follow a different distribution\nOutliers: Points that deviate significantly from the line\n\nAdvantages:\n\nVisual assessment of normality\nIdentifies outliers\nSuggests alternative distributions\n\n\n\n\n\n\n\n\n\nTipExample 3-10: Battery Life Normal Probability Plot\n\n\n\nConsider battery life data (in hours): 176, 191, 214, 220, 205, 192, 201, 190, 183, 185.\nThe normal probability plot helps assess whether these data could reasonably come from a normal distribution.\n\nR OutputR Code\n\n\n\n\n[1] \"Battery Life Data for Normal Probability Plot:\"\n\n\n     life sample_mean sample_sd sample_size  rank theoretical_quantile\n    &lt;num&gt;       &lt;num&gt;     &lt;num&gt;       &lt;int&gt; &lt;num&gt;                &lt;num&gt;\n 1:   176       195.7  14.03211          10     1           -1.6448536\n 2:   183       195.7  14.03211          10     2           -1.0364334\n 3:   185       195.7  14.03211          10     3           -0.6744898\n 4:   190       195.7  14.03211          10     4           -0.3853205\n 5:   191       195.7  14.03211          10     5           -0.1256613\n 6:   192       195.7  14.03211          10     6            0.1256613\n 7:   201       195.7  14.03211          10     7            0.3853205\n 8:   205       195.7  14.03211          10     8            0.6744898\n 9:   214       195.7  14.03211          10     9            1.0364334\n10:   220       195.7  14.03211          10    10            1.6448536\n    fitted_value\n           &lt;num&gt;\n 1:     172.6192\n 2:     181.1567\n 3:     186.2355\n 4:     190.2931\n 5:     193.9367\n 6:     197.4633\n 7:     201.1069\n 8:     205.1645\n 9:     210.2433\n10:     218.7808\n\n\n[1] \"Shapiro-Wilk p-value: 0.7173\"\n\n\n\n\n\n# Example 3-10: Normal Probability Plot\nbattery_life &lt;- data.table(\n  life = c(176, 191, 214, 220, 205, 192, 201, 190, 183, 185)\n) %&gt;%\n  fmutate(\n    sample_mean = fmean(life),\n    sample_sd = fsd(life),\n    sample_size = fnobs(life)\n  )\n\n# Create normal probability plot data\nbattery_sorted &lt;- battery_life %&gt;%\n  fmutate(rank = frank(life, ties.method = \"average\")) %&gt;%\n  fmutate(\n    theoretical_quantile = qnorm((rank - 0.5) / sample_size),\n    fitted_value = sample_mean + sample_sd * theoretical_quantile\n  ) %&gt;%\n  roworder(life)\n\nprint(\"Battery Life Data for Normal Probability Plot:\")\nprint(battery_sorted)\n\n# Test for normality using Shapiro-Wilk\nshapiro_test &lt;- shapiro.test(battery_life$life)\nprint(paste(\"Shapiro-Wilk p-value:\", round(shapiro_test$p.value, 4)))\n\nfwrite(battery_sorted, \"data/battery_normal_plot.csv\")\n\n\n\n\nInterpretation: If the points roughly follow a straight line, the normal distribution is a reasonable model for the data.\n\n\n\n\n\n\n\n\n\n\n\nNoteOther Probability Plots\n\n\n\nThe same probability plotting technique can be used with any distribution:\nLognormal Probability Plot:\n\nPlot (x_{(i)}, \\Phi^{-1}((i-0.5)/n)) where the x-axis uses a log scale\nOr plot (\\ln(x_{(i)}), \\Phi^{-1}((i-0.5)/n)) on normal scales\n\nWeibull Probability Plot:\n\nPlot (\\ln(x_{(i)}), \\ln(-\\ln(1-(i-0.5)/n)))\nStraight line indicates Weibull distribution\n\nExponential Probability Plot:\n\nPlot (x_{(i)}, -\\ln(1-(i-0.5)/n))\nStraight line indicates exponential distribution\n\nGeneral Approach:\n\nTransform data using the inverse CDF of the hypothesized distribution\nPlot against order statistics\nAssess linearity\n\n\n\n\n\n\n\n\n\nTipExample 3-11: Lognormal Probability Plot\n\n\n\nConsider failure time data that might follow a lognormal distribution. The lognormal probability plot helps determine if this distribution is appropriate.\n\nR OutputR Code\n\n\n\n\n[1] \"Lognormal Distribution Fit:\"\n\n\n    time  ln_time sample_size meanlog_est sdlog_est\n   &lt;num&gt;    &lt;num&gt;       &lt;int&gt;       &lt;num&gt;     &lt;num&gt;\n1:    81 4.394449          50    4.966551 0.4756993\n\n\n[1] \"KS test p-value for lognormal: 0.8849\"\n\n\n\n\n\n# Example 3-11: Lognormal Probability Plot\n# Simulate some failure time data that might be lognormal\nset.seed(42)\nfailure_times &lt;- data.table(\n  time = c(\n    81, 249, 117, 227, 134, 98, 135, 149, 225, 59,\n    291, 223, 127, 185, 181, 101, 205, 115, 240, 151,\n    98, 80, 198, 161, 240, 118, 177, 342, 197, 146,\n    158, 82, 83, 98, 104, 197, 64, 34, 65, 100,\n    139, 137, 342, 144, 215, 249, 149, 185, 151, 200\n  )\n) %&gt;%\n  fmutate(\n    ln_time = log(time),\n    sample_size = fnobs(time)\n  )\n\n# Fit lognormal distribution\nlognormal_fit &lt;- failure_times %&gt;%\n  fmutate(\n    meanlog_est = fmean(ln_time),\n    sdlog_est = fsd(ln_time)\n  )\n\nprint(\"Lognormal Distribution Fit:\")\nprint(lognormal_fit[1])\n\n# Test goodness of fit\nks_test_lognormal &lt;- ks.test(failure_times$time, \"plnorm\",\n  meanlog = lognormal_fit$meanlog_est[1],\n  sdlog = lognormal_fit$sdlog_est[1]\n)\nprint(paste(\"KS test p-value for lognormal:\", round(ks_test_lognormal$p.value, 4)))\n\nfwrite(failure_times, \"data/failure_times.csv\")",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#discrete-random-variables",
    "href": "book/Ch03.html#discrete-random-variables",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteDiscrete Random Variables\n\n\n\nA discrete random variable can take on only a finite or countably infinite number of distinct values. Common examples include:\n\nNumber of defective items in a batch\nNumber of customers arriving per hour\nNumber of successful trials in a sequence\n\nKey Characteristics:\n\nPossible values are usually integers\nProbability is concentrated at specific points\nBetween any two possible values, the probability is zero\n\n\n\n\n\n\n\n\n\n\n\nNoteProbability Mass Function (PMF)\n\n\n\nFor a discrete random variable X with possible values x_1, x_2, \\ldots, x_n, the probability mass function is:\nf(x_i) = P(X = x_i)\nProperties:\n\nf(x_i) \\geq 0 for all x_i\n\\sum_{i=1}^{n} f(x_i) = 1\nf(x) = 0 if x is not a possible value\n\nNotation: Sometimes written as p(x) or P(X = x)\n\n\n\n\n\n\n\n\nTipExample 3-12: Digital Transmission Errors\n\n\n\nLet X equal the number of bits in error in the next 4 bits transmitted. Suppose the probabilities are:\n\nP(X = 0) = 0.6561\nP(X = 1) = 0.2916\n\nP(X = 2) = 0.0486\nP(X = 3) = 0.0036\nP(X = 4) = 0.0001\n\nThis completely specifies the probability distribution of X.\n\n\n\n\n\n\nFigure 18: Probability mass function for transmission errors\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Transmission Error Distribution:\"\n\n\n   errors probability total_prob cumulative_prob\n    &lt;int&gt;       &lt;num&gt;      &lt;num&gt;           &lt;num&gt;\n1:      0      0.6561          1          0.6561\n2:      1      0.2916          1          0.9477\n3:      2      0.0486          1          0.9963\n4:      3      0.0036          1          0.9999\n5:      4      0.0001          1          1.0000\n\n\n\n\n\n# Example 3-12: Digital Transmission Errors (PMF)\ntransmission_data &lt;- data.table(\n  errors = 0:4,\n  probability = c(0.6561, 0.2916, 0.0486, 0.0036, 0.0001)\n) %&gt;%\n  fmutate(\n    # Verify probabilities sum to 1\n    total_prob = fsum(probability),\n    # Calculate cumulative probabilities\n    cumulative_prob = cumsum(probability)\n  )\n\nprint(\"Transmission Error Distribution:\")\nprint(transmission_data)\n\nfwrite(transmission_data, \"data/transmission_errors.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteCumulative Distribution Function for Discrete Variables\n\n\n\nThe cumulative distribution function of a discrete random variable X is:\nF(x) = P(X \\leq x) = \\sum_{x_i \\leq x} f(x_i)\nProperties:\n\nF(x) is a step function\nJumps occur at possible values of X\nJump size equals P(X = x_i)\nF(x) is non-decreasing\n\\lim_{x \\to -\\infty} F(x) = 0 and \\lim_{x \\to \\infty} F(x) = 1\n\nKey Relationship: P(a &lt; X \\leq b) = F(b) - F(a)\n\n\n\n\n\n\n\n\nTipExample 3-13: CDF for Transmission Errors\n\n\n\nContinuing the previous example:\n\nF(0) = P(X \\leq 0) = 0.6561\nF(1) = P(X \\leq 1) = 0.6561 + 0.2916 = 0.9477\nF(2) = P(X \\leq 2) = 0.9477 + 0.0486 = 0.9963\nF(3) = P(X \\leq 3) = 0.9963 + 0.0036 = 0.9999\nF(4) = P(X \\leq 4) = 0.9999 + 0.0001 = 1.0000\n\nNote: F(1.5) = P(X \\leq 1.5) = P(X \\leq 1) = 0.9477 since X cannot equal 1.5.\n\n\n\n\n\n\nFigure 19: Cumulative distribution function for transmission errors\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"CDF for Transmission Errors:\"\n\n\n   errors probability cumulative_prob     prob_statement\n    &lt;int&gt;       &lt;num&gt;           &lt;num&gt;             &lt;char&gt;\n1:      0      0.6561          0.6561 P(X &lt;= 0) = 0.6561\n2:      1      0.2916          0.9477 P(X &lt;= 1) = 0.9477\n3:      2      0.0486          0.9963 P(X &lt;= 2) = 0.9963\n4:      3      0.0036          0.9999 P(X &lt;= 3) = 0.9999\n5:      4      0.0001          1.0000      P(X &lt;= 4) = 1\n\n\n[1] \"CDF at Non-Integer Values:\"\n\n\n       x cdf_value  interpretation\n   &lt;num&gt;     &lt;num&gt;          &lt;char&gt;\n1:   1.5    0.9477 F(1.5) = 0.9477\n2:   2.3    0.9963 F(2.3) = 0.9963\n3:   3.7    0.9999 F(3.7) = 0.9999\n\n\n\n\n\n# Example 3-13: CDF for Transmission Errors\ncdf_transmission &lt;- transmission_data %&gt;%\n  fselect(errors, probability, cumulative_prob) %&gt;%\n  fmutate(\n    prob_statement = paste0(\"P(X &lt;= \", errors, \") = \", round(cumulative_prob, 4))\n  )\n\nprint(\"CDF for Transmission Errors:\")\nprint(cdf_transmission)\n\n# Example of intermediate values\nintermediate_vals &lt;- data.table(\n  x = c(1.5, 2.3, 3.7),\n  cdf_value = c(0.9477, 0.9963, 0.9999)\n) %&gt;%\n  fmutate(\n    interpretation = paste0(\"F(\", x, \") = \", cdf_value)\n  )\n\nprint(\"CDF at Non-Integer Values:\")\nprint(intermediate_vals)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMean and Variance for Discrete Random Variables\n\n\n\nLet X have possible values x_1, x_2, \\ldots, x_n with PMF f(x).\nMean (Expected Value):\n\n\\mu = E(X) = \\sum_{i=1}^{n} x_i f(x_i)\n\nVariance:\n\n\\begin{aligned}\n\\sigma^2 = V(X) &= E[(X - \\mu)^2] \\\\\n&= \\sum_{i=1}^{n} (x_i - \\mu)^2 f(x_i) \\\\\n&= \\sum_{i=1}^{n} x_i^2 f(x_i) - \\mu^2\n\\end{aligned}\n\nStandard Deviation: \\sigma = \\sqrt{V(X)}\nInterpretation:\n\nMean is the weighted average of possible values\nVariance measures spread around the mean\nStandard deviation is in original units\n\n\n\n\n\n\n\n\n\nTipExample 3-14: Mean and Variance Calculation\n\n\n\nFor the transmission error example:\nMean:\n\\begin{aligned}\n\\mu &= E(X) = 0(0.6561) + 1(0.2916) + 2(0.0486) + 3(0.0036) + 4(0.0001) \\\\\n&= 0 + 0.2916 + 0.0972 + 0.0108 + 0.0004 = 0.4\n\\end{aligned}\nVariance:\nFirst, calculate E(X^2):\nE(X^2) = 0^2(0.6561) + 1^2(0.2916) + 2^2(0.0486) + 3^2(0.0036) + 4^2(0.0001) = 0.52\nThen: V(X) = E(X^2) - [E(X)]^2 = 0.52 - (0.4)^2 = 0.52 - 0.16 = 0.36\nStandard Deviation: \\sigma = \\sqrt{0.36} = 0.6\n\nR OutputR Code\n\n\n\n\nError in eval(e, .data, pe): object 'e_x_squared' not found\n\n\n[1] \"Mean and Variance for Transmission Errors:\"\n\n\nError: object 'discrete_stats' not found\n\n\nError in eval(ei, .data, pe): object 'discrete_stats' not found\n\n\n[1] \"Detailed Variance Calculation:\"\n\n\nError: object 'calc_table' not found\n\n\n\n\n\n# Example 3-14: Mean and Variance Calculation for Discrete Distribution\ndiscrete_stats &lt;- transmission_data %&gt;%\n  fmutate(\n    x_times_p = errors * probability,\n    x_squared_times_p = errors^2 * probability\n  ) %&gt;%\n  fsummarise(\n    mean_x = fsum(x_times_p),\n    e_x_squared = fsum(x_squared_times_p),\n    variance_x = e_x_squared - mean_x^2,\n    sd_x = sqrt(variance_x)\n  )\n\nprint(\"Mean and Variance for Transmission Errors:\")\nprint(discrete_stats)\n\n# Create detailed calculation table\ncalc_table &lt;- transmission_data %&gt;%\n  fmutate(\n    x_times_p = errors * probability,\n    x_minus_mu = errors - discrete_stats$mean_x[1],\n    x_minus_mu_squared = x_minus_mu^2,\n    variance_term = x_minus_mu_squared * probability\n  )\n\nprint(\"Detailed Variance Calculation:\")\nprint(calc_table)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-15: Design Choice Problem\n\n\n\nTwo product designs are compared based on revenue potential:\n\nDesign A: Revenue of $3 million with probability 1 (certain)\nDesign B: Revenue of $7 million with probability 0.3, or $2 million with probability 0.7\n\nAnalysis:\nDesign A: E(X_A) = 3 million dollars, V(X_A) = 0\nDesign B: \n\\begin{aligned}\nE(X_B) &= 7(0.3) + 2(0.7) = 2.1 + 1.4 = 3.5 \\text{ million} \\\\\nV(X_B) &= (7-3.5)^2(0.3) + (2-3.5)^2(0.7) \\\\\n&= (3.5)^2(0.3) + (-1.5)^2(0.7) \\\\\n&= 3.675 + 1.575 = 5.25 \\text{ million}^2\n\\end{aligned}\n\nDecision: Design B has higher expected revenue but much higher risk (variance).",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#binomial-distribution",
    "href": "book/Ch03.html#binomial-distribution",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteBinomial Distribution\n\n\n\nA binomial experiment consists of n independent trials, each with:\n\nExactly two possible outcomes (success/failure)\nConstant probability p of success on each trial\nTrials are independent\n\nDefinition: The random variable X that counts the number of successes in n binomial trials has a binomial distribution with parameters n and p.\nProbability Mass Function:\n\nf(x) = P(X = x) = \\binom{n}{x} p^x (1-p)^{n-x}\n\nfor x = 0, 1, 2, \\ldots, n, where \\binom{n}{x} = \\frac{n!}{x!(n-x)!}.\nParameters:\n\nn: number of trials\np: probability of success on each trial\n\nMean and Variance:\n\n\\begin{aligned}\nE(X) &= np \\\\\nV(X) &= np(1-p)\n\\end{aligned}\n\nNotation: X \\sim \\text{Binomial}(n, p) or X \\sim B(n, p)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 20: Binomial distributions for selected values of n and p\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-16: Water Sample Analysis\n\n\n\nEach water sample has a 10% chance of containing high levels of organic solids. Assume samples are independent. For the next 18 samples:\nProblem 1: Probability that exactly 2 contain high solids?\nSolution:\nX \\sim B(18, 0.1)\nP(X = 2) = \\binom{18}{2} (0.1)^2 (0.9)^{16} = 153 \\times 0.01 \\times 0.1853 = 0.284\nProblem 2: Probability that at least 4 samples contain high solids?\nSolution:\n\n\\begin{aligned}\nP(X \\geq 4) &= 1 - P(X \\leq 3) \\\\\n&= 1 - \\sum_{x=0}^{3} \\binom{18}{x} (0.1)^x (0.9)^{18-x} \\\\\n&= 1 - [0.150 + 0.300 + 0.284 + 0.168] = 0.098\n\\end{aligned}\n\nProblem 3: Mean and variance?\nSolution:\n\n\\begin{aligned}\nE(X) &= np = 18 \\times 0.1 = 1.8 \\\\\nV(X) &= np(1-p) = 18 \\times 0.1 \\times 0.9 = 1.62\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Water Sample Binomial Analysis:\"\n\n\n       n     p test_values prob_exactly_2 prob_at_least_4 prob_at_least_4_alt\n   &lt;num&gt; &lt;num&gt;      &lt;list&gt;          &lt;num&gt;           &lt;num&gt;               &lt;num&gt;\n1:    18   0.1         2,4      0.2835121      0.09819684          0.09819684\n   prob_3_to_7 mean_binomial variance_binomial sd_binomial\n         &lt;num&gt;         &lt;num&gt;             &lt;num&gt;       &lt;num&gt;\n1:   0.2660305           1.8              1.62    1.272792\n\n\n[1] \"Binomial Probability Table (first 9 values):\"\n\n\n       x  probability cumulative\n   &lt;int&gt;        &lt;num&gt;      &lt;num&gt;\n1:     0 0.1500946353  0.1500946\n2:     1 0.3001892706  0.4502839\n3:     2 0.2835120889  0.7337960\n4:     3 0.1680071638  0.9018032\n5:     4 0.0700029849  0.9718061\n6:     5 0.0217787064  0.9935848\n7:     6 0.0052430219  0.9988279\n8:     7 0.0009986708  0.9998265\n9:     8 0.0001525747  0.9999791\n\n\n\n\n\n# Example 3-16: Water Sample Analysis (Binomial)\nwater_sample_data &lt;- data.table(\n  n = 18,\n  p = 0.1,\n  test_values = list(c(2, 4))\n) %&gt;%\n  fmutate(\n    # Problem 1: Exactly 2 samples\n    prob_exactly_2 = dbinom(2, size = n, prob = p),\n    # Problem 2: At least 4 samples\n    prob_at_least_4 = 1 - pbinom(3, size = n, prob = p),\n    prob_at_least_4_alt = pbinom(3, size = n, prob = p, lower.tail = FALSE),\n    # Problem 3: Between 3 and 7 (inclusive)\n    prob_3_to_7 = pbinom(7, size = n, prob = p) - pbinom(2, size = n, prob = p),\n    # Mean and variance\n    mean_binomial = n * p,\n    variance_binomial = n * p * (1 - p),\n    sd_binomial = sqrt(variance_binomial)\n  )\n\nprint(\"Water Sample Binomial Analysis:\")\nprint(water_sample_data)\n\n# Create binomial probability table\nbinomial_table &lt;- data.table(\n  x = 0:8\n) %&gt;%\n  fmutate(\n    probability = dbinom(x, size = 18, prob = 0.1),\n    cumulative = pbinom(x, size = 18, prob = 0.1)\n  )\n\nprint(\"Binomial Probability Table (first 9 values):\")\nprint(binomial_table)\n\nfwrite(binomial_table, \"data/binomial_probabilities.csv\")",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#poisson-process",
    "href": "book/Ch03.html#poisson-process",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NotePoisson Process\n\n\n\nA Poisson process models events occurring randomly over time or space. Examples include:\n\nEmail arrivals at a server\nRadioactive decay\nManufacturing defects\nCustomer arrivals\n\nDefinition: A Poisson process with rate \\lambda has these properties:\n\nIndependence: Events in disjoint intervals are independent\nStationarity: Rate is constant over time\nOrdinarity: At most one event occurs in any infinitesimal interval\n\nApplications:\n\nReliability engineering\nQueuing theory\n\nQuality control\nNetwork traffic modeling\n\n\n\n\n\n\n\nFigure 21: Events occurring randomly in a Poisson process\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotePoisson Distribution\n\n\n\nIf events occur according to a Poisson process with rate \\lambda, then the number of events X in a unit interval has a Poisson distribution.\nProbability Mass Function:\n\nf(x) = P(X = x) = \\frac{\\lambda^x e^{-\\lambda}}{x!}\n\nfor x = 0, 1, 2, \\ldots\nParameter:\n\n\\lambda &gt; 0: average number of events per unit interval\n\nMean and Variance:\n\nE(X) = V(X) = \\lambda\n\nKey Property: Mean equals variance (equidispersion)\nNotation: X \\sim \\text{Poisson}(\\lambda)\n\n\n\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\nFigure 22: Poisson distributions for selected values of \\lambda\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-17: Copper Wire Flaws\n\n\n\nFlaws in copper wire follow a Poisson distribution with mean 2.3 flaws per millimeter.\nProblem 1: Probability of exactly 2 flaws in 1 millimeter?\nSolution: X \\sim \\text{Poisson}(2.3), P(X = 2) = \\frac{e^{-2.3} \\times 2.3^2}{2!} = \\frac{e^{-2.3} \\times 5.29}{2} = 0.265\nProblem 2: Probability of 10 flaws in 5 millimeters?\nSolution: For 5 mm, \\lambda = 5 \\times 2.3 = 11.5 P(X = 10) = \\frac{e^{-11.5} \\times 11.5^{10}}{10!} = 0.113\n\nR OutputR Code\n\n\n\n\n[1] \"Poisson Distribution Analysis:\"\n\n\n   lambda_per_mm test_scenarios prob_2_in_1mm lambda_5mm prob_10_in_5mm\n           &lt;num&gt;         &lt;list&gt;         &lt;num&gt;      &lt;num&gt;          &lt;num&gt;\n1:           2.3            1,5     0.2651846       11.5      0.1129351\n   mean_1mm variance_1mm mean_5mm variance_5mm\n      &lt;num&gt;        &lt;num&gt;    &lt;num&gt;        &lt;num&gt;\n1:      2.3          2.3     11.5         11.5\n\n\n[1] \"Poisson Table for 1mm (lambda = 2.3):\"\n\n\n       x probability cumulative\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;\n1:     0  0.10025884  0.1002588\n2:     1  0.23059534  0.3308542\n3:     2  0.26518464  0.5960388\n4:     3  0.20330823  0.7993471\n5:     4  0.11690223  0.9162493\n6:     5  0.05377503  0.9700243\n\n\n\n\n\n# Example 3-17: Copper Wire Flaws (Poisson)\npoisson_data &lt;- data.table(\n  lambda_per_mm = 2.3,\n  test_scenarios = list(c(1, 5)) # 1 mm and 5 mm\n) %&gt;%\n  fmutate(\n    # Problem 1: Exactly 2 flaws in 1 mm\n    prob_2_in_1mm = dpois(2, lambda = lambda_per_mm),\n    # Problem 2: 10 flaws in 5 mm\n    lambda_5mm = lambda_per_mm * 5,\n    prob_10_in_5mm = dpois(10, lambda = lambda_5mm),\n    # Additional calculations\n    mean_1mm = lambda_per_mm,\n    variance_1mm = lambda_per_mm,\n    mean_5mm = lambda_5mm,\n    variance_5mm = lambda_5mm\n  )\n\nprint(\"Poisson Distribution Analysis:\")\nprint(poisson_data)\n\n# Create Poisson probability tables\npoisson_1mm &lt;- data.table(x = 0:10) %&gt;%\n  fmutate(\n    probability = dpois(x, lambda = 2.3),\n    cumulative = ppois(x, lambda = 2.3)\n  )\n\npoisson_5mm &lt;- data.table(x = 0:20) %&gt;%\n  fmutate(\n    probability = dpois(x, lambda = 11.5),\n    cumulative = ppois(x, lambda = 11.5)\n  )\n\nprint(\"Poisson Table for 1mm (lambda = 2.3):\")\nprint(head(poisson_1mm))\n\nfwrite(poisson_1mm, \"data/poisson_1mm.csv\")\nfwrite(poisson_5mm, \"data/poisson_5mm.csv\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteExponential Distribution\n\n\n\nThe exponential distribution models the time between events in a Poisson process.\nProbability Density Function:\nf(x) = \\lambda e^{-\\lambda x}, \\quad x \\geq 0\nCumulative Distribution Function:\nF(x) = 1 - e^{-\\lambda x}, \\quad x \\geq 0\nParameter:\n\n\\lambda &gt; 0: rate parameter (events per unit time)\n\nMean and Variance:\n\n\\begin{aligned}\nE(X) &= \\frac{1}{\\lambda} \\\\\nV(X) &= \\frac{1}{\\lambda^2}\n\\end{aligned}\n\nMemoryless Property: P(X &gt; s + t | X &gt; s) = P(X &gt; t)\nApplications: Reliability analysis, queuing systems, survival analysis\n\n\n\n\n\n\nFigure 23: Exponential probability density functions for selected values of \\lambda\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-18: Computer Network Log-ons\n\n\n\nUser log-ons to a system follow a Poisson process with mean 25 log-ons per hour.\nProblem: Probability of no log-ons in a 6-minute interval?\nSolution: Let X = time until first log-on (in hours). Then X \\sim \\text{Exponential}(25).\nWe want P(X &gt; 0.1) since 6 minutes = 0.1 hour.\nP(X &gt; 0.1) = e^{-25 \\times 0.1} = e^{-2.5} = 0.082\nAlternative approach: Number of events in 0.1 hour ~ Poisson(2.5) P(\\text{0 events}) = \\frac{e^{-2.5} \\times 2.5^0}{0!} = e^{-2.5} = 0.082\n\nR OutputR Code\n\n\n\n\n[1] \"Network Log-on Analysis:\"\n\n\n   lambda_per_hour time_interval_minutes time_interval_hours prob_no_logons\n             &lt;num&gt;                 &lt;num&gt;               &lt;num&gt;          &lt;num&gt;\n1:              25                     6                 0.1       0.082085\n   prob_no_logons_alt expected_events prob_0_events_poisson mean_time_between\n                &lt;num&gt;           &lt;num&gt;                 &lt;num&gt;             &lt;num&gt;\n1:           0.082085             2.5              0.082085              0.04\n   mean_time_minutes\n               &lt;num&gt;\n1:               2.4\n\n\n[1] \"Exponential Probabilities for Various Times:\"\n\n\n   time_hours  prob_exceed prob_within time_minutes\n        &lt;num&gt;        &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:        0.1 8.208500e-02   0.9179150            6\n2:        0.2 6.737947e-03   0.9932621           12\n3:        0.5 3.726653e-06   0.9999963           30\n4:        1.0 1.388794e-11   1.0000000           60\n\n\n\n\n\n# Example 3-18: Computer Network Log-ons (Exponential)\nnetwork_data &lt;- data.table(\n  lambda_per_hour = 25,\n  time_interval_minutes = 6\n) %&gt;%\n  fmutate(\n    time_interval_hours = time_interval_minutes / 60,\n    # Probability of no log-ons in 6 minutes\n    prob_no_logons = pexp(time_interval_hours, rate = lambda_per_hour, lower.tail = FALSE),\n    prob_no_logons_alt = exp(-lambda_per_hour * time_interval_hours),\n    # Verification using Poisson (events in interval)\n    expected_events = lambda_per_hour * time_interval_hours,\n    prob_0_events_poisson = dpois(0, lambda = expected_events),\n    # Mean time between log-ons\n    mean_time_between = 1 / lambda_per_hour,\n    # Convert to minutes\n    mean_time_minutes = mean_time_between * 60\n  )\n\nprint(\"Network Log-on Analysis:\")\nprint(network_data)\n\n# Additional exponential calculations\nexp_calculations &lt;- data.table(\n  time_hours = c(0.1, 0.2, 0.5, 1.0)\n) %&gt;%\n  fmutate(\n    prob_exceed = pexp(time_hours, rate = 25, lower.tail = FALSE),\n    prob_within = pexp(time_hours, rate = 25),\n    time_minutes = time_hours * 60\n  )\n\nprint(\"Exponential Probabilities for Various Times:\")\nprint(exp_calculations)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#normal-approximation-to-the-binomial-and-poisson-distributions",
    "href": "book/Ch03.html#normal-approximation-to-the-binomial-and-poisson-distributions",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteNormal Approximation to Binomial\n\n\n\nWhen n is large, the binomial distribution approaches normality due to the Central Limit Theorem.\nApproximation: If X \\sim B(n, p), then for large n: Z = \\frac{X - np}{\\sqrt{np(1-p)}} \\approx N(0, 1)\nRule of Thumb: Use when np &gt; 5 and n(1-p) &gt; 5\nContinuity Correction: Since we’re approximating discrete with continuous:\n\nP(X = k) \\approx P(k - 0.5 &lt; Y &lt; k + 0.5)\nP(X \\leq k) \\approx P(Y \\leq k + 0.5)\nP(X \\geq k) \\approx P(Y \\geq k - 0.5)\n\nwhere Y \\sim N(np, np(1-p)).\n\n\n\n\n\n\nFigure 24: Normal approximation to binomial distribution\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-19: Normal Approximation to Binomial\n\n\n\nFor n = 50 bits transmitted with error probability p = 0.1:\nExact calculation: P(X \\leq 2) = \\sum_{x=0}^{2} \\binom{50}{x} (0.1)^x (0.9)^{50-x} = 0.1117\nNormal approximation: X \\sim B(50, 0.1) approximately N(5, 4.5)\nWith continuity correction:\n\n\\begin{aligned}\nP(X \\leq 2) &\\approx P\\left(Z \\leq \\frac{2.5 - 5}{\\sqrt{4.5}}\\right) \\\\\n&= P(Z \\leq -1.18) = 0.119\n\\end{aligned}\n\nComparison: Exact = 0.1117, Approximation = 0.119 (very close!)\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Approximation to Binomial:\"\n\n\n       n     p test_value    np    nq conditions_met exact_prob mu_approx\n   &lt;num&gt; &lt;num&gt;      &lt;num&gt; &lt;num&gt; &lt;num&gt;         &lt;lgcl&gt;      &lt;num&gt;     &lt;num&gt;\n1:    50   0.1          2     5    45          FALSE  0.1117288         5\n   sigma_approx z_score_no_cc approx_no_cc z_score_with_cc approx_with_cc\n          &lt;num&gt;         &lt;num&gt;        &lt;num&gt;           &lt;num&gt;          &lt;num&gt;\n1:      2.12132     -1.414214    0.0786496       -1.178511      0.1192964\n   error_no_cc error_with_cc\n         &lt;num&gt;         &lt;num&gt;\n1:  0.03307915   0.007567658\n\n\n[1] \"Comparison Table (first 9 values):\"\n\n\n       x       exact  approx_cc       error\n   &lt;int&gt;       &lt;num&gt;      &lt;num&gt;       &lt;num&gt;\n1:     0 0.005153775 0.01694743 0.011793652\n2:     1 0.033785860 0.04948008 0.015694217\n3:     2 0.111728756 0.11929641 0.007567658\n4:     3 0.250293906 0.23975006 0.010543845\n5:     4 0.431198407 0.40683186 0.024366549\n6:     5 0.616123008 0.59316814 0.022954866\n7:     6 0.770226842 0.76024994 0.009976903\n8:     7 0.877854916 0.88070359 0.002848669\n9:     8 0.942132794 0.95051992 0.008387129\n\n\n\n\n\n# Example 3-19: Normal Approximation to Binomial\nnormal_approx_data &lt;- data.table(\n  n = 50,\n  p = 0.1,\n  test_value = 2\n) %&gt;%\n  fmutate(\n    # Check conditions for normal approximation\n    np = n * p,\n    nq = n * (1 - p),\n    conditions_met = (np &gt; 5) & (nq &gt; 5),\n    # Exact binomial calculation\n    exact_prob = pbinom(test_value, size = n, prob = p),\n    # Normal approximation parameters\n    mu_approx = np,\n    sigma_approx = sqrt(np * (1 - p)),\n    # Normal approximation without continuity correction\n    z_score_no_cc = (test_value - mu_approx) / sigma_approx,\n    approx_no_cc = pnorm(z_score_no_cc),\n    # Normal approximation with continuity correction\n    z_score_with_cc = (test_value + 0.5 - mu_approx) / sigma_approx,\n    approx_with_cc = pnorm(z_score_with_cc),\n    # Error calculations\n    error_no_cc = abs(exact_prob - approx_no_cc),\n    error_with_cc = abs(exact_prob - approx_with_cc)\n  )\n\nprint(\"Normal Approximation to Binomial:\")\nprint(normal_approx_data)\n\n# Compare multiple values\ncomparison_table &lt;- data.table(x = 0:8) %&gt;%\n  fmutate(\n    exact = pbinom(x, size = 50, prob = 0.1),\n    approx_cc = pnorm((x + 0.5 - 5) / sqrt(4.5)),\n    error = abs(exact - approx_cc)\n  )\n\nprint(\"Comparison Table (first 9 values):\")\nprint(comparison_table)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteNormal Approximation to Poisson\n\n\n\nFor large \\lambda, the Poisson distribution approaches normality.\nApproximation: If X \\sim \\text{Poisson}(\\lambda), then for large \\lambda: Z = \\frac{X - \\lambda}{\\sqrt{\\lambda}} \\approx N(0, 1)\nRule of Thumb: Use when \\lambda &gt; 5\nContinuity Correction: Apply the same corrections as for binomial approximation.\n\n\n\n\n\n\n\n\nTipExample 3-20: Normal Approximation to Poisson\n\n\n\nContamination particles in water follow Poisson(1000). Find P(X &lt; 950).\nExact: P(X &lt; 950) = \\sum_{x=0}^{949} \\frac{e^{-1000} \\times 1000^x}{x!} (computationally intensive)\nNormal approximation: X \\sim \\text{Poisson}(1000) approximately N(1000, 1000)\nWith continuity correction:\n\n\\begin{aligned}\nP(X &lt; 950) &= P(X \\leq 949) \\\\\n&\\approx P\\left(Z \\leq \\frac{949.5 - 1000}{\\sqrt{1000}}\\right) \\\\\n&= P(Z \\leq -1.60) = 0.055\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Normal Approximation to Poisson:\"\n\n\n   lambda test_value condition_met exact_prob_approx mu_approx sigma_approx\n    &lt;num&gt;      &lt;num&gt;        &lt;lgcl&gt;             &lt;num&gt;     &lt;num&gt;        &lt;num&gt;\n1:   1000        950          TRUE        0.05420667      1000     31.62278\n    z_score normal_approx approx_error\n      &lt;num&gt;         &lt;num&gt;        &lt;num&gt;\n1: -1.59695     0.0551384 0.0009317283\n\n\n[1] \"Poisson Approximation Examples:\"\n\n\n   lambda test_x     exact   z_score    approx       error\n    &lt;num&gt;  &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;       &lt;num&gt;\n1:     10     15 0.9512596 1.7392527 0.9590048 0.007745243\n2:     25     30 0.8633089 1.1000000 0.8643339 0.001025070\n3:     50     55 0.7844704 0.7778175 0.7816617 0.002808718\n4:    100    105 0.7128079 0.5500000 0.7088403 0.003967569\n\n\n\n\n\n# Example 3-20: Normal Approximation to Poisson\npoisson_approx_data &lt;- data.table(\n  lambda = 1000,\n  test_value = 950\n) %&gt;%\n  fmutate(\n    # Check condition for normal approximation\n    condition_met = lambda &gt; 5,\n    # Exact Poisson (approximated due to computational limits)\n    exact_prob_approx = ppois(test_value - 1, lambda = lambda), # P(X &lt; 950) = P(X &lt;= 949)\n    # Normal approximation parameters\n    mu_approx = lambda,\n    sigma_approx = sqrt(lambda),\n    # Normal approximation with continuity correction\n    z_score = (test_value - 0.5 - mu_approx) / sigma_approx,\n    normal_approx = pnorm(z_score),\n    # Error (approximate since exact is computationally intensive)\n    approx_error = abs(exact_prob_approx - normal_approx)\n  )\n\nprint(\"Normal Approximation to Poisson:\")\nprint(poisson_approx_data)\n\n# Additional Poisson approximation examples\npoisson_examples &lt;- data.table(\n  lambda = c(10, 25, 50, 100),\n  test_x = c(15, 30, 55, 105)\n) %&gt;%\n  fmutate(\n    exact = ppois(test_x, lambda = lambda),\n    z_score = (test_x + 0.5 - lambda) / sqrt(lambda),\n    approx = pnorm(z_score),\n    error = abs(exact - approx)\n  )\n\nprint(\"Poisson Approximation Examples:\")\nprint(poisson_examples)",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#more-than-one-random-variable-and-independence",
    "href": "book/Ch03.html#more-than-one-random-variable-and-independence",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteJoint Probability Distributions\n\n\n\nWhen multiple random variables are measured simultaneously, we need joint probability distributions.\nJoint PDF (Continuous): For continuous random variables X and Y:\n\nP(a &lt; X &lt; b, c &lt; Y &lt; d) = \\int_a^b \\int_c^d f(x,y) \\, dy \\, dx\n\nProperties:\n\nf(x,y) \\geq 0 for all (x,y)\n\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} f(x,y) \\, dy \\, dx = 1\n\nJoint PMF (Discrete): For discrete random variables: f(x,y) = P(X = x, Y = y)\nMarginal Distributions:\n\nf_X(x) = \\int_{-\\infty}^{\\infty} f(x,y) \\, dy (continuous)\nf_X(x) = \\sum_y f(x,y) (discrete)\n\n\n\n\n\n\n\nFigure 25: Scatter diagram showing joint behavior of two variables\n\n\n\n\n\n\n\n\n\nFigure 26: Joint probability density function surface\n\n\n\n\n\n\n\n\n\nFigure 27: Probability as volume under joint PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteIndependence of Random Variables\n\n\n\nRandom variables X_1, X_2, \\ldots, X_n are independent if:\nP(X_1 \\in E_1, X_2 \\in E_2, \\ldots, X_n \\in E_n) = P(X_1 \\in E_1) \\times P(X_2 \\in E_2) \\times \\cdots \\times P(X_n \\in E_n)\nfor any sets E_1, E_2, \\ldots, E_n.\nEquivalent Conditions:\n\nContinuous: f(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\times f_{X_2}(x_2) \\times \\cdots \\times f_{X_n}(x_n)\nDiscrete: f(x_1, x_2, \\ldots, x_n) = f_{X_1}(x_1) \\times f_{X_2}(x_2) \\times \\cdots \\times f_{X_n}(x_n)\n\nPractical Meaning: Knowledge about one variable provides no information about others.\n\n\n\n\n\n\n\n\nTipExample 3-21: Shaft Diameter Independence\n\n\n\nShaft diameters are normally distributed with mean 0.2508 inch and standard deviation 0.0005 inch. Each shaft has probability 0.919 of meeting specifications.\nProblem: If diameters are independent, what’s the probability that all 10 shafts meet specifications?\nSolution: \\begin{aligned}\nP(\\text{all 10 meet specs}) &= P(X_1 \\text{ meets}) \\times P(X_2 \\text{ meets}) \\times \\cdots \\times P(X_{10} \\text{ meets}) \\\\\n&= (0.919)^{10} = 0.430\n\\end{aligned}\nInterpretation: Only 43% chance that all 10 shafts will be acceptable.\n\n\n\n\n\n\n\n\nTipExample 3-22: System Reliability\n\n\n\nA system operates if there’s a functional path from left to right. Components function independently with given probabilities.\n\n\n\n\n\n\nFigure 28: System reliability diagram\n\n\n\nSolution: For a series system: P(\\text{system works}) = P(C_1 \\text{ and } C_2) = P(C_1) \\times P(C_2) = 0.9 \\times 0.95 = 0.855\nFor parallel systems, calculate probability that at least one component works.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#functions-of-random-variables",
    "href": "book/Ch03.html#functions-of-random-variables",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteLinear Combinations of Independent Random Variables\n\n\n\nFor independent random variables X_1, X_2, \\ldots, X_n with means \\mu_i and variances \\sigma_i^2, consider the linear function:\nY = c_0 + c_1X_1 + c_2X_2 + \\cdots + c_nX_n\nMean and Variance:\n\n\\begin{aligned}\nE(Y) &= c_0 + c_1\\mu_1 + c_2\\mu_2 + \\cdots + c_n\\mu_n \\\\\nV(Y) &= c_1^2\\sigma_1^2 + c_2^2\\sigma_2^2 + \\cdots + c_n^2\\sigma_n^2\n\\end{aligned}\n\nKey Properties:\n\nExpected value of a sum equals sum of expected values\nFor independent variables, variance of a sum equals sum of variances\nConstants factor out of variance as squares\n\nSpecial Case: If X_1, X_2, \\ldots, X_n are independent and normally distributed, then Y is also normally distributed.\n\n\n\n\n\n\n\n\nTipExample 3-23: Rectangular Part Perimeter\n\n\n\nA rectangular part has length X_1 and width X_2 that are independent and normally distributed: - X_1 \\sim N(2, 0.1^2) cm - X_2 \\sim N(5, 0.2^2) cm\nThe perimeter is Y = 2X_1 + 2X_2.\nSolution:\n\n\\begin{aligned}\nE(Y) &= 2E(X_1) + 2E(X_2) = 2(2) + 2(5) = 14 \\text{ cm} \\\\\nV(Y) &= 2^2V(X_1) + 2^2V(X_2) = 4(0.1^2) + 4(0.2^2) \\\\\n&= 4(0.01) + 4(0.04) = 0.04 + 0.16 = 0.20 \\text{ cm}^2 \\\\\n\\sigma_Y &= \\sqrt{0.20} = 0.447 \\text{ cm}\n\\end{aligned}\n\nSince X_1 and X_2 are normal and independent, Y \\sim N(14, 0.20).\nProbability calculation: P(Y &gt; 14.5) = P\\left(Z &gt; \\frac{14.5 - 14}{0.447}\\right) = P(Z &gt; 1.12) = 0.131\n\n\n\n\n\n\n\n\n\n\n\nNoteCovariance and Correlation\n\n\n\nWhen random variables are not independent, we need to account for their relationship.\nCovariance: \n\\text{Cov}(X, Y) = E[(X - \\mu_X)(Y - \\mu_Y)] = E(XY) - \\mu_X\\mu_Y\n\nCorrelation Coefficient: \n\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\n\nProperties:\n\n-1 \\leq \\rho_{XY} \\leq 1\n\\rho_{XY} = 0 indicates no linear relationship\n|\\rho_{XY}| = 1 indicates perfect linear relationship\nIf X and Y are independent, then \\rho_{XY} = 0 (but not vice versa)\n\nGeneral Variance Formula: For Y = c_0 + c_1X_1 + c_2X_2 + \\cdots + c_nX_n:\n\nV(Y) = \\sum_{i=1}^n c_i^2\\sigma_i^2 + 2\\sum_{i&lt;j} c_ic_j\\text{Cov}(X_i, X_j)\n\n\n\n\n\n\n\n\n\nTipExample 3-24: Portfolio Risk\n\n\n\nAn investment portfolio consists of two stocks with returns X_1 and X_2:\n\nE(X_1) = 0.12, V(X_1) = 0.04\nE(X_2) = 0.08, V(X_2) = 0.02\n\n\\text{Cov}(X_1, X_2) = 0.01\n\nPortfolio return: Y = 0.6X_1 + 0.4X_2\nSolution:\n\n\\begin{aligned}\nE(Y) &= 0.6(0.12) + 0.4(0.08) = 0.072 + 0.032 = 0.104 \\\\\nV(Y) &= (0.6)^2(0.04) + (0.4)^2(0.02) + 2(0.6)(0.4)(0.01) \\\\\n&= 0.0144 + 0.0032 + 0.0048 = 0.0224\n\\end{aligned}\n\nInterpretation: Positive covariance increases portfolio risk compared to independent case.\n\n\n\n\n\n\n\n\n\n\n\nNotePropagation of Error Formula\n\n\n\nFor nonlinear functions, we use Taylor series approximation around the means.\nSingle Variable: If Y = h(X) where X has mean \\mu_X and variance \\sigma_X^2:\n\n\\begin{aligned}\nE(Y) &\\approx h(\\mu_X) \\\\\nV(Y) &\\approx \\left(\\frac{dh}{dx}\\bigg|_{x=\\mu_X}\\right)^2 \\sigma_X^2\n\\end{aligned}\n\nMultiple Variables: If Y = h(X_1, X_2, \\ldots, X_n) with independent X_i:\n\n\\begin{aligned}\nE(Y) &\\approx h(\\mu_1, \\mu_2, \\ldots, \\mu_n) \\\\\nV(Y) &\\approx \\sum_{i=1}^n \\left(\\frac{\\partial h}{\\partial x_i}\\bigg|_{\\boldsymbol{\\mu}}\\right)^2 \\sigma_i^2\n\\end{aligned}\n\nApplications: Engineering tolerance analysis, measurement uncertainty\n\n\n\n\n\n\n\n\nTipExample 3-25: Electrical Power Dissipation\n\n\n\nPower dissipated by a resistor: P = I^2R where I is current and R is resistance.\nGiven: I \\sim N(20, 0.1^2) amperes, R = 80 ohms (constant)\nSolution: h(I) = I^2R = 80I^2, so \\frac{dh}{dI} = 160I\n\n\\begin{aligned}\nE(P) &\\approx h(\\mu_I) = 80(20)^2 = 32,000 \\text{ watts} \\\\\nV(P) &\\approx \\left(\\frac{dh}{dI}\\bigg|_{I=20}\\right)^2 \\sigma_I^2 \\\\\n&= (160 \\times 20)^2 (0.1)^2 = (3200)^2(0.01) = 102,400 \\text{ watts}^2 \\\\\n\\sigma_P &= \\sqrt{102,400} = 320 \\text{ watts}\n\\end{aligned}\n\n\nR OutputR Code\n\n\n\n\n[1] \"Power Dissipation Analysis:\"\n\n\n   current_mean current_sd resistance derivative_at_mean power_mean_approx\n          &lt;num&gt;      &lt;num&gt;      &lt;num&gt;              &lt;num&gt;             &lt;num&gt;\n1:           20        0.1         80               3200             32000\n   power_variance_approx power_sd_approx exact_mean exact_variance exact_sd\n                   &lt;num&gt;           &lt;num&gt;      &lt;num&gt;          &lt;num&gt;    &lt;num&gt;\n1:                102400             320    32000.8       102401.3  320.002\n\n\n[1] \"Multiple Variable Function Analysis:\"\n\n\n     mu1 sigma1_sq   mu2 sigma2_sq   mu3 sigma3_sq dY_dX1 dY_dX2 dY_dX3\n   &lt;num&gt;     &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;num&gt;     &lt;num&gt;  &lt;num&gt;  &lt;num&gt;  &lt;num&gt;\n1:    10         1     5      0.25     2      0.04    2.5      5  -12.5\n   Y_mean_approx Y_variance_approx Y_sd_approx\n           &lt;num&gt;             &lt;num&gt;       &lt;num&gt;\n1:            25             18.75    4.330127\n\n\n\n\n\n# Example 3-25: Electrical Power Dissipation (Propagation of Error)\npower_analysis &lt;- data.table(\n  current_mean = 20,\n  current_sd = 0.1,\n  resistance = 80\n) %&gt;%\n  fmutate(\n    # Function: P = I^2 * R\n    # Derivative: dP/dI = 2*I*R\n    derivative_at_mean = 2 * current_mean * resistance,\n    # Mean of P\n    power_mean_approx = current_mean^2 * resistance,\n    # Variance of P using propagation of error\n    power_variance_approx = (derivative_at_mean)^2 * current_sd^2,\n    power_sd_approx = sqrt(power_variance_approx),\n    # Exact calculation for comparison (since I is normal, I^2 follows chi-square scaled)\n    # For normal I, E[I^2] = mu^2 + sigma^2, Var[I^2] = 2*sigma^4 + 4*mu^2*sigma^2\n    exact_mean = (current_mean^2 + current_sd^2) * resistance,\n    exact_variance = (2 * current_sd^4 + 4 * current_mean^2 * current_sd^2) * resistance^2,\n    exact_sd = sqrt(exact_variance)\n  )\n\nprint(\"Power Dissipation Analysis:\")\nprint(power_analysis)\n\n# Multiple variable example: Y = X1*X2/X3\nmulti_var_data &lt;- data.table(\n  mu1 = 10, sigma1_sq = 1,\n  mu2 = 5, sigma2_sq = 0.25,\n  mu3 = 2, sigma3_sq = 0.04\n) %&gt;%\n  fmutate(\n    # Function: Y = X1*X2/X3\n    # Partial derivatives at means\n    dY_dX1 = mu2 / mu3,\n    dY_dX2 = mu1 / mu3,\n    dY_dX3 = -(mu1 * mu2) / (mu3^2),\n    # Approximate mean\n    Y_mean_approx = (mu1 * mu2) / mu3,\n    # Approximate variance\n    Y_variance_approx = (dY_dX1)^2 * sigma1_sq + (dY_dX2)^2 * sigma2_sq + (dY_dX3)^2 * sigma3_sq,\n    Y_sd_approx = sqrt(Y_variance_approx)\n  )\n\nprint(\"Multiple Variable Function Analysis:\")\nprint(multi_var_data)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-26: Multiple Variable Function\n\n\n\nConsider Y = \\frac{X_1X_2}{X_3} where X_1, X_2, X_3 are independent with:\n\n\\mu_1 = 10, \\sigma_1^2 = 1\n\\mu_2 = 5, \\sigma_2^2 = 0.25\n\n\\mu_3 = 2, \\sigma_3^2 = 0.04\n\nSolution:\n\n\\frac{\\partial h}{\\partial x_1} = \\frac{x_2}{x_3}, \\quad \\frac{\\partial h}{\\partial x_2} = \\frac{x_1}{x_3}, \\quad \\frac{\\partial h}{\\partial x_3} = -\\frac{x_1x_2}{x_3^2}\n\nAt (\\mu_1, \\mu_2, \\mu_3) = (10, 5, 2):\n\n\\frac{\\partial h}{\\partial x_1}\\bigg|_{\\boldsymbol{\\mu}} = \\frac{5}{2} = 2.5, \\quad \\frac{\\partial h}{\\partial x_2}\\bigg|_{\\boldsymbol{\\mu}} = \\frac{10}{2} = 5, \\quad \\frac{\\partial h}{\\partial x_3}\\bigg|_{\\boldsymbol{\\mu}} = -\\frac{50}{4} = -12.5\n\n\n\\begin{aligned}\nE(Y) &\\approx \\frac{10 \\times 5}{2} = 25 \\\\\nV(Y) &\\approx (2.5)^2(1) + (5)^2(0.25) + (-12.5)^2(0.04) \\\\\n&= 6.25 + 6.25 + 6.25 = 18.75\n\\end{aligned}",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#random-sample-statistics-and-the-central-limit-theorem",
    "href": "book/Ch03.html#random-sample-statistics-and-the-central-limit-theorem",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "NoteRandom Sample and Statistics\n\n\n\nRandom Sample: Independent random variables X_1, X_2, \\ldots, X_n with the same distribution are called a random sample of size n.\nStatistic: A statistic is any function of the random variables in a random sample. Examples include:\n\nSample mean: \\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\nSample variance: S^2 = \\frac{1}{n-1}\\sum_{i=1}^n (X_i - \\overline{X})^2\nSample range: R = X_{(n)} - X_{(1)}\n\nSampling Distribution: The probability distribution of a statistic is called its sampling distribution.\nKey Properties of Sample Mean: If X_1, X_2, \\ldots, X_n is a random sample from a population with mean \\mu and variance \\sigma^2:\n\n\\begin{aligned}\nE(\\overline{X}) &= \\mu \\\\\nV(\\overline{X}) &= \\frac{\\sigma^2}{n} \\\\\n\\sigma_{\\overline{X}} &= \\frac{\\sigma}{\\sqrt{n}}\n\\end{aligned}\n\n\n\n\n\n\n\n\n\nNoteCentral Limit Theorem\n\n\n\nCentral Limit Theorem: If X_1, X_2, \\ldots, X_n is a random sample of size n from a population with mean \\mu and variance \\sigma^2, then as n \\to \\infty:\nZ = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} N(0, 1)\nKey Points:\n\nApplies regardless of the population distribution shape\nApproximation improves as n increases\nRule of thumb: n \\geq 30 for reasonable approximation\nIf population is normal, then \\overline{X} is exactly normal for any n\n\nPractical Importance: - Justifies normal approximations - Foundation for confidence intervals - Basis for hypothesis testing - Explains why sample means are approximately normal\n\n\n\n\n\n\n\n\n\n\n\n(a) One die\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Two dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Three dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Five dice\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) Ten dice\n\n\n\n\n\n\n\nFigure 29: Distributions of average scores from throwing dice (demonstration of CLT)\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-27: Resistor Manufacturing\n\n\n\nAn electronics company manufactures resistors with mean resistance 100 Ω and standard deviation 10 Ω. Find the probability that a random sample of n = 25 resistors has average resistance less than 95 Ω.\nSolution: Let \\overline{X} be the sample mean resistance.\nBy the Central Limit Theorem: \n\\overline{X} \\sim N\\left(100, \\frac{10^2}{25}\\right) = N(100, 4)\n\n\n\\begin{aligned}\nP(\\overline{X} &lt; 95) &= P\\left(\\frac{\\overline{X} - 100}{10/\\sqrt{25}} &lt; \\frac{95 - 100}{2}\\right) \\\\\n&= P(Z &lt; -2.5) = 0.0062\n\\end{aligned}\n\nInterpretation: Only 0.62% chance that the sample mean is below 95 Ω.\n\nR OutputR Code\n\n\n\n\n[1] \"Central Limit Theorem - Resistor Example:\"\n\n\n   population_mean population_sd sample_size test_value sample_mean_mean\n             &lt;num&gt;         &lt;num&gt;       &lt;num&gt;      &lt;num&gt;            &lt;num&gt;\n1:             100            10          25         95              100\n   sample_mean_sd sample_mean_variance z_score probability percentile_5\n            &lt;num&gt;                &lt;num&gt;   &lt;num&gt;       &lt;num&gt;        &lt;num&gt;\n1:              2                    4    -2.5 0.006209665     96.71029\n   percentile_95\n           &lt;num&gt;\n1:      103.2897\n\n\n[1] \"CLT Demonstration - Effect of Sample Size:\"\n\n\n   sample_size sample_mean_sd prob_below_95 prob_above_105\n         &lt;num&gt;          &lt;num&gt;         &lt;num&gt;          &lt;num&gt;\n1:           1      10.000000  3.085375e-01   3.085375e-01\n2:           4       5.000000  1.586553e-01   1.586553e-01\n3:           9       3.333333  6.680720e-02   6.680720e-02\n4:          16       2.500000  2.275013e-02   2.275013e-02\n5:          25       2.000000  6.209665e-03   6.209665e-03\n6:          36       1.666667  1.349898e-03   1.349898e-03\n7:          49       1.428571  2.326291e-04   2.326291e-04\n8:          64       1.250000  3.167124e-05   3.167124e-05\n\n\n\n\n\n# Example 3-27: Resistor Manufacturing (Central Limit Theorem)\nresistor_clt &lt;- data.table(\n  population_mean = 100,\n  population_sd = 10,\n  sample_size = 25,\n  test_value = 95\n) %&gt;%\n  fmutate(\n    # Sample mean distribution\n    sample_mean_mean = population_mean,\n    sample_mean_sd = population_sd / sqrt(sample_size),\n    sample_mean_variance = sample_mean_sd^2,\n    # Probability calculation\n    z_score = (test_value - sample_mean_mean) / sample_mean_sd,\n    probability = pnorm(z_score),\n    # Additional percentiles\n    percentile_5 = qnorm(0.05, mean = sample_mean_mean, sd = sample_mean_sd),\n    percentile_95 = qnorm(0.95, mean = sample_mean_mean, sd = sample_mean_sd)\n  )\n\nprint(\"Central Limit Theorem - Resistor Example:\")\nprint(resistor_clt)\n\n# CLT demonstration with different sample sizes\nclt_demo &lt;- data.table(\n  sample_size = c(1, 4, 9, 16, 25, 36, 49, 64)\n) %&gt;%\n  fmutate(\n    sample_mean_sd = 10 / sqrt(sample_size),\n    prob_below_95 = pnorm(95, mean = 100, sd = sample_mean_sd),\n    prob_above_105 = pnorm(105, mean = 100, sd = sample_mean_sd, lower.tail = FALSE)\n  )\n\nprint(\"CLT Demonstration - Effect of Sample Size:\")\nprint(clt_demo)\n\n\n\n\n\n\n\n\n\n\n\n\nTipExample 3-28: Quality Control Application\n\n\n\nA production process has mean output 500 units/hour with standard deviation 50 units/hour. If we observe production for 36 hours, what’s the probability that average hourly production exceeds 510 units?\nSolution: n = 36, \\mu = 500, \\sigma = 50\n\n\\begin{aligned}\nP(\\overline{X} &gt; 510) &= P\\left(Z &gt; \\frac{510 - 500}{50/\\sqrt{36}}\\right) \\\\\n&= P\\left(Z &gt; \\frac{10}{50/6}\\right) \\\\\n&= P(Z &gt; 1.2) = 1 - 0.8849 = 0.1151\n\\end{aligned}\n\nInterpretation: About 11.5% chance of observing such high average production.",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "book/Ch03.html#summary-and-key-concepts",
    "href": "book/Ch03.html#summary-and-key-concepts",
    "title": "1 Random Variables and Probability Distributions",
    "section": "",
    "text": "ImportantChapter 3 Summary\n\n\n\nContinuous Random Variables:\n\nProbability density function (PDF): P(a &lt; X &lt; b) = \\int_a^b f(x)dx\nCumulative distribution function (CDF): F(x) = P(X \\leq x)\nMean: \\mu = \\int_{-\\infty}^{\\infty} x f(x)dx\nVariance: \\sigma^2 = \\int_{-\\infty}^{\\infty} (x-\\mu)^2 f(x)dx\n\nImportant Continuous Distributions:\n\nNormal: Bell-shaped, symmetric, completely specified by \\mu and \\sigma^2\nLognormal: For positive variables, related to normal via logarithm\nExponential: Memoryless, models time between events\nGamma: Generalizes exponential, models waiting times\nWeibull: Flexible shape, widely used in reliability\nBeta: Defined on [0,1], models proportions\n\nDiscrete Random Variables:\n\nProbability mass function (PMF): f(x) = P(X = x)\nMean: \\mu = \\sum x f(x)\nVariance: \\sigma^2 = \\sum (x-\\mu)^2 f(x)\n\nImportant Discrete Distributions:\n\nBinomial: Fixed number of independent trials\nPoisson: Counts of rare events\nGeometric: Number of trials until first success\nHypergeometric: Sampling without replacement\n\nMultiple Random Variables:\n\nIndependence: Joint distribution factors\nLinear combinations: E(aX + bY) = aE(X) + bE(Y)\nFor independent variables: V(aX + bY) = a^2V(X) + b^2V(Y)\nCovariance and correlation measure dependence\n\nCentral Limit Theorem:\n\nSample means approach normal distribution\nFoundation for statistical inference\nExplains robustness of normal-based methods\n\nApplications:\n\nQuality control and process monitoring\nReliability and survival analysis\nRisk assessment and decision making\nExperimental design and data analysis\n\n\n\n\nR OutputR Code\n\n\n\n\n[1] \"Distribution Summary Statistics:\"\n\n\n   distribution sample_mean sample_sd  sample_min sample_max sample_range\n         &lt;char&gt;       &lt;num&gt;     &lt;num&gt;       &lt;num&gt;      &lt;num&gt;        &lt;num&gt;\n1:       Normal    50.90406  9.128159 26.90831124   71.87333     44.96502\n2:  Exponential    10.43276  9.353413  0.04367371   43.65911     43.61544\n3:      Weibull    88.00317 42.605619  5.82109597  189.09828    183.27719\n4:    Lognormal    60.70836 31.776841 10.42973266  186.75246    176.32273\n          cv\n       &lt;num&gt;\n1: 0.1793208\n2: 0.8965424\n3: 0.4841373\n4: 0.5234344\n\n\n[1] \"Parameter Estimates:\"\n\n\n   distribution param1_name param1_estimate param2_name param2_estimate\n         &lt;char&gt;      &lt;char&gt;           &lt;num&gt;      &lt;char&gt;           &lt;num&gt;\n1:       Normal        mean      50.9040591          sd       9.0824033\n2:  Exponential        rate       0.0958519        &lt;NA&gt;              NA\n3:      Weibull       shape       2.1616282       scale      98.9731545\n4:    Lognormal     meanlog       3.9722223       sdlog       0.5320079\n\n\n[1] \"Goodness of Fit Tests:\"\n\n\n   distribution ks_statistic ks_p_value good_fit\n         &lt;char&gt;        &lt;num&gt;      &lt;num&gt;   &lt;lgcl&gt;\n1:       Normal   0.05871913  0.8807748     TRUE\n2:  Exponential   0.07511794  0.6251819     TRUE\n3:      Weibull   0.09167937  0.3699697     TRUE\n4:    Lognormal   0.09690997  0.3046243     TRUE\n\n\n\n\n\n# Comprehensive Summary and Distribution Fitting Examples\nlibrary(fitdistrplus)\n\n# Generate sample data for distribution fitting\nset.seed(123)\n\n# Example datasets\nnormal_sample &lt;- rnorm(100, mean = 50, sd = 10)\nexponential_sample &lt;- rexp(100, rate = 0.1)\nweibull_sample &lt;- rweibull(100, shape = 2, scale = 100)\nlognormal_sample &lt;- rlnorm(100, meanlog = 4, sdlog = 0.5)\n\n# Create comprehensive dataset\nsample_data &lt;- data.table(\n  normal_data = normal_sample,\n  exponential_data = exponential_sample,\n  weibull_data = weibull_sample,\n  lognormal_data = lognormal_sample\n) %&gt;%\n  fmutate(\n    observation = 1:fnobs(normal_data)\n  )\n\n# Fit distributions using fitdistrplus\nfit_normal &lt;- fitdist(sample_data$normal_data, \"norm\")\nfit_exponential &lt;- fitdist(sample_data$exponential_data, \"exp\")\nfit_weibull &lt;- fitdist(sample_data$weibull_data, \"weibull\")\nfit_lognormal &lt;- fitdist(sample_data$lognormal_data, \"lnorm\")\n\n# Summary statistics for each distribution\ndistribution_summary &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  sample_mean = c(\n    fmean(sample_data$normal_data),\n    fmean(sample_data$exponential_data),\n    fmean(sample_data$weibull_data),\n    fmean(sample_data$lognormal_data)\n  ),\n  sample_sd = c(\n    fsd(sample_data$normal_data),\n    fsd(sample_data$exponential_data),\n    fsd(sample_data$weibull_data),\n    fsd(sample_data$lognormal_data)\n  ),\n  sample_min = c(\n    fmin(sample_data$normal_data),\n    fmin(sample_data$exponential_data),\n    fmin(sample_data$weibull_data),\n    fmin(sample_data$lognormal_data)\n  ),\n  sample_max = c(\n    fmax(sample_data$normal_data),\n    fmax(sample_data$exponential_data),\n    fmax(sample_data$weibull_data),\n    fmax(sample_data$lognormal_data)\n  )\n) %&gt;%\n  fmutate(\n    sample_range = sample_max - sample_min,\n    cv = sample_sd / sample_mean\n  )\n\nprint(\"Distribution Summary Statistics:\")\nprint(distribution_summary)\n\n# Parameter estimates\nparam_estimates &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  param1_name = c(\"mean\", \"rate\", \"shape\", \"meanlog\"),\n  param1_estimate = c(\n    fit_normal$estimate[1],\n    fit_exponential$estimate[1],\n    fit_weibull$estimate[1],\n    fit_lognormal$estimate[1]\n  ),\n  param2_name = c(\"sd\", NA, \"scale\", \"sdlog\"),\n  param2_estimate = c(\n    fit_normal$estimate[2],\n    NA,\n    fit_weibull$estimate[2],\n    fit_lognormal$estimate[2]\n  )\n)\n\nprint(\"Parameter Estimates:\")\nprint(param_estimates)\n\n# Goodness of fit tests\ngof_tests &lt;- data.table(\n  distribution = c(\"Normal\", \"Exponential\", \"Weibull\", \"Lognormal\"),\n  ks_statistic = c(\n    ks.test(sample_data$normal_data, \"pnorm\",\n      mean = fit_normal$estimate[1], sd = fit_normal$estimate[2]\n    )$statistic,\n    ks.test(sample_data$exponential_data, \"pexp\",\n      rate = fit_exponential$estimate[1]\n    )$statistic,\n    ks.test(sample_data$weibull_data, \"pweibull\",\n      shape = fit_weibull$estimate[1], scale = fit_weibull$estimate[2]\n    )$statistic,\n    ks.test(sample_data$lognormal_data, \"plnorm\",\n      meanlog = fit_lognormal$estimate[1], sdlog = fit_lognormal$estimate[2]\n    )$statistic\n  ),\n  ks_p_value = c(\n    ks.test(sample_data$normal_data, \"pnorm\",\n      mean = fit_normal$estimate[1], sd = fit_normal$estimate[2]\n    )$p.value,\n    ks.test(sample_data$exponential_data, \"pexp\",\n      rate = fit_exponential$estimate[1]\n    )$p.value,\n    ks.test(sample_data$weibull_data, \"pweibull\",\n      shape = fit_weibull$estimate[1], scale = fit_weibull$estimate[2]\n    )$p.value,\n    ks.test(sample_data$lognormal_data, \"plnorm\",\n      meanlog = fit_lognormal$estimate[1], sdlog = fit_lognormal$estimate[2]\n    )$p.value\n  )\n) %&gt;%\n  fmutate(\n    good_fit = ks_p_value &gt; 0.05\n  )\n\nprint(\"Goodness of Fit Tests:\")\nprint(gof_tests)\n\n# Save all datasets\nfwrite(sample_data, \"data/distribution_samples.csv\")\nfwrite(distribution_summary, \"data/distribution_summary.csv\")\nfwrite(param_estimates, \"data/parameter_estimates.csv\")\nfwrite(gof_tests, \"data/goodness_of_fit.csv\")\n\n\n\n\n:::",
    "crumbs": [
      "Lecture Notes",
      "Table of Contents",
      "Random Variables and Probability Distributions"
    ]
  },
  {
    "objectID": "introR/IntroR.html",
    "href": "introR/IntroR.html",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "",
    "text": "Welcome to the exciting world of data analysis! This comprehensive guide will teach you three essential tools that work together to make data analysis both powerful and accessible:\n\nR: A free, open-source programming language specifically designed for statistics and data analysis\nRStudio: A user-friendly integrated development environment (IDE) that makes working with R much easier\nQuarto: A modern publishing system that allows you to create professional reports, presentations, and websites\n\nThese tools form a complete ecosystem for modern data science, allowing you to import data, analyze it, create visualizations, and share your findings in professional documents (Posit Team 2022).\n\n\nThe combination of R, RStudio, and Quarto offers several compelling advantages for beginners and professionals alike (Wickham and Grolemund 2016; Posit Team 2023):\n\nCompletely Free: All three tools are open-source and free to use, with no licensing fees or subscription costs\nBeginner-Friendly: Despite their power, these tools are designed with simple commands and intuitive interfaces\nProfessional Results: Create publication-ready charts, statistical analyses, and formatted reports (Wickham 2016)\nWidely Used: Millions of data scientists, researchers, and analysts worldwide use these tools daily\nGreat Community: Large, helpful community with extensive tutorials, documentation, and support\nReproducible Research: Your analysis can be easily shared and reproduced by others (Allaire et al. 2022)\nVersatile: Suitable for everything from simple calculations to complex statistical modeling"
  },
  {
    "objectID": "introR/IntroR.html#why-use-these-tools",
    "href": "introR/IntroR.html#why-use-these-tools",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "",
    "text": "The combination of R, RStudio, and Quarto offers several compelling advantages for beginners and professionals alike (Wickham and Grolemund 2016; Posit Team 2023):\n\nCompletely Free: All three tools are open-source and free to use, with no licensing fees or subscription costs\nBeginner-Friendly: Despite their power, these tools are designed with simple commands and intuitive interfaces\nProfessional Results: Create publication-ready charts, statistical analyses, and formatted reports (Wickham 2016)\nWidely Used: Millions of data scientists, researchers, and analysts worldwide use these tools daily\nGreat Community: Large, helpful community with extensive tutorials, documentation, and support\nReproducible Research: Your analysis can be easily shared and reproduced by others (Allaire et al. 2022)\nVersatile: Suitable for everything from simple calculations to complex statistical modeling"
  },
  {
    "objectID": "introR/IntroR.html#what-is-r",
    "href": "introR/IntroR.html#what-is-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.1 What is R?",
    "text": "2.1 What is R?\nR is much more than just a calculator—it’s a complete statistical computing environment. Originally developed by statisticians for statisticians, R has evolved into one of the world’s most popular tools for data analysis. Think of R as:\n\nA powerful calculator that can handle complex mathematical operations\nA data management system that can work with datasets of any size\nA graphics engine that creates beautiful, publication-ready charts\nA programming language that can automate repetitive tasks\nA statistical toolkit with thousands of specialized functions\n\nR is particularly valuable because it’s designed specifically for working with data, making tasks that are difficult in other software surprisingly straightforward."
  },
  {
    "objectID": "introR/IntroR.html#installing-r",
    "href": "introR/IntroR.html#installing-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.2 Installing R",
    "text": "2.2 Installing R\nGetting R installed on your computer is straightforward:\n\nVisit the official website: Go to https://cran.r-project.org/\nChoose your operating system: Click on “Download R for Windows,” “Download R for macOS,” or “Download R for Linux”\nDownload the latest version: Always choose the most recent version (usually at the top of the list)\nRun the installer: Use default settings unless you have specific requirements\nVerify installation: Open R to make sure it starts correctly\n\n\n\n\n\n\n\nTipInstallation Tip\n\n\n\nThe CRAN (Comprehensive R Archive Network) website is the official and safest place to download R. Avoid downloading from other websites to ensure you get an authentic, virus-free version."
  },
  {
    "objectID": "introR/IntroR.html#basic-math-in-r",
    "href": "introR/IntroR.html#basic-math-in-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.3 Basic Math in R",
    "text": "2.3 Basic Math in R\nLet’s start with the fundamentals. R can perform all standard mathematical operations and much more. Notice how we use clear argument names to make our code easy to understand:\n\n# Basic operations\n2 + 2\n\n[1] 4\n\n5 - 2\n\n[1] 3\n\n5 * 2\n\n[1] 10\n\n6 / 2\n\n[1] 3\n\n# Advanced functions with explicit arguments\nsqrt(x = 16)\n\n[1] 4\n\n2^3\n\n[1] 8\n\nabs(x = -5)\n\n[1] 5\n\nround(x = 3.14159, digits = 2)\n\n[1] 3.14\n\n# Variables\nmy_age &lt;- 25\nmy_height &lt;- 170\nbmi &lt;- my_height / (my_age * 2)\ncat(\"BMI:\", bmi, \"\\n\")\n\nBMI: 3.4 \n\n\nAs you can see, R can handle basic arithmetic effortlessly. The cat() function helps us display results clearly. Using explicit argument names like x = and digits = makes your code much easier to read and understand, especially when you’re learning."
  },
  {
    "objectID": "introR/IntroR.html#working-with-data-tables",
    "href": "introR/IntroR.html#working-with-data-tables",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.4 Working with Data Tables",
    "text": "2.4 Working with Data Tables\nOne of the most important concepts in data analysis is working with structured data. We’ll use the powerful data.table package along with fastverse and tidyverse for efficient data operations:\n\n# Load all required packages for this tutorial\nlibrary(data.table) # Fast data manipulation and file reading\nlibrary(fastverse) # Collection of fast R packages for data science\nlibrary(tidyverse) # Collection of packages for data science workflow\nlibrary(readxl) # Read Excel files (.xlsx, .xls)\nlibrary(openxlsx) # Write Excel files and advanced Excel operations\nlibrary(knitr) # Dynamic report generation and table formatting\nlibrary(ggplot2) # Advanced data visualization (part of tidyverse)\n\n\n\n\n\n\n\nTipPackage Loading\n\n\n\nAll the packages we need for this tutorial are loaded in the setup chunk above. This keeps our code organized and ensures everything is available when we need it.\n\n\n\n# Create data (packages already loaded in setup-packages chunk)\nstudents &lt;-\n  data.table(\n    name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n    age = c(20, 22, 21, 23),\n    grade = c(85, 92, 78, 88),\n    major = c(\"Math\", \"Physics\", \"Chemistry\", \"Biology\")\n  )\n\nstudents\n\n      name   age grade     major\n    &lt;char&gt; &lt;num&gt; &lt;num&gt;    &lt;char&gt;\n1:   Alice    20    85      Math\n2:     Bob    22    92   Physics\n3: Charlie    21    78 Chemistry\n4:   Diana    23    88   Biology\n\nstr(students)\n\nClasses 'data.table' and 'data.frame':  4 obs. of  4 variables:\n $ name : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\"\n $ age  : num  20 22 21 23\n $ grade: num  85 92 78 88\n $ major: chr  \"Math\" \"Physics\" \"Chemistry\" \"Biology\"\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nThe data.table package offers several advantages over base R (Dowle and Srinivasan 2023):\n\nFaster performance: Especially noticeable with larger datasets\nMore intuitive syntax: Operations often feel more natural\nMemory efficient: Uses less computer memory\nBetter for beginners: Clearer error messages and more predictable behavior"
  },
  {
    "objectID": "introR/IntroR.html#basic-statistics-with-fastverse",
    "href": "introR/IntroR.html#basic-statistics-with-fastverse",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.5 Basic Statistics with Fastverse",
    "text": "2.5 Basic Statistics with Fastverse\nNow that we have some data, let’s calculate basic statistics using the efficient fastverse functions. These functions are faster and more consistent than base R functions:\n\n# Basic statistics using fastverse (already loaded)\nstudents %&gt;%\n  fsummarise(\n    avg_age = fmean(age),\n    avg_grade = fmean(grade),\n    max_grade = fmax(grade),\n    min_grade = fmin(grade)\n  )\n\n   avg_age avg_grade max_grade min_grade\n     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:    21.5     85.75        92        78\n\n# Enhanced statistics\nstudents %&gt;%\n  fsummarise(\n    n_students = fnobs(age),\n    age_range = paste(fmin(age), \"to\", fmax(age)),\n    grade_range = paste(fmin(grade), \"to\", fmax(grade)),\n    grade_sd = round(x = fsd(grade), digits = 2)\n  )\n\n   n_students age_range grade_range grade_sd\n        &lt;int&gt;    &lt;char&gt;      &lt;char&gt;    &lt;num&gt;\n1:          4  20 to 23    78 to 92     5.91\n\n\nUnderstanding these basic statistics is crucial:\n\nMean (Average): The sum of all values divided by the number of values\nMaximum/Minimum: The highest and lowest values in your data\nStandard Deviation: How spread out your data points are\nCount: The number of observations in your dataset\n\nNotice how we use fmean(), fmax(), fmin(), and other fast functions from the fastverse package. These are more efficient than the base R equivalents."
  },
  {
    "objectID": "introR/IntroR.html#creating-and-saving-visualizations",
    "href": "introR/IntroR.html#creating-and-saving-visualizations",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "2.6 Creating and Saving Visualizations",
    "text": "2.6 Creating and Saving Visualizations\nOne of R’s greatest strengths is creating high-quality visualizations. The ggplot2 package (Wickham 2016) makes this process both powerful and intuitive. We’ll also learn how to save our plots:\n\n# Create visualization (ggplot2 already loaded)\np1 &lt;-\n  ggplot(data = students, mapping = aes(x = name, y = grade, fill = major)) +\n  geom_col() +\n  labs(title = \"Student Grades by Major\", x = \"Student\", y = \"Grade\", fill = \"Major\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Display the plot\np1\n\n\n\n\nStudent Grades Comparison\n\n\n\n# Save the plot\nggsave(\n  plot = p1,\n  filename = \"figures/student_grades.png\",\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nThis chart immediately shows us that Bob has the highest grade (92) and Charlie has the lowest (78). Visualizations like this make patterns in data much easier to spot than looking at numbers alone.\nNotice how we:\n\nStore the plot in a variable (p1) before displaying it\nUse explicit argument names in ggplot() like data = and mapping =\nSave the plot for future use or sharing"
  },
  {
    "objectID": "introR/IntroR.html#what-is-rstudio",
    "href": "introR/IntroR.html#what-is-rstudio",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "3.1 What is RStudio?",
    "text": "3.1 What is RStudio?\nWhile you can use R by itself, RStudio (Posit Team 2023) makes the experience much more pleasant and productive. RStudio is an Integrated Development Environment (IDE) that provides a user-friendly interface for R.\n\n3.1.1 The Four-Panel Layout\nRStudio organizes your work into four main areas:\n\nScript Editor (top-left): Where you write and edit your R code\n\nSyntax highlighting makes code easier to read\nAuto-completion helps you write code faster\nYou can save your scripts for later use\n\nConsole (bottom-left): Where you interact directly with R\n\nType commands and see immediate results\nView error messages and warnings\nTest code snippets quickly\n\nEnvironment/History (top-right): Shows your current workspace\n\nEnvironment tab: See all your data objects and variables\nHistory tab: Review commands you’ve run previously\nConnections tab: Manage database connections\n\nFiles/Plots/Packages/Help (bottom-right): Multiple useful tabs\n\nFiles tab: Navigate your computer’s file system\nPlots tab: View charts and graphs you create\nPackages tab: Install and manage R packages\nHelp tab: Access documentation and tutorials"
  },
  {
    "objectID": "introR/IntroR.html#installing-rstudio",
    "href": "introR/IntroR.html#installing-rstudio",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "3.2 Installing RStudio",
    "text": "3.2 Installing RStudio\nRStudio installation is straightforward, but R must be installed first:\n\nEnsure R is installed: RStudio requires R to be installed first\nVisit Posit: Go to https://posit.co/downloads/\nChoose RStudio Desktop: The free version is perfect for learning\nDownload and install: Follow the installation wizard with default settings\nLaunch RStudio: You should see the four-panel interface"
  },
  {
    "objectID": "introR/IntroR.html#creating-and-managing-projects",
    "href": "introR/IntroR.html#creating-and-managing-projects",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "3.3 Creating and Managing Projects",
    "text": "3.3 Creating and Managing Projects\nOne of RStudio’s best features is project management. Projects keep your work organized and make it easy to switch between different analyses:\n\n3.3.1 Why Use Projects?\n\nOrganization: Keep related files together\nWorking Directory: Automatically sets the correct folder\nPortability: Easy to share entire projects with others\nVersion Control: Integrate with Git for tracking changes\n\n\n\n3.3.2 Creating Your First Project\n\n# Create directories\ndirs &lt;- c(\"data\", \"figures\", \"R\", \"out\")\nsapply(dirs, dir.create, showWarnings = FALSE)\n\n# Sample data (data.table already loaded)\nsales &lt;-\n  data.table(\n    month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"),\n    sales = c(100, 120, 150, 130, 160, 180),\n    region = rep(c(\"North\", \"South\"), 3)\n  )\n\n# Export files (packages already loaded)\nfwrite(x = sales, file = \"data/sales.csv\")\nwrite.xlsx(x = sales, file = \"data/sales.xlsx\")\n\ncat(\"Files created:\\n\")\nlist.files(path = \"data\", full.names = TRUE)\n\nWhen you create a project, RStudio:\n\nCreates a dedicated folder for your work\nSets up the proper working directory\nRemembers your project settings\nMakes it easy to share your work with others\n\nNotice how we create an “out” folder to store our output files and saved plots."
  },
  {
    "objectID": "introR/IntroR.html#what-is-quarto",
    "href": "introR/IntroR.html#what-is-quarto",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "4.1 What is Quarto?",
    "text": "4.1 What is Quarto?\nQuarto (Allaire et al. 2022) represents the next generation of scientific and technical publishing. It’s a powerful system that allows you to combine code, text, and outputs into professional documents. Think of Quarto as a way to create reports that include:\n\nYour analysis code: So others can see exactly what you did\nResults and charts: Automatically generated from your code\nWritten explanations: Your insights and conclusions\nProfessional formatting: Ready for sharing or publication\n\n\n4.1.1 The Power of Reproducible Research\nTraditional data analysis often involves:\n\nAnalyzing data in one program\nCreating charts in another program\nWriting conclusions in a word processor\nManually copying results between programs\n\nThis approach has problems:\n\nError-prone: Easy to copy wrong numbers\nTime-consuming: Updates require changing multiple files\nNot reproducible: Others can’t verify your work\n\nQuarto solves these problems by combining everything in one document that automatically updates when your data or analysis changes."
  },
  {
    "objectID": "introR/IntroR.html#installing-quarto",
    "href": "introR/IntroR.html#installing-quarto",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "4.2 Installing Quarto",
    "text": "4.2 Installing Quarto\nQuarto installation is simple and integrates seamlessly with RStudio:\n\nVisit the official website: Go to https://quarto.org/docs/get-started/\nDownload for your system: Choose Windows, macOS, or Linux\nInstall with defaults: The installer will handle everything\nRestart RStudio: This enables Quarto integration\nVerify installation: You should see Quarto options in RStudio menus"
  },
  {
    "objectID": "introR/IntroR.html#quarto-projects",
    "href": "introR/IntroR.html#quarto-projects",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "4.3 Quarto Projects",
    "text": "4.3 Quarto Projects\nQuarto projects provide additional organization and publishing features beyond basic RStudio projects:\n\n4.3.1 Creating a Quarto Project\nIn RStudio:\n\nFile → New Project\nNew Directory\nQuarto Project\nChoose project type (Document, Website, Book, etc.)\nConfigure options (output formats, features)\nCreate Project\n\n\n\n4.3.2 Benefits of Quarto Projects\n\nMultiple output formats: HTML, PDF, Word from the same source\nConsistent styling: Professional appearance across all outputs\nCross-references: Automatic numbering for figures and tables\nBibliography management: Automatic citation formatting\nWebsite publishing: Easy deployment to GitHub Pages or other platforms"
  },
  {
    "objectID": "introR/IntroR.html#your-first-document",
    "href": "introR/IntroR.html#your-first-document",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "4.4 Your First Document",
    "text": "4.4 Your First Document\n\n4.4.1 Creating New Documents\nThe process is straightforward in RStudio:\n\nFile → New File → Quarto Document\nEnter document details: Title, author, output format\nChoose format: HTML is best for beginners\nClick Create: RStudio opens a template document\n\n\n\n4.4.2 Understanding Document Structure\nEvery Quarto document has three main parts:\n\nYAML Header: Configuration between --- lines\nText: Written in Markdown format\nCode Chunks: R code between ```{r} and ```\n\n\n\n4.4.3 Output Formats\nQuarto’s ability to create multiple formats from one source is powerful:\n\nHTML (for web sharing)\nformat: html\n\nBest for: Interactive sharing, online viewing\nFeatures: Can include interactive elements, easy to share via email or web\n\n\n\nPDF (for printing)\nformat: pdf\n\nBest for: Professional reports, academic papers\nFeatures: Page numbers, professional typography, print-ready\n\n\n\nWord (for collaboration)\nformat: docx\n\nBest for: Collaborating with non-R users\nFeatures: Compatible with Microsoft Word, easy editing by others"
  },
  {
    "objectID": "introR/IntroR.html#a-complete-example",
    "href": "introR/IntroR.html#a-complete-example",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "4.5 A Complete Example",
    "text": "4.5 A Complete Example\nLet’s create a complete example that demonstrates Quarto’s capabilities using fastverse functions:\n\n# Weather data\nweather &lt;-\n  data.table(\n    day = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n    temp = c(22, 25, 23, 27, 24),\n    humidity = c(60, 55, 65, 50, 58),\n    condition = c(\"Sunny\", \"Cloudy\", \"Rainy\", \"Sunny\", \"Partly Cloudy\")\n  )\n\n# Statistics with pipe and fastverse\nweather %&gt;%\n  fsummarise(\n    avg_temp = fmean(temp),\n    max_temp = fmax(temp),\n    min_temp = fmin(temp)\n  )\n\n   avg_temp max_temp min_temp\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:     24.2       27       22\n\n# Create and save visualization\np2 &lt;- ggplot(data = weather, mapping = aes(x = day, y = temp, fill = condition)) +\n  geom_col() +\n  labs(title = \"Daily Temperature\", x = \"Day\", y = \"Temperature (°C)\", fill = \"Condition\") +\n  theme_minimal() +\n  geom_text(mapping = aes(label = paste(temp, \"°C\")), vjust = -0.3)\n\n# Display the plot\np2\n\n\n\n\nWeekly Temperature Analysis\n\n\n\n# Save the plot\nggsave(\n  plot = p2,\n  filename = \"figures/daily_temperature.png\",\n  width = 10,\n  height = 6,\n  dpi = 300\n)\n\nThis example demonstrates several key Quarto features:\n\nCode execution: The R code runs automatically\nOutput capture: Results are included in the document\nFigure generation: Charts are created and properly captioned\nProfessional formatting: Everything looks polished\n\nNotice how we use fmean(), fmax(), and fmin() from fastverse for efficient statistical calculations."
  },
  {
    "objectID": "introR/IntroR.html#reading-and-writing-data",
    "href": "introR/IntroR.html#reading-and-writing-data",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "5.1 Reading and Writing Data",
    "text": "5.1 Reading and Writing Data\nModern data analysis often involves working with data stored in various formats. Here’s how to handle the most common ones using efficient functions:\n\n# Read data efficiently with explicit arguments (packages already loaded)\nsales_csv &lt;- fread(file = \"data/sales.csv\")\nsales_excel &lt;- read_excel(path = \"data/sales.xlsx\") %&gt;% as.data.table()\n\n# Compare datasets\nidentical(x = sales_csv, y = sales_excel)\nrbindlist(l = list(CSV = sales_csv, Excel = sales_excel), idcol = \"Source\")\n\n\n5.1.1 Understanding File Formats\n\nCSV files: Plain text, widely compatible, smaller file size\nExcel files: Can contain multiple sheets, formatted data, larger file size\ndata.table: More efficient than data.frame for larger datasets\n\nNotice how we use fread() instead of read.csv() - it’s much faster and more flexible."
  },
  {
    "objectID": "introR/IntroR.html#practical-examples",
    "href": "introR/IntroR.html#practical-examples",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "5.2 Practical Examples",
    "text": "5.2 Practical Examples\nLet’s work through some realistic examples that demonstrate common data analysis tasks using fastverse functions:\n\n# Test scores data\nscores &lt;-\n  data.table(\n    student = c(\"Anna\", \"Bob\", \"Carol\", \"David\", \"Eva\"),\n    math = c(85, 92, 78, 88, 95),\n    english = c(88, 85, 92, 80, 90),\n    science = c(82, 90, 85, 92, 88)\n  )\n\n# Display table with knitr (already loaded)\nkable(x = scores, caption = \"Student Test Scores\")\n\n\nStudent Test Scores\n\n\nstudent\nmath\nenglish\nscience\n\n\n\n\nAnna\n85\n88\n82\n\n\nBob\n92\n85\n90\n\n\nCarol\n78\n92\n85\n\n\nDavid\n88\n80\n92\n\n\nEva\n95\n90\n88\n\n\n\n\n# Calculate subject averages using fastverse\nsubject_summary &lt;-\n  scores %&gt;%\n  fsummarise(\n    Math = fmean(math),\n    English = fmean(english),\n    Science = fmean(science)\n  ) %&gt;%\n  pivot(\n    how = \"longer\",\n    names = list(\"Subject\", \"Average\")\n  )\n\nkable(x = subject_summary, caption = \"Subject Averages\", digits = 1)\n\n\nSubject Averages\n\n\nSubject\nAverage\n\n\n\n\nMath\n87.6\n\n\nEnglish\n87.0\n\n\nScience\n87.4\n\n\n\n\n\nThis table shows our data in a clean, professional format that’s easy to read and understand.\nNow let’s create a comparison visualization:\n\n# Reshape data for visualization\nsubject_avg &lt;-\n  scores %&gt;%\n  fsummarise(\n    Math    = fmean(math),\n    English = fmean(english),\n    Science = fmean(science)\n  ) %&gt;%\n  pivot(\n    how = \"longer\",\n    names = list(\"Subject\", \"Average\")\n  )\n\n# Create and save comparison chart\np3 &lt;- ggplot(data = subject_avg, mapping = aes(x = Subject, y = Average, fill = Subject)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  labs(title = \"Average Test Scores by Subject\", x = \"Subject\", y = \"Average Score\") +\n  theme_minimal() +\n  geom_text(mapping = aes(label = round(x = Average, digits = 1)), vjust = -0.3) +\n  ylim(0, 100)\n\n# Display the plot\np3\n\n\n\n\nComparison of Average Scores by Subject\n\n\n\n# Save the plot\nggsave(\n  plot = p3,\n  filename = \"figures/subject_averages.png\",\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nThis visualization immediately shows that Math scores are slightly higher than English scores on average, demonstrating how charts can reveal patterns that might not be obvious from tables alone.\nNotice how we use:\n\nfmean() for calculating averages efficiently\npivot() for reshaping data\nExplicit argument names throughout for clarity"
  },
  {
    "objectID": "introR/IntroR.html#getting-help-when-you-need-it",
    "href": "introR/IntroR.html#getting-help-when-you-need-it",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "6.1 Getting Help When You Need It",
    "text": "6.1 Getting Help When You Need It\nLearning R is a journey, and everyone needs help sometimes. Here are the best ways to get assistance:\n\n# Help functions with explicit arguments\n?mean\nhelp.search(pattern = \"regression\")\nexample(topic = \"mean\")\n\n# Package help\nhelp(package = \"data.table\")\n\n\n6.1.1 Additional Help Resources\n\nRStudio Help pane: Built-in documentation with examples\nStack Overflow: Huge community of R users answering questions\nR-bloggers: Daily articles about R techniques and applications\nLocal R User Groups: Many cities have R meetups and workshops"
  },
  {
    "objectID": "introR/IntroR.html#common-mistakes-and-how-to-avoid-them",
    "href": "introR/IntroR.html#common-mistakes-and-how-to-avoid-them",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "6.2 Common Mistakes and How to Avoid Them",
    "text": "6.2 Common Mistakes and How to Avoid Them\nLearning from common mistakes can save you hours of frustration:\n\n6.2.1 1. Case Sensitivity\nR distinguishes between uppercase and lowercase letters:\n\nMean ≠ mean (only mean is the correct function)\nData ≠ data (variable names must match exactly)\n\n\n\n6.2.2 2. Quotation Marks\nText must be enclosed in quotes:\n\nCorrect: \"Alice\", \"Sales Department\"\nIncorrect: Alice, Sales Department\n\n\n\n6.2.3 3. Package Loading\nPackages must be loaded before use:\n\nAlways run library(fastverse) before using fastverse functions\nLoad packages at the beginning of your script\n\n\n\n6.2.4 4. Parentheses and Brackets\nEvery opening parenthesis needs a closing one:\n\nfmean(students$age) ✓\nfmean(students$age ✗ (missing closing parenthesis)\n\n\n\n6.2.5 5. Using Explicit Arguments\nAlways use explicit argument names when learning:\n\nGood: round(x = 3.14159, digits = 2)\nLess clear: round(3.14159, 2)"
  },
  {
    "objectID": "introR/IntroR.html#keyboard-shortcuts-for-efficiency",
    "href": "introR/IntroR.html#keyboard-shortcuts-for-efficiency",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "6.3 Keyboard Shortcuts for Efficiency",
    "text": "6.3 Keyboard Shortcuts for Efficiency\nLearning these shortcuts will significantly speed up your work:\n\nCtrl+Enter (Windows) or Cmd+Enter (Mac): Run current line or selection\nCtrl+Shift+Enter: Run entire code chunk\nTab: Auto-complete function names and file paths\nCtrl+Z: Undo last action\nCtrl+Shift+C: Comment/uncomment selected lines\nCtrl+L: Clear console\nCtrl+1: Focus on script editor\nCtrl+2: Focus on console"
  },
  {
    "objectID": "introR/IntroR.html#best-practices-for-beginners",
    "href": "introR/IntroR.html#best-practices-for-beginners",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "6.4 Best Practices for Beginners",
    "text": "6.4 Best Practices for Beginners\nDeveloping good habits early will save you time and frustration:\n\n6.4.1 1. Project Organization\nmy-analysis/\n├── data/           # Raw data files\n├── R/              # R scripts\n├── figures/        # Generated plots\n├── out/            # Output files\n└── README.md       # Project description\n\n\n6.4.2 2. Code Documentation\n# Load required packages\nlibrary(data.table)\nlibrary(fastverse)\nlibrary(ggplot2)\n\n# Read sales data from CSV file\nsales_data &lt;- fread(file = \"data/sales_2023.csv\")\n\n# Calculate monthly averages\nmonthly_avg &lt;- fmean(x = sales_data$monthly_sales)\n\n\n6.4.3 3. Consistent Naming\n\nUse descriptive names: student_grades not sg\nBe consistent: if you use underscores, always use underscores\nAvoid spaces in names: sales_data not sales data\n\n\n\n6.4.4 4. Regular Saving\n\nSave your scripts frequently (Ctrl+S)\nUse meaningful file names with dates\nConsider version control (Git) for important projects\n\n\n\n6.4.5 5. Save Your Plots\n\nAlways save important visualizations\nUse consistent naming for plot files\nStore plots in a dedicated folder"
  },
  {
    "objectID": "introR/IntroR.html#package-related-problems",
    "href": "introR/IntroR.html#package-related-problems",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "7.1 Package-Related Problems",
    "text": "7.1 Package-Related Problems\nPackage issues are among the most common problems beginners encounter:\n\n# If you see \"packagename not found\"\ninstall.packages(\"packagename\")\nlibrary(packagename)\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Install multiple packages\ninstall.packages(c(\"data.table\", \"readxl\", \"openxlsx\"))\n\n# Session information\nsessionInfo()\n\n\n7.1.1 Additional Package Solutions\n\nUpdate R: Newer versions often resolve compatibility issues\nRestart R session: Session → Restart R in RStudio\nCheck internet connection: Package installation requires internet access\nTry different mirror: Some CRAN mirrors may be temporarily unavailable"
  },
  {
    "objectID": "introR/IntroR.html#data-import-issues",
    "href": "introR/IntroR.html#data-import-issues",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "7.2 Data Import Issues",
    "text": "7.2 Data Import Issues\nCommon problems when loading data:\n\n7.2.1 File Path Problems\n\nUse forward slashes: \"data/myfile.csv\" not \"data\\myfile.csv\"\nCheck working directory: Use getwd() to see current location\nUse relative paths: Avoid \"C:/Users/YourName/Desktop/file.csv\"\n\n\n\n7.2.2 File Format Issues\n\nCheck file extension: Ensure .csv files are actually CSV format\nEncoding problems: Try fread(file = \"file.csv\", encoding = \"UTF-8\")\nDelimiter issues: Some “CSV” files use semicolons or tabs"
  },
  {
    "objectID": "introR/IntroR.html#getting-more-help",
    "href": "introR/IntroR.html#getting-more-help",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "7.3 Getting More Help",
    "text": "7.3 Getting More Help\nWhen you’re stuck, try these resources in order:\n\nBuilt-in Help: Start with ?function_name\nRStudio Cheatsheets: Help → Cheatsheets\nGoogle Search: “R how to [your specific question]”\nStack Overflow: Include “R” in your search terms\nRStudio Community: community.rstudio.com\nLocal User Groups: Search for “R User Group [your city]”"
  },
  {
    "objectID": "introR/IntroR.html#what-youve-accomplished",
    "href": "introR/IntroR.html#what-youve-accomplished",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "8.1 What You’ve Accomplished",
    "text": "8.1 What You’ve Accomplished\nThrough this guide, you’ve learned to:\n✅ Install and set up the complete R data science toolkit\n✅ Perform basic mathematics and statistical calculations with fastverse\n✅ Create and manipulate data using data.table()\n✅ Generate professional visualizations with ggplot2\n✅ Build comprehensive reports with Quarto\n✅ Organize projects effectively in RStudio\n✅ Troubleshoot common problems independently\n✅ Apply best practices for reproducible research\n✅ Use explicit arguments for clearer, more readable code"
  },
  {
    "objectID": "introR/IntroR.html#your-next-steps",
    "href": "introR/IntroR.html#your-next-steps",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "8.2 Your Next Steps",
    "text": "8.2 Your Next Steps\nNow that you have the basics, here’s how to continue your learning journey:\n\nCreate a personal project: Analyze data you care about (sports, weather, personal finances)\nReproduce this tutorial: Try creating similar analyses with different data\nExperiment with styling: Modify colors, themes, and formatting in your charts\nPractice explicit arguments: Always use argument names in your functions\nLearn more ggplot2: Explore different chart types (scatter plots, histograms, box plots)\nMaster data import: Practice reading different file formats and cleaning messy data\nDevelop your workflow: Create templates for common analyses\nExplore fastverse: Learn more efficient functions for data manipulation\nStatistical methods: Learn about hypothesis testing, regression, and correlation analysis\nAdvanced Quarto: Explore presentations, websites, and interactive documents\nPackage ecosystem: Discover specialized packages for your field of interest\nAutomation: Learn to create functions and automate repetitive tasks"
  },
  {
    "objectID": "introR/IntroR.html#essential-learning-resources",
    "href": "introR/IntroR.html#essential-learning-resources",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "8.3 Essential Learning Resources",
    "text": "8.3 Essential Learning Resources\n\n8.3.1 Free Online Books\n\nR for Data Science (Wickham and Grolemund 2016): The definitive beginner’s guide\nQuarto Documentation: Comprehensive guide to all Quarto features\nggplot2 Book: Deep dive into data visualization\nfastverse Documentation: Learn efficient data manipulation\n\n\n\n8.3.2 Interactive Learning\n\nRStudio Education: Free courses and tutorials\nSwirl: Learn R interactively within R itself\nDataCamp: Structured courses (some free content)\n\n\n\n8.3.3 Community Resources\n\nR-bloggers: Daily articles and tutorials\n#RStats Twitter: Active community sharing tips and resources\nLocal R Meetups: Network with other R users in your area"
  },
  {
    "objectID": "introR/IntroR.html#final-encouragement",
    "href": "introR/IntroR.html#final-encouragement",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "8.4 Final Encouragement",
    "text": "8.4 Final Encouragement\nRemember that everyone starts as a beginner, and the R community is known for being welcoming and helpful. Don’t be discouraged if concepts take time to sink in—data analysis is a skill that develops with practice.\nThe tools you’ve learned today are used by:\n\nData scientists at major technology companies\nResearchers at universities worldwide\nAnalysts in government and non-profit organizations\nStudents in fields from psychology to finance\nProfessionals in healthcare, marketing, and countless other fields\n\nYou’re now part of a global community of people using these powerful tools to understand the world through data.\nKeep practicing, stay curious, and most importantly—have fun with your data analysis journey!\nRemember: Always use explicit argument names, save your work regularly, and don’t hesitate to ask for help when you need it."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "",
    "text": "This course provides a comprehensive introduction to statistical methods essential for science and engineering applications. Learn to analyze data, make informed decisions, and solve real-world problems using statistical techniques.\n\n📚 Access Lecture Notes 🎯 View Lecture Slides 💻 Introduction to R 📊 Course Resources"
  },
  {
    "objectID": "index.html#welcome-to-math-3020",
    "href": "index.html#welcome-to-math-3020",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "",
    "text": "This course provides a comprehensive introduction to statistical methods essential for science and engineering applications. Learn to analyze data, make informed decisions, and solve real-world problems using statistical techniques.\n\n📚 Access Lecture Notes 🎯 View Lecture Slides 💻 Introduction to R 📊 Course Resources"
  },
  {
    "objectID": "index.html#course-components",
    "href": "index.html#course-components",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "Course Components",
    "text": "Course Components\n\n\n📚 Interactive Lecture Notes\nAccess our comprehensive digital Lecture Notes with interactive examples, exercises, and real-world applications. Each chapter builds upon previous concepts with clear explanations and practical implementations.\nExplore Lecture Notes →\n\n\n🎯 Lecture Slides\nView professionally designed presentation slides for each lecture with detailed explanations, visual aids, and interactive elements. Perfect for review and study.\nBrowse Slides →\n\n\n💻 Introduction to R\nMaster R programming with comprehensive tutorials available in HTML, PDF, and Word formats. From basics to advanced statistical analysis techniques.\nStart Learning R →\n\n\n📊 Course Resources\nAccess complete course materials including syllabus, schedule, assignments, datasets, and supplementary resources for hands-on learning.\nView Resources →"
  },
  {
    "objectID": "index.html#course-highlights",
    "href": "index.html#course-highlights",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "Course Highlights",
    "text": "Course Highlights\n\n\n\n\n\n\n\nNote🚀 Interactive Learning\n\n\n\nAll materials are designed for active learning with embedded R code, interactive visualizations, and practical exercises that reinforce key concepts.\n\n\n\n\n\n\n\n\nTip🔬 Real-World Applications\n\n\n\nLearn statistics through authentic problems from engineering and scientific disciplines, preparing you for professional challenges.\n\n\n\n\n\n\n\n\nImportant📈 Comprehensive Coverage\n\n\n\nFrom descriptive statistics to inferential methods, covering all essential topics with depth and practical application focus."
  },
  {
    "objectID": "index.html#quick-access-information",
    "href": "index.html#quick-access-information",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "Quick Access Information",
    "text": "Quick Access Information\n\nInstructor: Muhammad Yaseen\nInstitution: School of Mathematical & Statistical Sciences, Clemson University\nDuration: Full semester course\nSoftware: R and RStudio (free downloads)\nSupport: Office hours, discussion forum, and email assistance"
  },
  {
    "objectID": "index.html#learning-outcomes",
    "href": "index.html#learning-outcomes",
    "title": "Math-3020: Statistics for Science & Engineering",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nBy the end of this course, you will be able to:\n\nAnalyze Data: Apply statistical methods to real-world datasets\nUse R Programming: Perform statistical analysis using industry-standard software\nInterpret Results: Draw meaningful conclusions from statistical analyses\nCommunicate Findings: Present statistical results clearly and effectively\nSolve Problems: Apply statistical thinking to engineering and scientific challenges\n\n\n\nThis course website is built with Quarto and licensed under CC BY-SA 4.0\nQuestions? Send an email or visit office hours."
  },
  {
    "objectID": "slides/introR.html",
    "href": "slides/introR.html",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "",
    "text": "Three Essential Tools:\n\nR: Free, powerful programming language for statistics\nRStudio: User-friendly interface for R\nQuarto: Modern publishing system for reports\n\n\nComplete Ecosystem for:\n\nData import and cleaning\nStatistical analysis\nProfessional visualizations\nReproducible reports\nScientific publishing\n\n\n\n\n\n\n\n\n\nTipWhy These Tools Matter\n\n\n\nUsed by millions of data scientists, researchers, and analysts worldwide for everything from academic research to business intelligence.\n\n\n\n\n\n\n\nKey Advantages:\n\nCompletely Free - No licensing fees ever\nBeginner-Friendly - Simple commands, intuitive interfaces\nProfessional Results - Publication-ready outputs\nWidely Used - Industry standard tools\nGreat Community - Extensive help and tutorials\n\n\nPractical Benefits:\n\nReproducible Research - Share and verify analyses\nVersatile Applications - From simple calculations to complex modeling\nMultiple Output Formats - HTML, PDF, Word from one source\nVersion Control - Track changes and collaborate effectively\n\n\n\nBottom Line: These tools transform how you work with data, making complex analyses accessible and professional reporting automatic.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#why-use-r-rstudio-and-quarto",
    "href": "slides/introR.html#why-use-r-rstudio-and-quarto",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Why Use R, RStudio, and Quarto?",
    "text": "Why Use R, RStudio, and Quarto?\n\n\nKey Advantages:\n\nCompletely Free - No licensing fees ever\nBeginner-Friendly - Simple commands, intuitive interfaces\nProfessional Results - Publication-ready outputs\nWidely Used - Industry standard tools\nGreat Community - Extensive help and tutorials\n\n\nPractical Benefits:\n\nReproducible Research - Share and verify analyses\nVersatile Applications - From simple calculations to complex modeling\nMultiple Output Formats - HTML, PDF, Word from one source\nVersion Control - Track changes and collaborate effectively\n\n\nBottom Line: These tools transform how you work with data, making complex analyses accessible and professional reporting automatic.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#what-is-r",
    "href": "slides/introR.html#what-is-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "What is R?",
    "text": "What is R?\n\n\nR is Much More Than a Calculator:\n\nStatistical Computing Environment - Built by statisticians for data analysis\nData Management System - Handle datasets of any size\nGraphics Engine - Beautiful, publication-ready charts\nProgramming Language - Automate repetitive tasks\nStatistical Toolkit - Thousands of specialized functions\n\n\nWhat Makes R Special:\n\nDesigned for Data - Tasks difficult elsewhere are straightforward in R\nExtensive Libraries - Over 19,000 packages available\nActive Development - Constantly updated with latest methods\nCross-Platform - Works on Windows, Mac, Linux\nIntegration - Connects with databases, web APIs, other languages",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#installing-r",
    "href": "slides/introR.html#installing-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Installing R",
    "text": "Installing R\nSimple 5-Step Process:\n\nVisit Official Website - https://cran.r-project.org/\nChoose Your OS - Windows, macOS, or Linux\nDownload Latest Version - Always get the most recent release\nRun Installer - Use default settings for beginners\nVerify Installation - Open R to confirm it works\n\n\n\n\n\n\n\nInstallation Tip\n\n\nCRAN (Comprehensive R Archive Network) is the official and safest source. Avoid third-party downloads to ensure authentic, secure software.\n\n\n\nNext Step: While R works alone, RStudio makes everything much easier!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#basic-math-in-r",
    "href": "slides/introR.html#basic-math-in-r",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Basic Math in R",
    "text": "Basic Math in R\n\n# Basic operations\n2 + 2\n\n[1] 4\n\n5 - 2\n\n[1] 3\n\n5 * 2\n\n[1] 10\n\n6 / 2\n\n[1] 3\n\n# Advanced functions with explicit arguments\nsqrt(x = 16)\n\n[1] 4\n\n2^3\n\n[1] 8\n\nabs(x = -5)\n\n[1] 5\n\nround(x = 3.14159, digits = 2)\n\n[1] 3.14\n\n# Variables\nmy_age &lt;- 25\nmy_height &lt;- 170\nbmi &lt;- my_height / (my_age * 2)\ncat(\"BMI:\", bmi, \"\\n\")\n\nBMI: 3.4 \n\n\nKey Concepts:\n\nExplicit Arguments - Use x = and digits = for clarity\nVariable Assignment - Use &lt;- to store values\nFunction Calls - Always include parentheses and argument names",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#working-with-data-tables",
    "href": "slides/introR.html#working-with-data-tables",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Working with Data Tables",
    "text": "Working with Data Tables\n\n# Load all required packages for this tutorial\nlibrary(data.table) # Fast data manipulation and file reading\nlibrary(fastverse) # Collection of fast R packages for data science\nlibrary(tidyverse) # Collection of packages for data science workflow\nlibrary(readxl) # Read Excel files (.xlsx, .xls)\nlibrary(openxlsx) # Write Excel files and advanced Excel operations\nlibrary(knitr) # Dynamic report generation and table formatting\nlibrary(ggplot2) # Advanced data visualization (part of tidyverse)\n\n\n# Create data (packages already loaded in setup-packages chunk)\nstudents &lt;-\n  data.table(\n    name = c(\"Alice\", \"Bob\", \"Charlie\", \"Diana\"),\n    age = c(20, 22, 21, 23),\n    grade = c(85, 92, 78, 88),\n    major = c(\"Math\", \"Physics\", \"Chemistry\", \"Biology\")\n  )\n\nstudents\n\n      name   age grade     major\n    &lt;char&gt; &lt;num&gt; &lt;num&gt;    &lt;char&gt;\n1:   Alice    20    85      Math\n2:     Bob    22    92   Physics\n3: Charlie    21    78 Chemistry\n4:   Diana    23    88   Biology\n\nstr(students)\n\nClasses 'data.table' and 'data.frame':  4 obs. of  4 variables:\n $ name : chr  \"Alice\" \"Bob\" \"Charlie\" \"Diana\"\n $ age  : num  20 22 21 23\n $ grade: num  85 92 78 88\n $ major: chr  \"Math\" \"Physics\" \"Chemistry\" \"Biology\"\n - attr(*, \".internal.selfref\")=&lt;externalptr&gt; \n\n\nWhy data.table? Faster performance, intuitive syntax, memory efficient, better for beginners",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#basic-statistics-with-fastverse",
    "href": "slides/introR.html#basic-statistics-with-fastverse",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Basic Statistics with Fastverse",
    "text": "Basic Statistics with Fastverse\n\n# Basic statistics using fastverse (already loaded)\nstudents %&gt;%\n  fsummarise(\n    avg_age = fmean(age),\n    avg_grade = fmean(grade),\n    max_grade = fmax(grade),\n    min_grade = fmin(grade)\n  )\n\n   avg_age avg_grade max_grade min_grade\n     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;     &lt;num&gt;\n1:    21.5     85.75        92        78\n\n# Enhanced statistics\nstudents %&gt;%\n  fsummarise(\n    n_students = fnobs(age),\n    age_range = paste(fmin(age), \"to\", fmax(age)),\n    grade_range = paste(fmin(grade), \"to\", fmax(grade)),\n    grade_sd = round(x = fsd(grade), digits = 2)\n  )\n\n   n_students age_range grade_range grade_sd\n        &lt;int&gt;    &lt;char&gt;      &lt;char&gt;    &lt;num&gt;\n1:          4  20 to 23    78 to 92     5.91\n\n\nEssential Statistics Explained:\n\nMean: Average of all values\nMax/Min: Highest and lowest values\n\nStandard Deviation: How spread out the data is\nCount: Number of observations",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#creating-professional-visualizations",
    "href": "slides/introR.html#creating-professional-visualizations",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Creating Professional Visualizations",
    "text": "Creating Professional Visualizations\n\n# Create visualization (ggplot2 already loaded)\np1 &lt;-\n  ggplot(data = students, mapping = aes(x = name, y = grade, fill = major)) +\n  geom_col() +\n  labs(title = \"Student Grades by Major\", x = \"Student\", y = \"Grade\", fill = \"Major\") +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Display the plot\np1\n\n\n\n\n\n\n\n# Save the plot\nggsave(\n  plot = p1,\n  filename = \"figures/student_grades.png\",\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nVisualization Benefits: Patterns become immediately obvious - Bob has highest grade (92), Charlie lowest (78)",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#what-is-rstudio",
    "href": "slides/introR.html#what-is-rstudio",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "What is RStudio?",
    "text": "What is RStudio?\n\n\nIntegrated Development Environment (IDE):\nRStudio transforms R from a basic command line into a professional workspace with:\n\nSyntax Highlighting - Color-coded commands\nAuto-completion - Faster, error-free coding\n\nProject Management - Organized workflows\nIntegrated Help - Documentation at your fingertips\n\n\nFour-Panel Layout:\n\nScript Editor (top-left) - Write and save code\nConsole (bottom-left) - Interactive R commands\nEnvironment/History (top-right) - See variables and past commands\nFiles/Plots/Help (bottom-right) - Navigation and outputs\n\n\nBottom Line: RStudio makes R accessible to beginners while remaining powerful for experts",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#installing-rstudio",
    "href": "slides/introR.html#installing-rstudio",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Installing RStudio",
    "text": "Installing RStudio\nPrerequisites and Steps:\n\n\nBefore Installing:\n\nR Must Be Installed First - RStudio requires R to function\nCheck R Installation - Open R to verify it works\n\nInstallation Process:\n\nVisit https://posit.co/downloads/\nChoose RStudio Desktop (free version)\nDownload for your operating system\nRun installer with default settings\n\n\nAfter Installation:\n\nLaunch RStudio - You should see four-panel interface\nVerify R Connection - Console should show R version\nExplore Interface - Familiarize yourself with panels\n\nFirst Steps:\n\nCreate a new script (File → New File → R Script)\nTry typing 2 + 2 in console\nUse Help panel to explore documentation",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#creating-and-managing-projects",
    "href": "slides/introR.html#creating-and-managing-projects",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Creating and Managing Projects",
    "text": "Creating and Managing Projects\n\n# Create directories\ndirs &lt;- c(\"data\", \"figures\", \"R\", \"out\")\nsapply(dirs, dir.create, showWarnings = FALSE)\n\n# Sample data (data.table already loaded)\nsales &lt;-\n  data.table(\n    month = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\"),\n    sales = c(100, 120, 150, 130, 160, 180),\n    region = rep(c(\"North\", \"South\"), 3)\n  )\n\n# Export files (packages already loaded)\nfwrite(x = sales, file = \"data/sales.csv\")\nwrite.xlsx(x = sales, file = \"data/sales.xlsx\")\n\ncat(\"Files created:\\n\")\nlist.files(path = \"data\", full.names = TRUE)\n\n\n\nWhy Use Projects?\n\nOrganization - Keep related files together\nWorking Directory - Automatic folder management\nPortability - Easy sharing with collaborators\nVersion Control - Git integration for tracking changes\n\n\nProject Benefits:\n\nReproducibility - Others can run your code easily\nCollaboration - Share entire project folders\nBackup - Everything in one place\nScalability - From simple analyses to complex research\n\n\nBest Practice: Always work within projects - it saves time and prevents errors!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#what-is-quarto",
    "href": "slides/introR.html#what-is-quarto",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "What is Quarto?",
    "text": "What is Quarto?\n\n\nNext Generation Publishing System:\nQuarto combines code, results, and narrative in professional documents:\n\nCode Execution - Run R code automatically\nDynamic Results - Charts and tables update automatically\n\nProfessional Formatting - Publication-ready appearance\nMultiple Formats - HTML, PDF, Word from one source\n\n\nReproducible Research Revolution:\nTraditional Approach Problems:\n\nAnalyze in one program\nChart in another program\n\nWrite in word processor\nManually copy results (error-prone!)\n\nQuarto Solution:\n\nEverything in one document\nAutomatic updates when data changes\nComplete transparency and reproducibility",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#installing-quarto",
    "href": "slides/introR.html#installing-quarto",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Installing Quarto",
    "text": "Installing Quarto\nSimple Installation Process:\n\n\nInstallation Steps:\n\nVisit Official Site - https://quarto.org/docs/get-started/\nDownload Installer - Choose Windows, macOS, or Linux\nRun with Defaults - Standard installation handles everything\nRestart RStudio - Enables Quarto integration\nVerify Installation - Look for Quarto options in menus\n\n\nIntegration Benefits:\n\nSeamless RStudio Integration - New file types available\nRender Buttons - One-click document creation\nPreview Modes - See results while writing\nProject Templates - Quick start options\nVersion Control - Works with Git automatically\n\n\nResult: Professional document creation becomes as easy as writing an email!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#output-formats",
    "href": "slides/introR.html#output-formats",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Output Formats",
    "text": "Output Formats\n\n\nHTML (Web Sharing)\nformat: html\nBest for:\n\nInteractive sharing\nOnline viewing\n\nEmail distribution\nWeb publishing\n\nFeatures:\n\nInteractive elements\nEasy sharing via links\nMobile responsive\nSearch functionality\n\n\nPDF (Professional)\nformat: pdf\nBest for:\n\nAcademic papers\nProfessional reports\nPrint documents\nArchival purposes\n\nFeatures:\n\nPage numbers\nProfessional typography\nPrint-ready quality\nConsistent formatting\n\n\nWord (Collaboration)\nformat: docx\nBest for:\n\nTeam collaboration\nClient reviews\nComment workflows\nNon-R users\n\nFeatures:\n\nMicrosoft Word compatible\nTrack changes support\nEasy editing by others\nFamiliar interface\n\n\nPower: Write once, publish everywhere - same content, multiple professional formats!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#complete-quarto-example",
    "href": "slides/introR.html#complete-quarto-example",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Complete Quarto Example",
    "text": "Complete Quarto Example\n\n# Weather data\nweather &lt;-\n  data.table(\n    day = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"),\n    temp = c(22, 25, 23, 27, 24),\n    humidity = c(60, 55, 65, 50, 58),\n    condition = c(\"Sunny\", \"Cloudy\", \"Rainy\", \"Sunny\", \"Partly Cloudy\")\n  )\n\n# Statistics with pipe and fastverse\nweather %&gt;%\n  fsummarise(\n    avg_temp = fmean(temp),\n    max_temp = fmax(temp),\n    min_temp = fmin(temp)\n  )\n\n   avg_temp max_temp min_temp\n      &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:     24.2       27       22\n\n# Create and save visualization\np2 &lt;- ggplot(data = weather, mapping = aes(x = day, y = temp, fill = condition)) +\n  geom_col() +\n  labs(title = \"Daily Temperature\", x = \"Day\", y = \"Temperature (°C)\", fill = \"Condition\") +\n  theme_minimal() +\n  geom_text(mapping = aes(label = paste(temp, \"°C\")), vjust = -0.3)\n\n# Display the plot\np2\n\n\n\n\n\n\n\n# Save the plot\nggsave(\n  plot = p2,\n  filename = \"figures/daily_temperature.png\",\n  width = 10,\n  height = 6,\n  dpi = 300\n)\n\nKey Features Demonstrated: Automatic code execution, professional formatting, figure captioning, statistical analysis integration",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#reading-and-writing-data",
    "href": "slides/introR.html#reading-and-writing-data",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Reading and Writing Data",
    "text": "Reading and Writing Data\n\n# Read data efficiently with explicit arguments (packages already loaded)\nsales_csv &lt;- fread(file = \"data/sales.csv\")\nsales_excel &lt;- read_excel(path = \"data/sales.xlsx\") %&gt;% as.data.table()\n\n# Compare datasets\nidentical(x = sales_csv, y = sales_excel)\nrbindlist(l = list(CSV = sales_csv, Excel = sales_excel), idcol = \"Source\")\n\n\n\nFile Format Comparison:\n\nCSV Files - Plain text, widely compatible, smaller size\nExcel Files - Multiple sheets, formatting, larger size\n\ndata.table - R-optimized, fastest performance\n\nWhy fread() over read.csv()?\n\nMuch faster performance\nBetter type detection\nMore flexible with delimiters\nCleaner handling of messy data\n\n\nBest Practices:\n\nUse relative paths - “data/file.csv” not “C:/Users/…”\nConsistent naming - lowercase, underscores, descriptive\nOrganized folders - separate data, scripts, outputs\nBackup originals - never modify raw data files",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#practical-data-analysis-example",
    "href": "slides/introR.html#practical-data-analysis-example",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Practical Data Analysis Example",
    "text": "Practical Data Analysis Example\n\n# Test scores data\nscores &lt;-\n  data.table(\n    student = c(\"Anna\", \"Bob\", \"Carol\", \"David\", \"Eva\"),\n    math = c(85, 92, 78, 88, 95),\n    english = c(88, 85, 92, 80, 90),\n    science = c(82, 90, 85, 92, 88)\n  )\n\n# Display table with knitr (already loaded)\nkable(x = scores, caption = \"Student Test Scores\")\n\n\nStudent Test Scores\n\n\nstudent\nmath\nenglish\nscience\n\n\n\n\nAnna\n85\n88\n82\n\n\nBob\n92\n85\n90\n\n\nCarol\n78\n92\n85\n\n\nDavid\n88\n80\n92\n\n\nEva\n95\n90\n88\n\n\n\n\n# Calculate subject averages using fastverse\nsubject_summary &lt;-\n  scores %&gt;%\n  fsummarise(\n    Math = fmean(math),\n    English = fmean(english),\n    Science = fmean(science)\n  ) %&gt;%\n  pivot(\n    how = \"longer\",\n    names = list(\"Subject\", \"Average\")\n  )\n\nkable(x = subject_summary, caption = \"Subject Averages\", digits = 1)\n\n\nSubject Averages\n\n\nSubject\nAverage\n\n\n\n\nMath\n87.6\n\n\nEnglish\n87.0\n\n\nScience\n87.4",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#creating-comparison-visualizations",
    "href": "slides/introR.html#creating-comparison-visualizations",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Creating Comparison Visualizations",
    "text": "Creating Comparison Visualizations\n\n# Reshape data for visualization\nsubject_avg &lt;-\n  scores %&gt;%\n  fsummarise(\n    Math    = fmean(math),\n    English = fmean(english),\n    Science = fmean(science)\n  ) %&gt;%\n  pivot(\n    how = \"longer\",\n    names = list(\"Subject\", \"Average\")\n  )\n\n# Create and save comparison chart\np3 &lt;- ggplot(data = subject_avg, mapping = aes(x = Subject, y = Average, fill = Subject)) +\n  geom_col(alpha = 0.8, show.legend = FALSE) +\n  labs(title = \"Average Test Scores by Subject\", x = \"Subject\", y = \"Average Score\") +\n  theme_minimal() +\n  geom_text(mapping = aes(label = round(x = Average, digits = 1)), vjust = -0.3) +\n  ylim(0, 100)\n\n# Display the plot\np3\n\n\n\n\n\n\n\n# Save the plot\nggsave(\n  plot = p3,\n  filename = \"figures/subject_averages.png\",\n  width = 8,\n  height = 6,\n  dpi = 300\n)\n\nInsight: Visualization immediately reveals that Math scores are highest on average, demonstrating the power of charts over tables alone.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#getting-help-when-you-need-it",
    "href": "slides/introR.html#getting-help-when-you-need-it",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Getting Help When You Need It",
    "text": "Getting Help When You Need It\n\n# Help functions with explicit arguments\n?mean\nhelp.search(pattern = \"regression\")\nexample(topic = \"mean\")\n\n# Package help\nhelp(package = \"data.table\")\n\n\n\nBuilt-in Help System:\n\nFunction Help - ?function_name for documentation\nSearch Help - help.search(\"topic\") for related functions\nExamples - example(\"function\") for working code\nPackage Help - help(package = \"packagename\") for overview\n\n\nExternal Resources:\n\nStack Overflow - Huge Q&A community\nRStudio Community - Friendly, helpful forum\n\nR-bloggers - Daily tutorials and tips\nLocal User Groups - In-person networking and learning\nDocumentation Sites - Official package guides\n\n\nRemember: Every expert was once a beginner - the R community is known for being welcoming and helpful!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#common-mistakes-and-solutions",
    "href": "slides/introR.html#common-mistakes-and-solutions",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Common Mistakes and Solutions",
    "text": "Common Mistakes and Solutions\n\n\nCritical Mistakes to Avoid:\n\nCase Sensitivity - Mean ≠ mean\nQuotation Marks - Text needs quotes: \"Alice\"\nPackage Loading - Always library(package) first\nParentheses - Every ( needs a )\nExplicit Arguments - Use round(x = 3.14, digits = 2)\n\n\nProject Organization:\nmy-analysis/\n├── data/           # Raw data files\n├── R/              # R scripts  \n├── figures/        # Generated plots\n├── out/            # Output files\n└── README.md       # Project description\n\nBest Practice: Develop good habits early - they save hours of debugging later!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#keyboard-shortcuts-for-efficiency",
    "href": "slides/introR.html#keyboard-shortcuts-for-efficiency",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Keyboard Shortcuts for Efficiency",
    "text": "Keyboard Shortcuts for Efficiency\n\n\nEssential Shortcuts:\n\nCtrl+Enter (Win) / Cmd+Enter (Mac) - Run current line\nCtrl+Shift+Enter - Run entire code chunk\nTab - Auto-complete function names\nCtrl+Z - Undo last action\nCtrl+Shift+C - Comment/uncomment lines\n\n\nNavigation Shortcuts:\n\nCtrl+L - Clear console\nCtrl+1 - Focus on script editor\n\nCtrl+2 - Focus on console\nCtrl+S - Save current file\nCtrl+Shift+N - New script file\n\n\nTime Saver: Master 3-4 shortcuts first, then gradually add more - they dramatically speed up your workflow!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#troubleshooting-common-issues",
    "href": "slides/introR.html#troubleshooting-common-issues",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Troubleshooting Common Issues",
    "text": "Troubleshooting Common Issues\n\n# If you see \"packagename not found\"\ninstall.packages(\"packagename\")\nlibrary(packagename)\n\ninstall.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# Install multiple packages\ninstall.packages(c(\"data.table\", \"readxl\", \"openxlsx\"))\n\n# Session information\nsessionInfo()\n\n\n\nPackage Problems:\n\n“Package not found” - Install first: install.packages(\"packagename\")\nLoading errors - Check package spelling and internet connection\nVersion conflicts - Update R and packages regularly\nMissing dependencies - R usually installs these automatically\n\n\nData Import Issues:\n\nFile not found - Check file path and working directory\nEncoding problems - Try encoding = \"UTF-8\" parameter\nWrong delimiters - Some “CSV” files use semicolons or tabs\nPath problems - Use forward slashes: \"data/file.csv\"\n\n\nDebug Strategy: Read error messages carefully, Google specific errors, check documentation, ask for help - in that order!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#what-youve-accomplished-today",
    "href": "slides/introR.html#what-youve-accomplished-today",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "What You’ve Accomplished Today",
    "text": "What You’ve Accomplished Today\n\n\nTechnical Skills Mastered:\n✅ Installation - R, RStudio, Quarto setup\n✅ Basic Operations - Math, statistics with fastverse\n✅ Data Management - Creating and manipulating data.table\n✅ Visualizations - Professional charts with ggplot2\n✅ Reports - Dynamic documents with Quarto\n✅ Best Practices - Explicit arguments, project organization\n\nConceptual Understanding:\n✅ Reproducible Research - Code + results + narrative\n✅ Modern Workflow - Projects, version control, collaboration\n✅ Professional Output - Multiple formats from one source\n✅ Community Resources - Help systems and support networks\n✅ Troubleshooting - Independent problem-solving skills\n\nAchievement Unlocked: You now have the foundation for modern data science!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#your-learning-journey-continues",
    "href": "slides/introR.html#your-learning-journey-continues",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Your Learning Journey Continues",
    "text": "Your Learning Journey Continues\n\n\nImmediate Next Steps:\n\nPersonal Project - Analyze data you care about\nPractice Explicit Arguments - Always use parameter names\nReproduce This Tutorial - Try with different data\nExperiment with Styling - Modify colors and themes\nMaster Basic Workflow - Projects → Scripts → Reports\n\n\nSkill Development Path:\n\nAdvanced ggplot2 - Scatter plots, histograms, faceting\nData Import Mastery - Excel, databases, web APIs\nStatistical Methods - Regression, hypothesis testing\nAdvanced Quarto - Presentations, websites, books\nPackage Ecosystem - Specialized tools for your field\n\n\nRemember: Every expert started exactly where you are now - the key is consistent practice!",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#essential-learning-resources",
    "href": "slides/introR.html#essential-learning-resources",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Essential Learning Resources",
    "text": "Essential Learning Resources\n\n\nFree Online Books:\n\nR for Data Science (Wickham and Grolemund 2016) - The definitive beginner’s guide\nQuarto Documentation - Comprehensive feature guide\n\nggplot2 Book (Wickham 2016) - Deep dive into visualization\nfastverse Documentation - Efficient data manipulation\n\nInteractive Learning:\n\nRStudio Education - Free courses and tutorials\nSwirl - Learn R interactively within R\nDataCamp - Structured courses (some free)\n\n\nCommunity Resources:\n\nR-bloggers - Daily articles and tutorials\n#RStats Twitter - Active community sharing tips\nLocal R Meetups - Network with other users\nStack Overflow - Q&A for specific problems\nRStudio Community - Friendly help forum\n\nProfessional Development:\n\nConferences - useR!, rstudio::conf\nCertification - RStudio certifications available\nSpecialized Training - Industry-specific workshops",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#final-encouragement",
    "href": "slides/introR.html#final-encouragement",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Final Encouragement",
    "text": "Final Encouragement\n\n\nYou’re Joining a Global Community:\nThese tools are used daily by:\n\nData Scientists at Google, Netflix, Facebook\nResearchers at universities worldwide\n\nAnalysts in government and non-profits\nStudents in psychology, finance, biology\nProfessionals in healthcare, marketing, sports\n\n\nRemember:\n\nEveryone starts as a beginner - You’re in good company\nThe community is welcoming - Don’t hesitate to ask for help\nPractice makes progress - Consistent work beats perfection\nFocus on problems you care about - Personal interest drives learning\nDocument your journey - Future you will thank present you\n\n\n\n\n\n\n\n\nYour Mantra Going Forward\n\n\nAlways use explicit argument names, save your work regularly, and don’t hesitate to ask for help when you need it.",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#thank-you",
    "href": "slides/introR.html#thank-you",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "Thank You!",
    "text": "Thank You!\n\n\nKey Takeaways:\n\nFree, powerful tools for professional data analysis\nReproducible research changes how you work with data\n\nStrong community support for continuous learning\nMultiple output formats from single source documents\nModern workflow that scales from simple to complex\n\n\nContact & Resources:\n\nQuestions? Use RStudio Community forum\nAdvanced Help? Stack Overflow with #r tag\nStay Updated? Follow R-bloggers and #RStats\nLocal Community? Search for R User Groups\nOfficial Docs? R, RStudio, and Quarto websites\n\n\nHappy Analyzing! 🎉📊📈",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  },
  {
    "objectID": "slides/introR.html#r-packages-used",
    "href": "slides/introR.html#r-packages-used",
    "title": "Introduction to R, RStudio, and Quarto",
    "section": "R Packages Used",
    "text": "R Packages Used\n\n# Load all required packages for this tutorial\nlibrary(data.table) # Fast data manipulation and file reading\nlibrary(fastverse) # Collection of fast R packages for data science\nlibrary(tidyverse) # Collection of packages for data science workflow\nlibrary(readxl) # Read Excel files (.xlsx, .xls)\nlibrary(openxlsx) # Write Excel files and advanced Excel operations\nlibrary(knitr) # Dynamic report generation and table formatting\nlibrary(ggplot2) # Advanced data visualization (part of tidyverse)\n\n\n\n\n\n\n\nPackage Ecosystem Overview\n\n\ndata.table (Dowle and Srinivasan 2023) - High-performance data manipulation, much faster than base R data.frame\nfastverse - Collection of fast, complementary packages for efficient data science workflows\ntidyverse (Wickham and Grolemund 2016) - Integrated packages for data science: ggplot2, dplyr, readr, and more\nreadxl & openxlsx - Read and write Excel files without requiring Excel installation\nknitr - Dynamic document generation and professional table formatting\nggplot2 (Wickham 2016) - Grammar of graphics for beautiful, publication-ready visualizations",
    "crumbs": [
      "Lecture Slides",
      "Lectures",
      "Introduction to R, RStudio, and Quarto"
    ]
  }
]